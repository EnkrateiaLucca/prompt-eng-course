{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to set your OpenAI API key in the .env file\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization Use Case\n",
    "\n",
    "In this notebook we'll look at a text summarization use case where we'll perform some prompt engineering experiments to reach a \n",
    "desired and functional solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll follow the general steps from our prompt engineering template from notebook \n",
    "[1.1](./1.1-simple-framework-for-building-prompts.ipynb), however we'll integrate a few strategies to each step so we reach more comprehensive results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define your task clearly\n",
    "2. Define an evaluation metric\n",
    "3. Generate prompt candidates\n",
    "4. Experiment\n",
    "5. Settle on a required performance threshold to stop experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our task\n",
    "\n",
    "Our task will be to __summarize research papers__. Our initial specification of this task will be:\n",
    "\n",
    "_Summarize the main findings and methodology of the following paper._\n",
    "\n",
    "(followed by feeding the model with the paper's contents).\n",
    "\n",
    "Remember that this initial specification of the task can be re-addressed if through experimentation we see that we are not readching the desired results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Evaluation metric\n",
    "\n",
    "There are many metrics we can use to quantify the quality of a summarization task, for example:\n",
    "\n",
    "- __ROUGE__ (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "    - Source: Lin, C. Y. (2004). ROUGE: A package for automatic evaluation of summaries. Text Summarization Branches Out.\n",
    "    - Definition: ROUGE is one of the most popular metrics used to evaluate automatic summarization of texts as well as machine translation. It works by comparing an automatically produced summary or translation against a set of reference summaries (typically human-generated).\n",
    "- __BLEU__ (Bilingual Evaluation Understudy)\n",
    "    - Source: Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2002). BLEU: a method for automatic evaluation of machine translation. Proceedings of the 40th Annual Meeting on Association for Computational Linguistics.\n",
    "    - Definition: Originally designed for evaluating machine-translated texts, BLEU is also used for summarization. It measures the precision of the generated summaries by comparing the n-grams of the output with the n-grams of the reference text, providing scores based on the overlap.\n",
    "- __METEOR__ (Metric for Evaluation of Translation with Explicit ORdering)\n",
    "        - Source: Banerjee, S., & Lavie, A. (2005). METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.\n",
    "        - Definition: METEOR is another metric used for evaluation, which improves upon the foundations of BLEU by incorporating synonymy and stemming, allowing for a more nuanced matching between the evaluated summary and the reference text.\n",
    "- __BERTScore__\n",
    "    - Source: Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., & Artzi, Y. (2019). BERTScore: Evaluating Text Generation with BERT. arXiv preprint arXiv:1904.09675.\n",
    "    - Definition: BERTScore leverages the contextual embeddings from BERT language models to compare the semantic similarity of tokens between the candidate summary and reference texts. This metric evaluates the quality of content in summaries at a deeper, more semantic level.\n",
    "- __LEPOR__ (Length Penalty, Overlap, and Recall)\n",
    "    - Source: Han, L., Wong, D. F., & Chao, L. S. (2012). LEPOR: An augmented machine translation evaluation metric. Proceedings of the Workshop on Statistical Machine Translation.\n",
    "    - LEPOR is a metric that combines several factors including n-gram precision and recall, length penalty, and word order to evaluate translations, which can also be applied to summarization tasks. It provides a holistic view of the summary's quality.\n",
    "\n",
    "However as outlined in this paper, conventional referencebased metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Therefore, we'll be using an approach inspired by this [example](https://cookbook.openai.com/examples/evaluation/how_to_eval_abstractive_summarization) from the OpenAI cookbook that leverages GPT-4 as a judge for the quality of the summarization outputs obtained using the following criteria:\n",
    "\n",
    "1. Relevance: Evaluates if the summary includes only important information and excludes redundancies.\n",
    "2. Coherence: Assesses the logical flow and organization of the summary.\n",
    "3. Consistency: Checks if the summary aligns with the facts in the source document.\n",
    "4. Fluency: Rates the grammar and readability of the summary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplified version of this experimentation pipeline will be:\n",
    "\n",
    "![](./assets-resources/pipeline-experiment-summarization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where we iterate until we reach some criteria threshold that we'll define empirically as we experiment with different examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Prompt Candidates & Experiment\n",
    "\n",
    "Now that we have a clear definition of our task and we have settled on an evaluation metric. Let's generate our first batch of prompt candidates and experiment with them.\n",
    "\n",
    "For that we'll also use GPT-4 to save some time, combined with pydantic to return structured outputs (list of prompts to experiment with)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PROMPT_CANDIDATES = 5\n",
    "TASK_DEFINITION = \"Summarize the main findings and methodology of the following paper.\"\n",
    "MODEL_GEN_PROMPT_CANDIDATES = \"gpt-4-turbo\"\n",
    "MODEL_EVAL = \"gpt-4-turbo\"\n",
    "MODEL_SUMMARY = \"gpt-3.5-turbo-0125\"\n",
    "SYS_MSG_GEN_PROMPT_CANDIDATES = \"You are a prompt engineering expert, specialized in generating appropriate prompts for a given task.\"\n",
    "SYS_MSG_EVAL = \"\"\"You are an evaluation engine, specialized in scoring the quality of summaries. \\\n",
    "    You always output 4 criteria scores: relevance(0-5), coherence(0-5), consistency(0-5) and fluency(0-5).\n",
    "    1. Relevance: Evaluates if the summary includes only important information and excludes redundancies.\n",
    "    2. Coherence: Assesses the logical flow and organization of the summary.\n",
    "    3. Consistency: Checks if the summary aligns with the facts in the source document.\n",
    "    4. Fluency: Rates the grammar and readability of the summary.\n",
    "    \"\"\"\n",
    "SYS_MSG_SUMMARY = \"You are a summarization engine, you will be fed a research paper and output a summary in a desired format, specified by the users.\"\n",
    "TEMP_PROMPT_CANDIDATES = 0.2\n",
    "TEMP_SUMMARY = 0.0\n",
    "PDF_PATH = \"./assets-resources/geval_method_paper.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptCandidates(prompt_candidates=['Summarize the main findings and methodology of the paper.', 'Provide a concise summary of the key findings and the methodology used in the paper.', 'Outline the principal results and the methods employed in the study described in the paper.', 'Summarize the core findings and the research methodology of the paper.', 'Detail the main outcomes and the approach used in the research paper.'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "import instructor\n",
    "from typing import List\n",
    "from openai import OpenAI\n",
    "\n",
    "client_structured = instructor.from_openai(OpenAI())\n",
    "client = OpenAI()\n",
    "\n",
    "class PromptCandidates(BaseModel):\n",
    "    prompt_candidates: List[str]\n",
    "\n",
    "prompt_generate_candidates = f\"Generate a list of {NUM_PROMPT_CANDIDATES} for the following task:\\n\\n{TASK_DEFINITION}\"\n",
    "\n",
    "def generate_prompt_candidates(prompt: str):\n",
    "    response = client_structured.chat.completions.create(\n",
    "        model=MODEL_GEN_PROMPT_CANDIDATES,\n",
    "        messages=[{\"role\": \"system\", \"content\": SYS_MSG_GEN_PROMPT_CANDIDATES},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "        response_model=PromptCandidates,\n",
    "        temperature=TEMP_PROMPT_CANDIDATES)\n",
    "    \n",
    "    return response\n",
    "\n",
    "prompt_generate_candidates_response = generate_prompt_candidates(prompt_generate_candidates)\n",
    "prompt_generate_candidates_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Provide a concise summary of the key findings and the methodology used in the paper.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_candidates = prompt_generate_candidates_response.prompt_candidates\n",
    "prompt_candidates[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now that we have the list with the prompt candidates, we can experiment with them! All we need is to define a function to generate the summaries using the chatgpt api and another to evaluate the quality of the summaries using the evaluation criteria we've just discussed.\n",
    "\n",
    "One thing to note! Given the approach we've chosen, we'll have to feed the model both the paper and the summary\n",
    "to generate appropriate scores for the performance of each output which is not necessarily ideal, given that it can take a toll\n",
    "in terms of token cost. On the other hand, it will give us the ability to automate the entire process, which is quite nice! ;)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "class SummaryScore(BaseModel):\n",
    "    relevance_score: float = Field(description=\"Evaluates if the summary includes only important information and excludes redundancies.\")\n",
    "    coherence_score: float = Field(description=\"Assesses the logical flow and organization of the summary.\")\n",
    "    consistency_score: float = Field(description=\"Checks if the summary aligns with the facts in the source document.\")\n",
    "    fluency_score: float = Field(description=\"Rates the grammar and readability of the summary.\")\n",
    "\n",
    "\n",
    "def eval_summary_output(paper_contents: str, summary_output: str):\n",
    "    scoring_prompt = f\"Given this task: {TASK_DEFINITION} with the contents of a research paper: \\n\\\n",
    "        {paper_contents} \\n\\\n",
    "        Score the quality of the summary generated below: \\n\\\n",
    "            {summary_output}.\"\n",
    "    response = client_structured.chat.completions.create(\n",
    "        model=MODEL_EVAL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYS_MSG_EVAL},\n",
    "            {\"role\": \"user\", \"content\": scoring_prompt},\n",
    "        ],\n",
    "        response_model=SummaryScore)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_paper(prompt_summary: str):\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_SUMMARY,\n",
    "        messages=[{\"role\": \"system\", \"content\": SYS_MSG_SUMMARY},\n",
    "                  {\"role\": \"user\", \"content\": prompt_summary}],\n",
    "        temperature=TEMP_SUMMARY)\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def load_paper(pdf_path: str):\n",
    "    from langchain_community.document_loaders import PyPDFLoader\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = loader.load()\n",
    "    paper_contents = \" \".join([p.page_content for p in pages])\n",
    "    return paper_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 2511–2522\\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\\nG-E VAL: NLG Evaluation using G PT-4 with Better Human Alignment\\nYang Liu Dan Iter Yichong Xu\\nShuohang Wang Ruochen Xu Chenguang Zhu\\nMicrosoft Azure AI\\nyaliu10@microsoft.com\\nAbstract\\nThe quality of texts generated by natural lan-\\nguage generation (NLG) systems is hard to\\nmeasure automatically. Conventional reference-\\nbased metrics, such as BLEU and ROUGE,\\nhave been shown to have relatively low cor-\\nrelation with human judgments, especially for\\ntasks that require creativity and diversity. Re-\\ncent studies suggest using large language mod-\\nels (LLMs) as reference-free metrics for NLG\\nevaluation, which have the benefit of being ap-\\nplicable to new tasks that lack human refer-\\nences. However, these LLM-based evaluators\\nstill have lower human correspondence than\\nmedium-size neural evaluators. In this work,\\nwe present G-E VAL, a framework of using\\nlarge language models with chain-of-thoughts\\n(CoT) and a form-filling paradigm, to assess the\\nquality of NLG outputs. We experiment with\\ntwo generation tasks, text summarization and\\ndialogue generation. We show that G-E VAL\\nwith GPT-4 as the backbone model achieves a\\nSpearman correlation of 0.514with human on\\nsummarization task, outperforming all previous\\nmethods by a large margin. We also propose\\nanalysis on the behavior of LLM-based eval-\\nuators, and highlight the potential concern of\\nLLM-based evaluators having a bias towards\\nthe LLM-generated texts.1\\n1 Introduction\\nEvaluating the quality of natural language genera-\\ntion systems is a challenging problem even when\\nlarge language models can generate high-quality\\nand diverse texts that are often indistinguishable\\nfrom human-written texts (Ouyang et al., 2022).\\nTraditional automatic metrics, such as BLEU (Pap-\\nineni et al., 2002), ROUGE (Lin, 2004), and ME-\\nTEOR (Banerjee and Lavie, 2005), are widely used\\nfor NLG evaluation, but they have been shown to\\nhave relatively low correlation with human judg-\\nments, especially for open-ended generation tasks.\\n1https://github.com/nlpyang/gevalMoreover, these metrics require associated refer-\\nence output, which is costly to collect for new tasks.\\nRecent studies propose directly using LLMs as\\nreference-free NLG evaluators (Fu et al., 2023;\\nWang et al., 2023a). The idea is to use the LLMs to\\nscore the candidate output based on its generation\\nprobability without any reference target, under the\\nassumption that the LLMs have learned to assign\\nhigher probabilities to high-quality and fluent texts.\\nMeanwhile, it is becoming popular to use more\\npowerful LLMs like GPT-4 to evaluate smaller or\\nstudent models, like in Alpaca (Taori et al., 2023)\\nand Vicuna (Zheng et al., 2023). However, the\\nvalidity and reliability of using LLMs as NLG eval-\\nuators have not been systematically investigated.\\nIn addition, meta-evaluations show that these LLM-\\nbased evaluators still have lower human correspon-\\ndence than medium-size neural evaluators (Zhong\\net al., 2022). Thus, there is a need for a more ef-\\nfective and reliable framework for using LLMs for\\nNLG evaluation.\\nIn this paper, we propose G-E VAL, a framework\\nof using LLMs with chain-of-thoughts (CoT) (Wei\\net al., 2022) to evaluate the quality of generated\\ntexts in a form-filling paradigm. By only feeding\\nthe Task Introduction and the Evaluation Criteria\\nas a prompt, we ask LLMs to generate a CoT of\\ndetailed Evaluation Steps. Then we use the prompt\\nalong with the generated CoT to evaluate the NLG\\noutputs. The evaluator output is formatted as a\\nform. Moreover, the probabilities of the output\\nrating tokens can be used to refine the final met-\\nric. We conduct extensive experiments on three\\nmeta-evaluation benchmarks of two NLG tasks:\\ntext summarization and dialogue generation. The\\nresults show that G-E VAL can outperform existing\\nNLG evaluators by a large margin in terms of corre-\\nlation with human evaluations. Finally, we conduct\\nanalysis on the behavior of LLM-based evaluators,\\nand highlight the potential issue of LLM-based\\nevaluator having a bias towards the LLM-generated2511 texts.\\nTo summarize, our main contributions and find-\\nings in this paper are:\\n1.G-E VAL generally outperforms reference-\\nbased and reference-free baseline metrics in\\nterms of correlation with human quality judg-\\nments, especially for open-ended and creative\\nNLG tasks, such as dialogue response genera-\\ntion.\\n2.We propose to use automatic chain-of-thought\\nto improve the performance of LLM-based\\nevaluators by providing more context and\\nguidance.\\n3.We propose to re-weight the discrete scores by\\ntheir respective token probabilities to provide\\na more fine-grained continuous score for G-\\nEVAL.\\n4.We conduct an analysis of the potential is-\\nsue that LLM-based metrics have a prefer-\\nence of LLM-generated texts over human-\\nwritten texts, which may lead to the self-\\nreinforcement of LLMs if LLM-based metrics\\nare used as the reward signal for improving\\nthemselves.\\n2 Method\\nG-E VAL is a prompt-based evaluator with three\\nmain components: 1) a prompt that contains the def-\\ninition of the evaluation task and the desired evalu-\\nation criteria, 2) a chain-of-thoughts (CoT) that is\\na set of intermediate instructions generated by the\\nLLM describing the detailed evaluation steps, and\\n3) a scoring function that calls LLM and calculates\\nthe score based on the probabilities of the return\\ntokens.\\nPrompt for NLG Evaluation The prompt is a\\nnatural language instruction that defines the evalu-\\nation task and the desired evaluation criteria. For\\nexample, for text summarization, the prompt can\\nbe:\\nYou will be given one summary written\\nfor a news article. Your task is to rate\\nthe summary on one metric.\\nPlease make sure you read and under-\\nstand these instructions carefully. Please\\nkeep this document open while reviewing,\\nand refer to it as needed.The prompt should also contain customized eval-\\nuation criteria for different NLG tasks and, such as\\ncoherence, conciseness, or grammar. For example,\\nfor evaluating coherence in text summarization, we\\nadd the following content to the prompt:\\nEvaluation Criteria:\\nCoherence (1-5) - the collective quality\\nof all sentences. We align this dimen-\\nsion with the DUC quality question of\\nstructure and coherence whereby \"the\\nsummary should be well-structured and\\nwell-organized. The summary should not\\njust be a heap of related information, but\\nshould build from sentence to sentence\\nto a coherent body of information about\\na topic.\"\\nAuto Chain-of-Thoughts for NLG Evaluation\\nThe chain-of-thoughts (CoT) is a sequence of in-\\ntermediate representations that are generated by\\nthe LLM during the text generation process. For\\nevaluation tasks, some criteria need a more detailed\\nevaluation instruction beyond the simple definition,\\nand it is time-consuming to manually design such\\nevaluation steps for each task. We find that LLM\\ncan generate such evaluation steps by itself. The\\nCoT can provide more context and guidance for the\\nLLM to evaluate the generated text, and can also\\nhelp to explain the evaluation process and results.\\nFor example, for evaluating coherence in text sum-\\nmarization, we add a line of “Evaluation Steps:\" to\\nthe prompt and let LLM to generate the following\\nCoT automatically:\\n1. Read the news article carefully and\\nidentify the main topic and key points.\\n2. Read the summary and compare it to\\nthe news article. Check if the summary\\ncovers the main topic and key points of\\nthe news article, and if it presents them\\nin a clear and logical order.\\n3. Assign a score for coherence on a\\nscale of 1 to 5, where 1 is the lowest and\\n5 is the highest based on the Evaluation\\nCriteria.\\nScoring Function The scoring function calls the\\nLLM with the designed prompt, auto CoT, the input\\ncontext and the target text that needs to be evalu-\\nated. Unlike GPTScore (Fu et al., 2023) which uses2512 Auto\\nCoTTask Introduction\\nYou will be given one summary written for a news \\narticle.  Your task is to rate the summary on one \\nmetric  ……\\nEvaluation Criteria\\nCoherence (1 -5) -  the collective quality of all \\nsentences. We align this dimension with the DUC \\nquality question of structure and coherence ……\\nEvaluation Steps\\n1. Read the news article carefully and identify the \\nmain topic and key points.\\n2. Read the summary and compare it to the news \\narticle. Check if the summary covers the main topic \\nand key points of the news article, and if it \\npresents them in a clear and logical order.\\n3. Assign a score for coherence on a scale of 1 to \\n5, where 1 is the lowest and 5 is the highest based \\non the Evaluation Criteria.Input Context\\nArticle: Paul Merson has restarted his row with \\nAndros Townsend after the Tottenham midfielder \\nwas brought on with only seven minutes remaining \\nin his team \\'s 0 -0 draw with Burnley on ……\\nInput Target\\nSummary: Paul merson was brought on with only \\nseven minutes remaining in his team \\'s 0 -0 draw \\nwith burnley  ……\\nEvaluation Form (scores ONLY):\\n- Coherence:\\nWeighted Summed Score: 2.59G-EVALUser Input\\n00.20.40.6\\n1 2 3 4 5Figure 1: The overall framework of G-E VAL. We first input Task Introduction and Evaluation Criteria to the LLM,\\nand ask it to generate a CoT of detailed Evaluation Steps. Then we use the prompt along with the generated CoT to\\nevaluate the NLG outputs in a form-filling paradigm. Finally, we use the probability-weighted summation of the\\noutput scores as the final score.\\nthe conditional probability of generating the tar-\\nget text as an evaluation metric, G-E VAL directly\\nperforms the evaluation task with a form-filling\\nparadigm. This provides a more flexible way to\\nevaluate the text as the model can behave directly\\nbased on the evaluation criteria and steps. For ex-\\nample, for evaluating coherence in text summariza-\\ntion, we concatenate the prompt, the CoT, the news\\narticle, and the summary, and then call the LLM\\nto output a score from 1 to 5 for each evaluation\\naspect, based on the defined criteria.\\nHowever, we notice this direct scoring function\\nhas two issues:\\n1.For some evaluation tasks, one digit usually\\ndominates the distribution of the scores, such\\nas 3 for a 1 - 5 scale. This may lead to the low\\nvariance of the scores and the low correlation\\nwith human judgments.\\n2.LLMs usually only output integer scores, even\\nwhen the prompt explicitly requests decimal\\nvalues. This leads to many ties in evaluation\\nscores which do not capture the subtle differ-\\nence between generated texts.\\nTo address these issues, we propose using theprobabilities of output tokens from LLMs to nor-\\nmalize the scores and take their weighted summa-\\ntion as the final results. Formally, given a set of\\nscores (like from 1 to 5) predefined in the prompt\\nS={s1, s2, ..., s n}, the probability of each score\\np(si)is calculated by the LLM, and the final score\\nis:\\nscore =n∑\\ni=1p(si)×si (1)\\nThis method obtains more fine-grained, continu-\\nous scores that better reflect the quality and diver-\\nsity of the generated texts.\\n3 Experiments\\nFollowing Zhong et al. (2022), we meta-evaluate\\nour evaluator on three benchmarks, SummEval,\\nTopical-Chat and QAGS, of two NLG tasks, sum-\\nmarization and dialogue response generation.\\n3.1 Implementation Details\\nWe use OpenAI’s GPT family as our LLMs, includ-\\ning GPT-3.5 (text-davinci-003) and GPT-4. For\\nGPT-3.5, we set decoding temperature to 0 to in-\\ncrease the model’s determinism. For GPT-4, as it2513 MetricsCoherence Consistency Fluency Relevance A VG\\nρ τ ρ τ ρ τ ρ τ ρ τ\\nROUGE-1 0.167 0.126 0.160 0.130 0.115 0.094 0.326 0.252 0.192 0.150\\nROUGE-2 0.184 0.139 0.187 0.155 0.159 0.128 0.290 0.219 0.205 0.161\\nROUGE-L 0.128 0.099 0.115 0.092 0.105 0.084 0.311 0.237 0.165 0.128\\nBERTScore 0.284 0.211 0.110 0.090 0.193 0.158 0.312 0.243 0.225 0.175\\nMOVERSscore 0.159 0.118 0.157 0.127 0.129 0.105 0.318 0.244 0.191 0.148\\nBARTScore 0.448 0.342 0.382 0.315 0.356 0.292 0.356 0.273 0.385 0.305\\nUniEval 0.575 0.442 0.446 0.371 0.449 0.371 0.426 0.325 0.474 0.377\\nGPTScore 0.434 – 0.449 – 0.403 – 0.381 – 0.417 –\\nG-E VAL-3.5 0.440 0.335 0.386 0.318 0.424 0.347 0.385 0.293 0.401 0.320\\n- Probs 0.359 0.313 0.361 0.344 0.339 0.323 0.327 0.288 0.346 0.317\\nG-E VAL-4 0.582 0.457 0.507 0.425 0.506 0.455 0.547 0.433 0.514 0.418\\n- Probs 0.560 0.472 0.501 0.459 0.505 0.473 0.511 0.444 0.502 0.446\\n- CoT 0.564 0.454 0.493 0.413 0.483 0.431 0.538 0.427 0.500 0.407\\n- Description 0.513 0.424 0.421 0.344 0.447 0.373 0.479 0.388 0.479 0.377\\nTable 1: Summary-level Spearman ( ρ) and Kendall-Tau ( τ) correlations of different metrics on SummEval bench-\\nmark. G-E VAL without probabilities ( italicized ) should not be considered as a fair comparison to other metrics on τ,\\nas it leads to many ties in the scores. This results in a higher Kendall-Tau correlation, but it does not fairly reflect\\nthe true evaluation ability. More details are in Section 4.\\ndoes not support the output of token probabilities,\\nwe set ‘ n= 20, temperature = 1, top_p= 1’ to\\nsample 20 times to estimate the token probabilities.\\nWe use G-E VAL-4 to indicate G-E VALwith GPT-4\\nas the backbone model, and G-E VAL-3.5 to indi-\\ncateG-E VAL with GPT-3.5 as the backbone model.\\nExample prompts for each task are provided in the\\nAppendix.\\n3.2 Benchmarks\\nWe adopt three meta-evaluation benchmarks to\\nmeasure the correlation between G-E VAL and\\nhuman judgments.\\nSummEval (Fabbri et al., 2021) is a bench-\\nmark that compares different evaluation methods\\nfor summarization. It gives human ratings for four\\naspects of each summary: fluency ,coherence ,\\nconsistency andrelevance . It is built on the\\nCNN/DailyMail dataset (Hermann et al., 2015)\\nTopical-Chat (Mehri and Eskenazi, 2020)\\nis a testbed for meta-evaluating different evaluators\\non dialogue response generation systems that use\\nknowledge. We follow (Zhong et al., 2022) to use\\nits human ratings on four aspects: naturalness ,\\ncoherence ,engagingness andgroundedness .\\nQAGS (Wang et al., 2020) is a benchmark\\nfor evaluating hallucinations in the summarizationtask. It aims to measure the consistency\\ndimension of summaries by asking and answering\\nquestions. It is collected from two different news\\nsummarization datasets CNN/DailyMail and\\nXSum.\\n3.3 Baselines\\nWe evaluate G-E VAL against various evaluators\\nthat achieved state-of-the-art performance.\\nBERTScore (Zhang et al., 2019) measures the\\nsimilarity between two texts based on the contextu-\\nalized embedding from BERT (Devlin et al., 2019).\\nMoverScore (Zhao et al., 2019) improves\\nBERTScore by adding soft alignments and new\\naggregation methods to obtain a more robust simi-\\nlarity measure.\\nBARTScore (Yuan et al., 2021) is a unified eval-\\nuator which evaluate with the average likelihood\\nof the pretrained encoder-decoder model, BART\\n(Lewis et al., 2020). It can predict different scores\\ndepending on the formats of source and target.\\nFactCC andQAGS (Kry ´sci´nski et al., 2020;\\nWang et al., 2020) are two evaluators that measure\\nthe factual consistency of generated summaries.\\nFactCC is a BERT-based classifier that predicts\\nwhether a summary is consistent with the source\\ndocument. QAGS is a question-answering based\\nevaluator that generates questions from the sum-\\nmary and checks if the answers can be found in the\\nsource document.2514 MetricsNaturalness Coherence Engagingness Groundedness A VG\\nr ρ r ρ r ρ r ρ r ρ\\nROUGE-L 0.176 0.146 0.193 0.203 0.295 0.300 0.310 0.327 0.243 0.244\\nBLEU-4 0.180 0.175 0.131 0.235 0.232 0.316 0.213 0.310 0.189 0.259\\nMETEOR 0.212 0.191 0.250 0.302 0.367 0.439 0.333 0.391 0.290 0.331\\nBERTScore 0.226 0.209 0.214 0.233 0.317 0.335 0.291 0.317 0.262 0.273\\nUSR 0.337 0.325 0.416 0.377 0.456 0.465 0.222 0.447 0.358 0.403\\nUniEval 0.455 0.330 0.602 0.455 0.573 0.430 0.577 0.453 0.552 0.417\\nG-E VAL-3.5 0.532 0.539 0.519 0.544 0.660 0.691 0.586 0.567 0.574 0.585\\nG-E VAL-4 0.549 0.565 0.594 0.605 0.627 0.631 0.531 0.551 0.575 0.588\\nTable 2: Turn-level Spearman ( ρ) and Kendall-Tau ( τ) correlations of different metrics on Topical-Chat benchmark.\\nUSR (Mehri and Eskenazi, 2020) is evaluator\\nthat assesses dialogue response generation from\\ndifferent perspectives. It has several versions that\\nassign different scores to each target response.\\nUniEval (Zhong et al., 2022) is a unified evalua-\\ntor that can evaluate different aspects of text gen-\\neration as QA tasks. It uses a pretrained T5 model\\n(Raffel et al., 2020) to encode the evaluation task,\\nsource and target texts as questions and answers,\\nand then computes the QA score as the evaluation\\nscore. It can also handle different evaluation tasks\\nby changing the question format.\\nGPTScore (Fu et al., 2023) is a new framework\\nthat evaluates texts with generative pre-training\\nmodels like GPT-3. It assumes that a generative\\npre-training model will assign a higher probability\\nof high-quality generated text following a given in-\\nstruction and context. Unlike G-E VAL, GPTScore\\nformulates the evaluation task as a conditional gen-\\neration problem instead of a form-filling problem.\\nWe report the score of GPTScore with GPT3-text-\\ndavinci-003 as the LLM, which is also usually re-\\nferred as GPT-3.5.\\n3.4 Results for Summarization\\nWe adopt the same approach as Zhong et al. (2022)\\nto evaluate different summarization metrics using\\nsummary-level Spearman and Kendall-Tau corre-\\nlation. The first part of Table 1 shows the results\\nof metrics that compare the semantic similarity\\nbetween the model output and the reference text.\\nThese metrics perform poorly on most dimensions.\\nThe second part shows the results of metrics that\\nuse neural networks to learn from human ratings of\\nsummary quality. These metrics have much higher\\ncorrelations than the similarity-based metrics, sug-\\ngesting that they are more reliable for summariza-\\ntion evaluation.In the last part of Table 1 which corresponds to\\nGPT-based evaluators, GPTScore also uses GPTs\\nfor evaluating summarization texts, but relies on\\nGPT’s conditional probabilities of the given tar-\\nget.G-E VAL substantially surpasses all previous\\nstate-of-the-art evaluators on the SummEval bench-\\nmark. G-E VAL-4 achieved much higher human\\ncorrespondence compared with G-E VAL-3.5 on\\nboth Spearman and Kendall-Tau correlation, which\\nindicates that the larger model size of GPT-4 is\\nbeneficial for summarization evaluation. G-E VAL\\nalso outperforms GPTScore on several dimension,\\ndemonstrating the effectiveness of the simple form-\\nfilling paradigm.\\n3.5 Results for Dialogue Generation\\nWe use the Topical-chat benchmark from Mehri\\nand Eskenazi (2020) to measure how well differ-\\nent evaluators agree with human ratings on the\\nquality of dialogue responses. We calculate the\\nPearson and Spearman correlation for each turn of\\nthe dialogue. Table 2 shows that similarity-based\\nmetrics have good agreement with humans on how\\nengaging andgrounded the responses are, but not\\non the other aspects. With respect to the learning-\\nbased evaluators, before G-E VAL, UniEval predicts\\nscores that are most consistent with human judg-\\nments across all aspects.\\nAs shown in the last part, G-E VAL also substan-\\ntially surpasses all previous state-of-the-art eval-\\nuator on the Topical-Chat benchmark. Notably,\\ntheG-E VAL-3.5 can achieve similar results with\\nG-E VAL-4. This indicates that this benchmark is\\nrelatively easy for the G-E VAL model.\\n3.6 Results on Hallucinations\\nAdvanced NLG models often produce text that does\\nnot match the context input (Cao et al., 2018), and2515 MetricsQAGS-CNN QAGS-XSUM Average\\nr ρ τ r ρ τ r ρ τ\\nROUGE-2 0.459 0.418 0.333 0.097 0.083 0.068 0.278 0.250 0.200\\nROUGE-L 0.357 0.324 0.254 0.024 -0.011 -0.009 0.190 0.156 0.122\\nBERTScore 0.576 0.505 0.399 0.024 0.008 0.006 0.300 0.256 0.202\\nMoverScore 0.414 0.347 0.271 0.054 0.044 0.036 0.234 0.195 0.153\\nFactCC 0.416 0.484 0.376 0.297 0.259 0.212 0.356 0.371 0.294\\nQAGS 0.545 - - 0.175 - - 0.375 - -\\nBARTScore 0.735 0.680 0.557 0.184 0.159 0.130 0.459 0.420 0.343\\nCTC 0.619 0.564 0.450 0.309 0.295 0.242 0.464 0.430 0.346\\nUniEval 0.682 0.662 0.532 0.461 0.488 0.399 0.571 0.575 0.465\\nG-E VAL-3.5 0.477 0.516 0.410 0.211 0.406 0.343 0.344 0.461 0.377\\nG-E VAL-4 0.631 0.685 0.591 0.558 0.537 0.472 0.599 0.611 0.525\\nTable 3: Pearson ( r), Spearman ( ρ) and Kendall-Tau ( τ) correlations of different metrics on QAGS benchmark.\\nrecent studies find even powerful LLMs also suffer\\nfrom the problem of hallucination. This motivates\\nrecent research to design evaluators for measuring\\ntheconsistency aspect in summarization (Kry ´s-\\nci´nski et al., 2020; Wang et al., 2020; Cao et al.,\\n2020; Durmus et al., 2020). We test the QAGS\\nmeta-evaluation benchmark, which includes two\\ndifferent summarization datasets: CNN/DailyMail\\nand XSum (Narayan et al., 2018) Table 3 shows\\nthat BARTScore performs well on the more ex-\\ntractive subset (QAGS-CNN), but has low correla-\\ntion on the more abstractive subset (QAGS-Xsum).\\nUniEval has good correlation on both subsets of\\nthe data.\\nOn average, G-E VAL-4 outperforms all state-of-\\nthe-art evaluators on QAGS, with a large margin\\non QAGS-Xsum. G-E VAL-3.5, on the other hand,\\nfailed to perform well on this benchmark, which\\nindicates that the consistency aspect is sensitive to\\nthe LLM’s capacity. This result is consistent with\\nTable 1.\\n4 Analysis\\nWill G-E VAL prefer LLM-based outputs? One\\nconcern about using LLM as an evaluator is that it\\nmay prefer the outputs generated by the LLM itself,\\nrather than the high-quality human-written texts.\\nTo investigate this issue, we conduct an experi-\\nment on the summarization task, where we com-\\npare the evaluation scores of the LLM-generated\\nand the human-written summaries. We use the\\ndataset collected in Zhang et al. (2023), where they\\nfirst ask freelance writers to write high-quality sum-\\nmaries for news articles, and then ask annotators\\nto compare human-written summaries and LLM-\\n3.753.83.853.93.954\\nHuman\\nSummaryGPT-3.5\\nSummaryHuman\\nSummaryGPT-3.5\\nSummaryHuman\\nSummaryGPT-3.5\\nSummary\\nHuman Summary is Better LLM Summary is Better Equally GoodFigure 2: Averaged G-E VAL-4’s scores for human-\\nwritten summaries and GPT-3.5 summaries, divided\\nby human judges’ preference.\\ngenerated summaries (using GPT-3.5, text-davinci-\\n003).\\nThe dataset can be divided in three categories:\\n1) human-written summaries that are rated higher\\nthan GPT-3.5 summaries by human judges, 2)\\nhuman-written summaries that are rated lower\\nthan GPT-3.5 summaries by human judges, and 3)\\nhuman-written summaries and GPT-3.5 summaries\\nare rated equally good by human judges. We use G-\\nEVAL-4 to evaluate the summaries in each category,\\nand compare the averaged scores.2\\nThe results are shown in Figure 2. We can see\\nthat, G-E VAL-4 assigns higher scores to human-\\nwritten summaries when human judges also pre-\\nfer human-written summaries, and assigns lower\\nscores when human judges prefer GPT-3.5 sum-\\nmaries. However, G-E VAL-4 always gives higher\\nscores to GPT-3.5 summaries than human-written\\n2We use G-E VAL-4 in this experiment, because its su-\\nperiority in evaluating summarization tasks. Although it has\\ndifferent distribution with with GPT-3.5, the two LLMs should\\nshare similar behaviors in terms of text generation.2516 summaries, even when human judges prefer human-\\nwritten summaries. We propose two potential rea-\\nsons for this phenomenon:\\n1.NLG outputs from high-quality systems are\\nin natural difficult to evaluate. The authors of\\nthe original paper found that inter-annotator\\nagreement on judging human-written and\\nLLM-generated summaries is very low, with\\nKrippendorff’s alpha at 0.07.\\n2.G-E VAL may have a bias towards the LLM-\\ngenerated summaries because the model could\\nshare the same concept of evaluation criteria\\nduring generation and evaluation.\\nOur work should be considered as a preliminary\\nstudy on this issue, and more research is needed\\nto fully understand the behavior of LLM-based\\nevaluators to reduce its inherent bias towards LLM-\\ngenerated text. We highlight this concern in the\\ncontext that LLM-based evaluators may lead to\\nself-reinforcement of LLMs if the evaluation score\\nis used as a reward signal for further tuning. And\\nthis could result in the over-fitting of the LLMs to\\ntheir own evaluation criteria, rather than the true\\nevaluation criteria of the NLG tasks.\\nThe Effect of Chain-of-Thoughts We compare\\nthe performance of G-E VAL with and without\\nchain-of-thoughts (CoT) on the SummEval bench-\\nmark. Table 1 shows that G-E VAL-4 with CoT has\\nhigher correlation than G-E VAL-4 without CoT\\non all dimensions, especially for fluency . This\\nsuggests that CoT can provide more context and\\nguidance for the LLM to evaluate the generated\\ntext, and can also help to explain the evaluation\\nprocess and results. And it is shown that CoT is\\nmore useful on consistency andfluency dimen-\\nsions. We also provide results of G-E VAL with\\na simple prompting baseline on SummEval (only\\nasking GPT-4 to score a summary from 1-5 on\\neach dimension, without detailed task introduction,\\nevaluation criteria and CoT).\\nThe Effect of Probability Normalization We\\ncompare the performance of G-E VAL with and\\nwithout probability normalization on the Sum-\\nmEval benchmark. Table 1 shows that, on Kendall-\\nTau correlation, G-E VAL-4 with probabilities is\\ninferior to G-E VAL-4 without probabilities on Sum-\\nmEval. We believe this is related to the calculation\\nof Kendall-Tau correlation, which is based on the\\nnumber of concordant and discordant pairs. Directscoring without probabilities can lead to many ties,\\nwhich are not counted as either concordant or dis-\\ncordant. This may result in a higher Kendall-Tau\\ncorrelation, but it does not reflect the model’s true\\ncapacity of evaluating the generated texts. On the\\nother hand, probability normalization can obtain\\nmore fine-grained, continuous scores that better\\ncapture the subtle difference between generated\\ntexts. This is reflected by the higher Spearman cor-\\nrelation of G-E VAL-4 with probabilities, which is\\nbased on the rank order of the scores.\\nThe Effect of Different LLMs We compare\\nthe performance of G-E VAL with different LLMs\\non the SummEval and QAGS benchmarks. Ta-\\nble 1 and Table 3 show that G-E VAL-4 has\\nhigher correlation than G-E VAL-3.5 on most di-\\nmensions and datasets, except for engagingness\\nand groundedness on the Topical-Chat bench-\\nmark. This demonstrates that a better LLM can\\nimprove the performance of G-E VAL, especially\\nfor more challenging and complex evaluation tasks,\\nsuch as consistency andrelevance .\\n5 Related Work\\nNgram-based Metrics Ngram-based metrics re-\\nfer to the scores for evaluating the NLG models by\\nmeasuring the lexical overlap between a generated\\ntext and a reference text. BLEU (Papineni et al.,\\n2002) is the most widely used metric for machine\\ntranslation evaluation, which calculates the geomet-\\nric mean of modified n-gram precision and a brevity\\npenalty. ROUGE (Lin, 2004) is a recall-oriented\\nmetric for summarization evaluation, which mea-\\nsures the n-gram overlap between a generated sum-\\nmary and a set of reference summaries. It has been\\nshown that more than 60% of recent papers on\\nNLG only rely on ROUGE or BLEU to evaluate\\ntheir systems (Kasai et al., 2022). However, these\\nmetrics fail to measure content quality (Reiter and\\nBelz, 2009) or capture syntactic errors (Stent et al.,\\n2005), and therefore do not reflect the reliability of\\nNLG systems accurately.\\nEmbedding-based Metrics Embedding-based\\nmetrics refer to the scores for evaluating the NLG\\nmodels by measuring the semantic similarity be-\\ntween a generated text and a reference text based\\non the word or sentence embeddings. WMD (Kus-\\nner et al., 2015) is a metric that measures the dis-\\ntance between two texts based on the word embed-\\ndings. BERTScore (Zhang et al., 2019) measures2517 the similarity between two texts based on the con-\\ntextualized embedding from BERT (Devlin et al.,\\n2019). MoverScore (Zhao et al., 2019) improves\\nBERTScore by adding soft alignments and new\\naggregation methods to obtain a more robust simi-\\nlarity measure. (Clark et al., 2019) propose a metric\\nthat evaluates multi-sentence texts by computing\\nthe similarity between the generated text and the\\nreference text based on the sentence embeddings.\\nTask-specific Evaluators Task-specific metrics\\nrefer to the scores for evaluating the NLG mod-\\nels by measuring the quality of the generated texts\\nbased on the specific task requirements. For ex-\\nample, summarization tasks need to assess the\\nconsistency of the generated summaries (Kry ´s-\\nci´nski et al., 2020; Wang et al., 2020; Cao et al.,\\n2020; Durmus et al., 2020), and dialogue response\\ngeneration tasks need to assess the coherence of\\nthe generated responses (Dziri et al., 2019; Ye et al.,\\n2021; Ghazarian et al., 2019). However, these met-\\nrics are not generalizable to other NLG tasks, and\\nthey are not able to measure the overall quality of\\nthe generated texts.\\nUnified Evaluators Recently, some evaluators\\nhave been developed to assess text quality from\\nmultiple dimensions by varying the input and out-\\nput contents (Yuan et al., 2021) or the model vari-\\nants (Mehri and Eskenazi, 2020) they use. UniEval\\n(Zhong et al., 2022) is a unified evaluator that can\\nevaluate different aspects of text generation as QA\\ntasks. By changing the question format, it can han-\\ndle different evaluation tasks.\\nLLM-based Evaluators Fu et al. (2023) propose\\nGPTScore, a new framework that evaluated texts\\nwith generative pre-training models like GPT-3. It\\nassumes that a generative pre-training model will\\nassign a higher probability of high-quality gener-\\nated text following a given instruction and context.\\nWang et al. (2023a) conduct a preliminary survey\\nof using ChatGPT as a NLG evaluator. Kocmi and\\nFedermann (2023); Lu et al. (2023) proposed to\\nuse GPT models for evaluating machine translation\\ntasks. Very recently, Wang et al. (2023b) investi-\\ngated the problem of unfairness when using large\\nmodels in evaluating dialogue responses.\\n6 Conclusion\\nIn this paper, we propose G-E VAL, a framework of\\nusing LLM with chain-of-thoughts (CoT) to eval-\\nuate the quality of generated texts. We conductextensive experiments on two NLG tasks, text sum-\\nmarization and dialogue generation, and show that\\nG-E VAL can outperform state-of-the-art evaluators\\nand achieve higher human correspondence. We\\nalso propose preliminary analysis on the behavior\\nof LLM-based evaluators, and highlight the poten-\\ntial issue of LLM-based evaluator having a bias\\ntowards the LLM-generated texts. We hope our\\nwork can inspire more research on using LLMs for\\nNLG evaluation, and also raise awareness of the\\npotential risks and challenges of using LLMs as\\nevaluators.\\nLimitations\\nG-E VALis a framework that uses LLMs to evaluate\\nthe quality of generated texts. However, it also has\\nsome limitations that need to be addressed in future\\nwork.\\n1.As we already discussed in the paper, G-E VAL\\nmay have a bias towards the LLM-generated\\ntexts. This may lead to the self-reinforcement\\nof LLMs if the evaluation score is used as\\na reward signal for further tuning. And this\\ncould result in the over-fitting of the LLMs to\\ntheir own evaluation criteria, rather than the\\ntrue evaluation criteria of the NLG tasks.\\n2.G-E VAL is limited by the availability and ac-\\ncessibility of LLMs. Currently, most LLMs\\nare not publicly available, and require special\\naccess or payment to use. This may limit the\\napplicability and reproducibility of G-E VAL.\\nMoreover, the LLMs are constantly updated,\\nwhich may lead to inconsistent evaluation re-\\nsults across different versions of the LLMs.\\n3.We meta-evaluate G-E VALon two NLG tasks,\\ntext summarization and dialogue generation.\\nHowever, there are some emerging NLG tasks\\nin the LLM era where users prompt with free-\\nform natural language instructions. In this\\ncase, the evaluation criteria may need to be\\nmore flexible and adaptive to the user’s inten-\\ntion and preference. Therefore, more research\\nis needed to explore how to use G-E VAL for\\nevaluating these new types of NLG tasks.\\nEthics Statement\\nTheG-E VAL framework we proposed is designed\\nto offer a more effective and reliable method for\\nassessing natural language generation systems. Its2518 purpose is to aid researchers, developers, and other\\ninterested parties in evaluating the quality of text\\nproduced by NLG systems. Possible risks could\\nexist if G-E VAL is unable to precisely evaluate the\\nquality of produced texts or shows a preference for\\nLLM-created texts. This could lead to developers\\noverestimating the performance of their systems\\nor unintentionally reinforcing biases in their mod-\\nels. Furthermore, users depending on the generated\\nmaterial may receive low-quality or biased infor-\\nmation.\\nReferences\\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\\nautomatic metric for mt evaluation with improved cor-\\nrelation with human judgments. In Proceedings of\\nthe acl workshop on intrinsic and extrinsic evaluation\\nmeasures for machine translation and/or summariza-\\ntion, pages 65–72.\\nMeng Cao, Yue Dong, Jiapeng Wu, and Jackie Chi Kit\\nCheung. 2020. Factual error correction for abstrac-\\ntive summarization models. In Proceedings of the\\n2020 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) , pages 6251–6258.\\nZiqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018.\\nFaithful to the original: Fact aware neural abstractive\\nsummarization. In thirty-second AAAI conference on\\nartificial intelligence .\\nElizabeth Clark, Asli Celikyilmaz, and Noah A Smith.\\n2019. Sentence mover’s similarity: Automatic evalu-\\nation for multi-sentence texts. In Proceedings of the\\n57th Annual Meeting of the Association for Compu-\\ntational Linguistics , pages 2748–2760.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. Bert: Pre-training of deep\\nbidirectional transformers for language understand-\\ning. In Proceedings of the 2019 Conference of the\\nNorth American Chapter of the Association for Com-\\nputational Linguistics: Human Language Technolo-\\ngies, Volume 1 (Long and Short Papers) , pages 4171–\\n4186.\\nEsin Durmus, He He, and Mona Diab. 2020. Feqa: A\\nquestion answering evaluation framework for faith-\\nfulness assessment in abstractive summarization. In\\nProceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics , pages 5055–\\n5070.\\nNouha Dziri, Ehsan Kamalloo, Kory Mathewson, and\\nOsmar R Zaiane. 2019. Evaluating coherence in di-\\nalogue systems using entailment. In Proceedings of\\nthe 2019 Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies, Volume 1 (Long and\\nShort Papers) , pages 3806–3812.Alexander R Fabbri, Wojciech Kry ´sci´nski, Bryan Mc-\\nCann, Caiming Xiong, Richard Socher, and Dragomir\\nRadev. 2021. Summeval: Re-evaluating summariza-\\ntion evaluation. Transactions of the Association for\\nComputational Linguistics , 9:391–409.\\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\\nLiu. 2023. Gptscore: Evaluate as you desire. arXiv\\npreprint arXiv:2302.04166 .\\nSarik Ghazarian, Johnny Wei, Aram Galstyan, and\\nNanyun Peng. 2019. Better automatic evaluation\\nof open-domain dialogue systems with contextual-\\nized embeddings. In Proceedings of the Workshop\\non Methods for Optimizing and Evaluating Neural\\nLanguage Generation , pages 82–89, Minneapolis,\\nMinnesota. Association for Computational Linguis-\\ntics.\\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\\nand Phil Blunsom. 2015. Teaching machines to read\\nand comprehend. Advances in neural information\\nprocessing systems , 28.\\nJungo Kasai, Keisuke Sakaguchi, Ronan Le Bras,\\nLavinia Dunagan, Jacob Morrison, Alexander Fabbri,\\nYejin Choi, and Noah A. Smith. 2022. Bidimensional\\nleaderboards: Generate and evaluate language hand\\nin hand. In Proceedings of the 2022 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies , pages 3540–3557, Seattle, United States.\\nAssociation for Computational Linguistics.\\nTom Kocmi and Christian Federmann. 2023. Large\\nlanguage models are state-of-the-art evaluators of\\ntranslation quality. arXiv preprint arXiv:2302.14520 .\\nWojciech Kry ´sci´nski, Bryan McCann, Caiming Xiong,\\nand Richard Socher. 2020. Evaluating the factual\\nconsistency of abstractive text summarization. In\\nProceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP) ,\\npages 9332–9346.\\nMatt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Wein-\\nberger. 2015. From word embeddings to document\\ndistances. In International conference on machine\\nlearning , pages 957–966. PMLR.\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\\nBART: denoising sequence-to-sequence pre-training\\nfor natural language generation, translation, and com-\\nprehension. In Proceedings of the 58th Annual Meet-\\ning of the Association for Computational Linguistics,\\nACL 2020, Online, July 5-10, 2020 , pages 7871–7880.\\nAssociation for Computational Linguistics.\\nChin-Yew Lin. 2004. Rouge: A package for automatic\\nevaluation of summaries. In Text summarization\\nbranches out , pages 74–81.2519 Qingyu Lu, Baopu Qiu, Liang Ding, Liping Xie, and\\nDacheng Tao. 2023. Error analysis prompting en-\\nables human-like translation evaluation in large lan-\\nguage models: A case study on chatgpt. arXiv\\npreprint arXiv:2303.13809 .\\nShikib Mehri and Maxine Eskenazi. 2020. USR: An\\nunsupervised and reference free evaluation metric\\nfor dialog generation. In Proceedings of the 58th\\nAnnual Meeting of the Association for Computational\\nLinguistics , pages 681–707, Online. Association for\\nComputational Linguistics.\\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\\n2018. Don’t give me the details, just the summary!\\ntopic-aware convolutional neural networks for ex-\\ntreme summarization. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Lan-\\nguage Processing , pages 1797–1807.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\\n2022. Training language models to follow instruc-\\ntions with human feedback. Advances in Neural\\nInformation Processing Systems , 35:27730–27744.\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\\nJing Zhu. 2002. Bleu: a method for automatic evalu-\\nation of machine translation. In Proceedings of the\\n40th annual meeting of the Association for Computa-\\ntional Linguistics , pages 311–318.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J Liu. 2020. Exploring the limits\\nof transfer learning with a unified text-to-text trans-\\nformer. Journal of Machine Learning Research , 21:1–\\n67.\\nEhud Reiter and Anja Belz. 2009. An investigation into\\nthe validity of some metrics for automatically evalu-\\nating natural language generation systems. Computa-\\ntional Linguistics , 35(4):529–558.\\nAmanda Stent, Matthew Marge, and Mohit Singhai.\\n2005. Evaluating evaluation methods for generation\\nin the presence of variation. In Proceedings of the 6th\\ninternational conference on Computational Linguis-\\ntics and Intelligent Text Processing , pages 341–351.\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\\nAn instruction-following llama model. https://\\ngithub.com/tatsu-lab/stanford_alpaca .\\nAlex Wang, Kyunghyun Cho, and Mike Lewis. 2020.\\nAsking and answering questions to evaluate the fac-\\ntual consistency of summaries. In Proceedings of the\\n58th Annual Meeting of the Association for Compu-\\ntational Linguistics , pages 5008–5020.\\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\\n2023a. Is chatgpt a good nlg evaluator? a preliminary\\nstudy. arXiv preprint arXiv:2303.04048 .Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai\\nLin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\\n2023b. Large language models are not fair evaluators.\\narXiv preprint arXiv:2305.17926 .\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\\nChain of thought prompting elicits reasoning in large\\nlanguage models. Advances in neural information\\nprocessing systems , 28.\\nZheng Ye, Liucun Lu, Lishan Huang, Liang Lin, and\\nXiaodan Liang. 2021. Towards quantifiable dialogue\\ncoherence evaluation. In Proceedings of the 59th An-\\nnual Meeting of the Association for Computational\\nLinguistics and the 11th International Joint Confer-\\nence on Natural Language Processing (Volume 1:\\nLong Papers) , pages 2718–2729.\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\\nBartscore: Evaluating generated text as text gener-\\nation. Advances in Neural Information Processing\\nSystems , 34.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\\nuating text generation with bert. arXiv preprint\\narXiv:1904.09675 .\\nTianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang,\\nKathleen McKeown, and Tatsunori B. Hashimoto.\\n2023. Benchmarking large language models for news\\nsummarization.\\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\\ntian M Meyer, and Steffen Eger. 2019. Moverscore:\\nText generation evaluating with contextualized em-\\nbeddings and earth mover distance. In Proceedings\\nof the 2019 Conference on Empirical Methods in Nat-\\nural Language Processing and the 9th International\\nJoint Conference on Natural Language Processing\\n(EMNLP-IJCNLP) , pages 563–578.\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\\nllm-as-a-judge with mt-bench and chatbot arena.\\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu\\nJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\\nJiawei Han. 2022. Towards a unified multi-\\ndimensional evaluator for text generation. In Pro-\\nceedings of the 2022 Conference on Empirical Meth-\\nods in Natural Language Processing , pages 2023–\\n2038, Abu Dhabi, United Arab Emirates.\\nA Example Prompts\\nEvaluate Coherence in the Summarization Task\\nYou will be given one summary written\\nfor a news article.2520 Your task is to rate the summary on one\\nmetric.\\nPlease make sure you read and under-\\nstand these instructions carefully. Please\\nkeep this document open while reviewing,\\nand refer to it as needed.\\nEvaluation Criteria:\\nCoherence (1-5) - the collective quality\\nof all sentences. We align this dimension\\nwith the DUC quality question of\\nstructure and coherence whereby \"the\\nsummary should be well-structured and\\nwell-organized. The summary should not\\njust be a heap of related information, but\\nshould build from sentence to sentence\\nto a coherent body of information about\\na topic.\"\\nEvaluation Steps:\\n1. Read the news article carefully and\\nidentify the main topic and key points.\\n2. Read the summary and compare it to\\nthe news article. Check if the summary\\ncovers the main topic and key points of\\nthe news article, and if it presents them\\nin a clear and logical order.\\n3. Assign a score for coherence on a\\nscale of 1 to 5, where 1 is the lowest and\\n5 is the highest based on the Evaluation\\nCriteria.\\nExample:\\nSource Text:\\n{{Document}}\\nSummary:\\n{{Summary}}\\nEvaluation Form (scores ONLY):\\n- Coherence:\\nEvaluate Engagingness in the Dialogue Genera-\\ntion Task\\nYou will be given a conversation between\\ntwo individuals. You will then be given\\none potential response for the next turn\\nin the conversation. The response con-\\ncerns an interesting fact, which will be\\nprovided as well.Your task is to rate the responses on one\\nmetric.\\nPlease make sure you read and under-\\nstand these instructions carefully. Please\\nkeep this document open while reviewing,\\nand refer to it as needed.\\nEvaluation Crieteria:\\nEngagingness (1-3) Is the response\\ndull/interesting?\\n- A score of 1 (dull) means that the re-\\nsponse is generic and dull.\\n- A score of 2 (somewhat interesting)\\nmeans the response is somewhat inter-\\nesting and could engage you in the con-\\nversation (e.g., an opinion, thought)\\n- A score of 3 (interesting) means the\\nresponse is very interesting or presents\\nan interesting fact\\nEvaluation Steps:\\n1. Read the conversation, the correspond-\\ning fact and the response carefully.\\n2. Rate the response on a scale of 1-3 for\\nengagingness, according to the criteria\\nabove.\\n3. Provide a brief explanation for your\\nrating, referring to specific aspects of\\nthe response and the conversation.\\nExample:\\nConversation History:\\n{{Document}}\\nCorresponding Fact:\\n{{Fact}}\\nResponse:\\n{{Response}}\\nEvaluation Form (scores ONLY):\\n- Engagingness:\\nEvaluate Hallucinations\\nHuman Evaluation of Text Summariza-\\ntion Systems:\\nFactual Consistency: Does the\\nsummary untruthful or misleading facts2521 that are not supported by the source text?\\nSource Text:\\n{{Document}}\\nSummary:\\n{{Summary}}\\nDoes the summary contain factual\\ninconsistency?\\nAnswer:2522'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_contents = load_paper(PDF_PATH)\n",
    "\n",
    "paper_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run all the examples, let's do one through the entire pipeline to guarantee everything will work out as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Summarize the main findings and methodology of the paper.\\n\\'\\'\\'Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 2511–2522\\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\\nG-E VAL: NLG Evaluation using G PT-4 with Better Human Alignment\\nYang Liu Dan Iter Yichong Xu\\nShuohang Wang Ruochen Xu Chenguang Zhu\\nMicrosoft Azure AI\\nyaliu10@microsoft.com\\nAbstract\\nThe quality of texts generated by natural lan-\\nguage generation (NLG) systems is hard to\\nmeasure automatically. Conventional reference-\\nbased metrics, such as BLEU and ROUGE,\\nhave been shown to have relatively low cor-\\nrelation with human judgments, especially for\\ntasks that require creativity and diversity. Re-\\ncent studies suggest using large language mod-\\nels (LLMs) as reference-free metrics for NLG\\nevaluation, which have the benefit of being ap-\\nplicable to new tasks that lack human refer-\\nences. However, these LLM-based evaluators\\nstill have lower human correspondence than\\nmedium-size neural evaluators. In this work,\\nwe present G-E VAL, a framework of using\\nlarge language models with chain-of-thoughts\\n(CoT) and a form-filling paradigm, to assess the\\nquality of NLG outputs. We experiment with\\ntwo generation tasks, text summarization and\\ndialogue generation. We show that G-E VAL\\nwith GPT-4 as the backbone model achieves a\\nSpearman correlation of 0.514with human on\\nsummarization task, outperforming all previous\\nmethods by a large margin. We also propose\\nanalysis on the behavior of LLM-based eval-\\nuators, and highlight the potential concern of\\nLLM-based evaluators having a bias towards\\nthe LLM-generated texts.1\\n1 Introduction\\nEvaluating the quality of natural language genera-\\ntion systems is a challenging problem even when\\nlarge language models can generate high-quality\\nand diverse texts that are often indistinguishable\\nfrom human-written texts (Ouyang et al., 2022).\\nTraditional automatic metrics, such as BLEU (Pap-\\nineni et al., 2002), ROUGE (Lin, 2004), and ME-\\nTEOR (Banerjee and Lavie, 2005), are widely used\\nfor NLG evaluation, but they have been shown to\\nhave relatively low correlation with human judg-\\nments, especially for open-ended generation tasks.\\n1https://github.com/nlpyang/gevalMoreover, these metrics require associated refer-\\nence output, which is costly to collect for new tasks.\\nRecent studies propose directly using LLMs as\\nreference-free NLG evaluators (Fu et al., 2023;\\nWang et al., 2023a). The idea is to use the LLMs to\\nscore the candidate output based on its generation\\nprobability without any reference target, under the\\nassumption that the LLMs have learned to assign\\nhigher probabilities to high-quality and fluent texts.\\nMeanwhile, it is becoming popular to use more\\npowerful LLMs like GPT-4 to evaluate smaller or\\nstudent models, like in Alpaca (Taori et al., 2023)\\nand Vicuna (Zheng et al., 2023). However, the\\nvalidity and reliability of using LLMs as NLG eval-\\nuators have not been systematically investigated.\\nIn addition, meta-evaluations show that these LLM-\\nbased evaluators still have lower human correspon-\\ndence than medium-size neural evaluators (Zhong\\net al., 2022). Thus, there is a need for a more ef-\\nfective and reliable framework for using LLMs for\\nNLG evaluation.\\nIn this paper, we propose G-E VAL, a framework\\nof using LLMs with chain-of-thoughts (CoT) (Wei\\net al., 2022) to evaluate the quality of generated\\ntexts in a form-filling paradigm. By only feeding\\nthe Task Introduction and the Evaluation Criteria\\nas a prompt, we ask LLMs to generate a CoT of\\ndetailed Evaluation Steps. Then we use the prompt\\nalong with the generated CoT to evaluate the NLG\\noutputs. The evaluator output is formatted as a\\nform. Moreover, the probabilities of the output\\nrating tokens can be used to refine the final met-\\nric. We conduct extensive experiments on three\\nmeta-evaluation benchmarks of two NLG tasks:\\ntext summarization and dialogue generation. The\\nresults show that G-E VAL can outperform existing\\nNLG evaluators by a large margin in terms of corre-\\nlation with human evaluations. Finally, we conduct\\nanalysis on the behavior of LLM-based evaluators,\\nand highlight the potential issue of LLM-based\\nevaluator having a bias towards the LLM-generated2511 texts.\\nTo summarize, our main contributions and find-\\nings in this paper are:\\n1.G-E VAL generally outperforms reference-\\nbased and reference-free baseline metrics in\\nterms of correlation with human quality judg-\\nments, especially for open-ended and creative\\nNLG tasks, such as dialogue response genera-\\ntion.\\n2.We propose to use automatic chain-of-thought\\nto improve the performance of LLM-based\\nevaluators by providing more context and\\nguidance.\\n3.We propose to re-weight the discrete scores by\\ntheir respective token probabilities to provide\\na more fine-grained continuous score for G-\\nEVAL.\\n4.We conduct an analysis of the potential is-\\nsue that LLM-based metrics have a prefer-\\nence of LLM-generated texts over human-\\nwritten texts, which may lead to the self-\\nreinforcement of LLMs if LLM-based metrics\\nare used as the reward signal for improving\\nthemselves.\\n2 Method\\nG-E VAL is a prompt-based evaluator with three\\nmain components: 1) a prompt that contains the def-\\ninition of the evaluation task and the desired evalu-\\nation criteria, 2) a chain-of-thoughts (CoT) that is\\na set of intermediate instructions generated by the\\nLLM describing the detailed evaluation steps, and\\n3) a scoring function that calls LLM and calculates\\nthe score based on the probabilities of the return\\ntokens.\\nPrompt for NLG Evaluation The prompt is a\\nnatural language instruction that defines the evalu-\\nation task and the desired evaluation criteria. For\\nexample, for text summarization, the prompt can\\nbe:\\nYou will be given one summary written\\nfor a news article. Your task is to rate\\nthe summary on one metric.\\nPlease make sure you read and under-\\nstand these instructions carefully. Please\\nkeep this document open while reviewing,\\nand refer to it as needed.The prompt should also contain customized eval-\\nuation criteria for different NLG tasks and, such as\\ncoherence, conciseness, or grammar. For example,\\nfor evaluating coherence in text summarization, we\\nadd the following content to the prompt:\\nEvaluation Criteria:\\nCoherence (1-5) - the collective quality\\nof all sentences. We align this dimen-\\nsion with the DUC quality question of\\nstructure and coherence whereby \"the\\nsummary should be well-structured and\\nwell-organized. The summary should not\\njust be a heap of related information, but\\nshould build from sentence to sentence\\nto a coherent body of information about\\na topic.\"\\nAuto Chain-of-Thoughts for NLG Evaluation\\nThe chain-of-thoughts (CoT) is a sequence of in-\\ntermediate representations that are generated by\\nthe LLM during the text generation process. For\\nevaluation tasks, some criteria need a more detailed\\nevaluation instruction beyond the simple definition,\\nand it is time-consuming to manually design such\\nevaluation steps for each task. We find that LLM\\ncan generate such evaluation steps by itself. The\\nCoT can provide more context and guidance for the\\nLLM to evaluate the generated text, and can also\\nhelp to explain the evaluation process and results.\\nFor example, for evaluating coherence in text sum-\\nmarization, we add a line of “Evaluation Steps:\" to\\nthe prompt and let LLM to generate the following\\nCoT automatically:\\n1. Read the news article carefully and\\nidentify the main topic and key points.\\n2. Read the summary and compare it to\\nthe news article. Check if the summary\\ncovers the main topic and key points of\\nthe news article, and if it presents them\\nin a clear and logical order.\\n3. Assign a score for coherence on a\\nscale of 1 to 5, where 1 is the lowest and\\n5 is the highest based on the Evaluation\\nCriteria.\\nScoring Function The scoring function calls the\\nLLM with the designed prompt, auto CoT, the input\\ncontext and the target text that needs to be evalu-\\nated. Unlike GPTScore (Fu et al., 2023) which uses2512 Auto\\nCoTTask Introduction\\nYou will be given one summary written for a news \\narticle.  Your task is to rate the summary on one \\nmetric  ……\\nEvaluation Criteria\\nCoherence (1 -5) -  the collective quality of all \\nsentences. We align this dimension with the DUC \\nquality question of structure and coherence ……\\nEvaluation Steps\\n1. Read the news article carefully and identify the \\nmain topic and key points.\\n2. Read the summary and compare it to the news \\narticle. Check if the summary covers the main topic \\nand key points of the news article, and if it \\npresents them in a clear and logical order.\\n3. Assign a score for coherence on a scale of 1 to \\n5, where 1 is the lowest and 5 is the highest based \\non the Evaluation Criteria.Input Context\\nArticle: Paul Merson has restarted his row with \\nAndros Townsend after the Tottenham midfielder \\nwas brought on with only seven minutes remaining \\nin his team \\'s 0 -0 draw with Burnley on ……\\nInput Target\\nSummary: Paul merson was brought on with only \\nseven minutes remaining in his team \\'s 0 -0 draw \\nwith burnley  ……\\nEvaluation Form (scores ONLY):\\n- Coherence:\\nWeighted Summed Score: 2.59G-EVALUser Input\\n00.20.40.6\\n1 2 3 4 5Figure 1: The overall framework of G-E VAL. We first input Task Introduction and Evaluation Criteria to the LLM,\\nand ask it to generate a CoT of detailed Evaluation Steps. Then we use the prompt along with the generated CoT to\\nevaluate the NLG outputs in a form-filling paradigm. Finally, we use the probability-weighted summation of the\\noutput scores as the final score.\\nthe conditional probability of generating the tar-\\nget text as an evaluation metric, G-E VAL directly\\nperforms the evaluation task with a form-filling\\nparadigm. This provides a more flexible way to\\nevaluate the text as the model can behave directly\\nbased on the evaluation criteria and steps. For ex-\\nample, for evaluating coherence in text summariza-\\ntion, we concatenate the prompt, the CoT, the news\\narticle, and the summary, and then call the LLM\\nto output a score from 1 to 5 for each evaluation\\naspect, based on the defined criteria.\\nHowever, we notice this direct scoring function\\nhas two issues:\\n1.For some evaluation tasks, one digit usually\\ndominates the distribution of the scores, such\\nas 3 for a 1 - 5 scale. This may lead to the low\\nvariance of the scores and the low correlation\\nwith human judgments.\\n2.LLMs usually only output integer scores, even\\nwhen the prompt explicitly requests decimal\\nvalues. This leads to many ties in evaluation\\nscores which do not capture the subtle differ-\\nence between generated texts.\\nTo address these issues, we propose using theprobabilities of output tokens from LLMs to nor-\\nmalize the scores and take their weighted summa-\\ntion as the final results. Formally, given a set of\\nscores (like from 1 to 5) predefined in the prompt\\nS={s1, s2, ..., s n}, the probability of each score\\np(si)is calculated by the LLM, and the final score\\nis:\\nscore =n∑\\ni=1p(si)×si (1)\\nThis method obtains more fine-grained, continu-\\nous scores that better reflect the quality and diver-\\nsity of the generated texts.\\n3 Experiments\\nFollowing Zhong et al. (2022), we meta-evaluate\\nour evaluator on three benchmarks, SummEval,\\nTopical-Chat and QAGS, of two NLG tasks, sum-\\nmarization and dialogue response generation.\\n3.1 Implementation Details\\nWe use OpenAI’s GPT family as our LLMs, includ-\\ning GPT-3.5 (text-davinci-003) and GPT-4. For\\nGPT-3.5, we set decoding temperature to 0 to in-\\ncrease the model’s determinism. For GPT-4, as it2513 MetricsCoherence Consistency Fluency Relevance A VG\\nρ τ ρ τ ρ τ ρ τ ρ τ\\nROUGE-1 0.167 0.126 0.160 0.130 0.115 0.094 0.326 0.252 0.192 0.150\\nROUGE-2 0.184 0.139 0.187 0.155 0.159 0.128 0.290 0.219 0.205 0.161\\nROUGE-L 0.128 0.099 0.115 0.092 0.105 0.084 0.311 0.237 0.165 0.128\\nBERTScore 0.284 0.211 0.110 0.090 0.193 0.158 0.312 0.243 0.225 0.175\\nMOVERSscore 0.159 0.118 0.157 0.127 0.129 0.105 0.318 0.244 0.191 0.148\\nBARTScore 0.448 0.342 0.382 0.315 0.356 0.292 0.356 0.273 0.385 0.305\\nUniEval 0.575 0.442 0.446 0.371 0.449 0.371 0.426 0.325 0.474 0.377\\nGPTScore 0.434 – 0.449 – 0.403 – 0.381 – 0.417 –\\nG-E VAL-3.5 0.440 0.335 0.386 0.318 0.424 0.347 0.385 0.293 0.401 0.320\\n- Probs 0.359 0.313 0.361 0.344 0.339 0.323 0.327 0.288 0.346 0.317\\nG-E VAL-4 0.582 0.457 0.507 0.425 0.506 0.455 0.547 0.433 0.514 0.418\\n- Probs 0.560 0.472 0.501 0.459 0.505 0.473 0.511 0.444 0.502 0.446\\n- CoT 0.564 0.454 0.493 0.413 0.483 0.431 0.538 0.427 0.500 0.407\\n- Description 0.513 0.424 0.421 0.344 0.447 0.373 0.479 0.388 0.479 0.377\\nTable 1: Summary-level Spearman ( ρ) and Kendall-Tau ( τ) correlations of different metrics on SummEval bench-\\nmark. G-E VAL without probabilities ( italicized ) should not be considered as a fair comparison to other metrics on τ,\\nas it leads to many ties in the scores. This results in a higher Kendall-Tau correlation, but it does not fairly reflect\\nthe true evaluation ability. More details are in Section 4.\\ndoes not support the output of token probabilities,\\nwe set ‘ n= 20, temperature = 1, top_p= 1’ to\\nsample 20 times to estimate the token probabilities.\\nWe use G-E VAL-4 to indicate G-E VALwith GPT-4\\nas the backbone model, and G-E VAL-3.5 to indi-\\ncateG-E VAL with GPT-3.5 as the backbone model.\\nExample prompts for each task are provided in the\\nAppendix.\\n3.2 Benchmarks\\nWe adopt three meta-evaluation benchmarks to\\nmeasure the correlation between G-E VAL and\\nhuman judgments.\\nSummEval (Fabbri et al., 2021) is a bench-\\nmark that compares different evaluation methods\\nfor summarization. It gives human ratings for four\\naspects of each summary: fluency ,coherence ,\\nconsistency andrelevance . It is built on the\\nCNN/DailyMail dataset (Hermann et al., 2015)\\nTopical-Chat (Mehri and Eskenazi, 2020)\\nis a testbed for meta-evaluating different evaluators\\non dialogue response generation systems that use\\nknowledge. We follow (Zhong et al., 2022) to use\\nits human ratings on four aspects: naturalness ,\\ncoherence ,engagingness andgroundedness .\\nQAGS (Wang et al., 2020) is a benchmark\\nfor evaluating hallucinations in the summarizationtask. It aims to measure the consistency\\ndimension of summaries by asking and answering\\nquestions. It is collected from two different news\\nsummarization datasets CNN/DailyMail and\\nXSum.\\n3.3 Baselines\\nWe evaluate G-E VAL against various evaluators\\nthat achieved state-of-the-art performance.\\nBERTScore (Zhang et al., 2019) measures the\\nsimilarity between two texts based on the contextu-\\nalized embedding from BERT (Devlin et al., 2019).\\nMoverScore (Zhao et al., 2019) improves\\nBERTScore by adding soft alignments and new\\naggregation methods to obtain a more robust simi-\\nlarity measure.\\nBARTScore (Yuan et al., 2021) is a unified eval-\\nuator which evaluate with the average likelihood\\nof the pretrained encoder-decoder model, BART\\n(Lewis et al., 2020). It can predict different scores\\ndepending on the formats of source and target.\\nFactCC andQAGS (Kry ´sci´nski et al., 2020;\\nWang et al., 2020) are two evaluators that measure\\nthe factual consistency of generated summaries.\\nFactCC is a BERT-based classifier that predicts\\nwhether a summary is consistent with the source\\ndocument. QAGS is a question-answering based\\nevaluator that generates questions from the sum-\\nmary and checks if the answers can be found in the\\nsource document.2514 MetricsNaturalness Coherence Engagingness Groundedness A VG\\nr ρ r ρ r ρ r ρ r ρ\\nROUGE-L 0.176 0.146 0.193 0.203 0.295 0.300 0.310 0.327 0.243 0.244\\nBLEU-4 0.180 0.175 0.131 0.235 0.232 0.316 0.213 0.310 0.189 0.259\\nMETEOR 0.212 0.191 0.250 0.302 0.367 0.439 0.333 0.391 0.290 0.331\\nBERTScore 0.226 0.209 0.214 0.233 0.317 0.335 0.291 0.317 0.262 0.273\\nUSR 0.337 0.325 0.416 0.377 0.456 0.465 0.222 0.447 0.358 0.403\\nUniEval 0.455 0.330 0.602 0.455 0.573 0.430 0.577 0.453 0.552 0.417\\nG-E VAL-3.5 0.532 0.539 0.519 0.544 0.660 0.691 0.586 0.567 0.574 0.585\\nG-E VAL-4 0.549 0.565 0.594 0.605 0.627 0.631 0.531 0.551 0.575 0.588\\nTable 2: Turn-level Spearman ( ρ) and Kendall-Tau ( τ) correlations of different metrics on Topical-Chat benchmark.\\nUSR (Mehri and Eskenazi, 2020) is evaluator\\nthat assesses dialogue response generation from\\ndifferent perspectives. It has several versions that\\nassign different scores to each target response.\\nUniEval (Zhong et al., 2022) is a unified evalua-\\ntor that can evaluate different aspects of text gen-\\neration as QA tasks. It uses a pretrained T5 model\\n(Raffel et al., 2020) to encode the evaluation task,\\nsource and target texts as questions and answers,\\nand then computes the QA score as the evaluation\\nscore. It can also handle different evaluation tasks\\nby changing the question format.\\nGPTScore (Fu et al., 2023) is a new framework\\nthat evaluates texts with generative pre-training\\nmodels like GPT-3. It assumes that a generative\\npre-training model will assign a higher probability\\nof high-quality generated text following a given in-\\nstruction and context. Unlike G-E VAL, GPTScore\\nformulates the evaluation task as a conditional gen-\\neration problem instead of a form-filling problem.\\nWe report the score of GPTScore with GPT3-text-\\ndavinci-003 as the LLM, which is also usually re-\\nferred as GPT-3.5.\\n3.4 Results for Summarization\\nWe adopt the same approach as Zhong et al. (2022)\\nto evaluate different summarization metrics using\\nsummary-level Spearman and Kendall-Tau corre-\\nlation. The first part of Table 1 shows the results\\nof metrics that compare the semantic similarity\\nbetween the model output and the reference text.\\nThese metrics perform poorly on most dimensions.\\nThe second part shows the results of metrics that\\nuse neural networks to learn from human ratings of\\nsummary quality. These metrics have much higher\\ncorrelations than the similarity-based metrics, sug-\\ngesting that they are more reliable for summariza-\\ntion evaluation.In the last part of Table 1 which corresponds to\\nGPT-based evaluators, GPTScore also uses GPTs\\nfor evaluating summarization texts, but relies on\\nGPT’s conditional probabilities of the given tar-\\nget.G-E VAL substantially surpasses all previous\\nstate-of-the-art evaluators on the SummEval bench-\\nmark. G-E VAL-4 achieved much higher human\\ncorrespondence compared with G-E VAL-3.5 on\\nboth Spearman and Kendall-Tau correlation, which\\nindicates that the larger model size of GPT-4 is\\nbeneficial for summarization evaluation. G-E VAL\\nalso outperforms GPTScore on several dimension,\\ndemonstrating the effectiveness of the simple form-\\nfilling paradigm.\\n3.5 Results for Dialogue Generation\\nWe use the Topical-chat benchmark from Mehri\\nand Eskenazi (2020) to measure how well differ-\\nent evaluators agree with human ratings on the\\nquality of dialogue responses. We calculate the\\nPearson and Spearman correlation for each turn of\\nthe dialogue. Table 2 shows that similarity-based\\nmetrics have good agreement with humans on how\\nengaging andgrounded the responses are, but not\\non the other aspects. With respect to the learning-\\nbased evaluators, before G-E VAL, UniEval predicts\\nscores that are most consistent with human judg-\\nments across all aspects.\\nAs shown in the last part, G-E VAL also substan-\\ntially surpasses all previous state-of-the-art eval-\\nuator on the Topical-Chat benchmark. Notably,\\ntheG-E VAL-3.5 can achieve similar results with\\nG-E VAL-4. This indicates that this benchmark is\\nrelatively easy for the G-E VAL model.\\n3.6 Results on Hallucinations\\nAdvanced NLG models often produce text that does\\nnot match the context input (Cao et al., 2018), and2515 MetricsQAGS-CNN QAGS-XSUM Average\\nr ρ τ r ρ τ r ρ τ\\nROUGE-2 0.459 0.418 0.333 0.097 0.083 0.068 0.278 0.250 0.200\\nROUGE-L 0.357 0.324 0.254 0.024 -0.011 -0.009 0.190 0.156 0.122\\nBERTScore 0.576 0.505 0.399 0.024 0.008 0.006 0.300 0.256 0.202\\nMoverScore 0.414 0.347 0.271 0.054 0.044 0.036 0.234 0.195 0.153\\nFactCC 0.416 0.484 0.376 0.297 0.259 0.212 0.356 0.371 0.294\\nQAGS 0.545 - - 0.175 - - 0.375 - -\\nBARTScore 0.735 0.680 0.557 0.184 0.159 0.130 0.459 0.420 0.343\\nCTC 0.619 0.564 0.450 0.309 0.295 0.242 0.464 0.430 0.346\\nUniEval 0.682 0.662 0.532 0.461 0.488 0.399 0.571 0.575 0.465\\nG-E VAL-3.5 0.477 0.516 0.410 0.211 0.406 0.343 0.344 0.461 0.377\\nG-E VAL-4 0.631 0.685 0.591 0.558 0.537 0.472 0.599 0.611 0.525\\nTable 3: Pearson ( r), Spearman ( ρ) and Kendall-Tau ( τ) correlations of different metrics on QAGS benchmark.\\nrecent studies find even powerful LLMs also suffer\\nfrom the problem of hallucination. This motivates\\nrecent research to design evaluators for measuring\\ntheconsistency aspect in summarization (Kry ´s-\\nci´nski et al., 2020; Wang et al., 2020; Cao et al.,\\n2020; Durmus et al., 2020). We test the QAGS\\nmeta-evaluation benchmark, which includes two\\ndifferent summarization datasets: CNN/DailyMail\\nand XSum (Narayan et al., 2018) Table 3 shows\\nthat BARTScore performs well on the more ex-\\ntractive subset (QAGS-CNN), but has low correla-\\ntion on the more abstractive subset (QAGS-Xsum).\\nUniEval has good correlation on both subsets of\\nthe data.\\nOn average, G-E VAL-4 outperforms all state-of-\\nthe-art evaluators on QAGS, with a large margin\\non QAGS-Xsum. G-E VAL-3.5, on the other hand,\\nfailed to perform well on this benchmark, which\\nindicates that the consistency aspect is sensitive to\\nthe LLM’s capacity. This result is consistent with\\nTable 1.\\n4 Analysis\\nWill G-E VAL prefer LLM-based outputs? One\\nconcern about using LLM as an evaluator is that it\\nmay prefer the outputs generated by the LLM itself,\\nrather than the high-quality human-written texts.\\nTo investigate this issue, we conduct an experi-\\nment on the summarization task, where we com-\\npare the evaluation scores of the LLM-generated\\nand the human-written summaries. We use the\\ndataset collected in Zhang et al. (2023), where they\\nfirst ask freelance writers to write high-quality sum-\\nmaries for news articles, and then ask annotators\\nto compare human-written summaries and LLM-\\n3.753.83.853.93.954\\nHuman\\nSummaryGPT-3.5\\nSummaryHuman\\nSummaryGPT-3.5\\nSummaryHuman\\nSummaryGPT-3.5\\nSummary\\nHuman Summary is Better LLM Summary is Better Equally GoodFigure 2: Averaged G-E VAL-4’s scores for human-\\nwritten summaries and GPT-3.5 summaries, divided\\nby human judges’ preference.\\ngenerated summaries (using GPT-3.5, text-davinci-\\n003).\\nThe dataset can be divided in three categories:\\n1) human-written summaries that are rated higher\\nthan GPT-3.5 summaries by human judges, 2)\\nhuman-written summaries that are rated lower\\nthan GPT-3.5 summaries by human judges, and 3)\\nhuman-written summaries and GPT-3.5 summaries\\nare rated equally good by human judges. We use G-\\nEVAL-4 to evaluate the summaries in each category,\\nand compare the averaged scores.2\\nThe results are shown in Figure 2. We can see\\nthat, G-E VAL-4 assigns higher scores to human-\\nwritten summaries when human judges also pre-\\nfer human-written summaries, and assigns lower\\nscores when human judges prefer GPT-3.5 sum-\\nmaries. However, G-E VAL-4 always gives higher\\nscores to GPT-3.5 summaries than human-written\\n2We use G-E VAL-4 in this experiment, because its su-\\nperiority in evaluating summarization tasks. Although it has\\ndifferent distribution with with GPT-3.5, the two LLMs should\\nshare similar behaviors in terms of text generation.2516 summaries, even when human judges prefer human-\\nwritten summaries. We propose two potential rea-\\nsons for this phenomenon:\\n1.NLG outputs from high-quality systems are\\nin natural difficult to evaluate. The authors of\\nthe original paper found that inter-annotator\\nagreement on judging human-written and\\nLLM-generated summaries is very low, with\\nKrippendorff’s alpha at 0.07.\\n2.G-E VAL may have a bias towards the LLM-\\ngenerated summaries because the model could\\nshare the same concept of evaluation criteria\\nduring generation and evaluation.\\nOur work should be considered as a preliminary\\nstudy on this issue, and more research is needed\\nto fully understand the behavior of LLM-based\\nevaluators to reduce its inherent bias towards LLM-\\ngenerated text. We highlight this concern in the\\ncontext that LLM-based evaluators may lead to\\nself-reinforcement of LLMs if the evaluation score\\nis used as a reward signal for further tuning. And\\nthis could result in the over-fitting of the LLMs to\\ntheir own evaluation criteria, rather than the true\\nevaluation criteria of the NLG tasks.\\nThe Effect of Chain-of-Thoughts We compare\\nthe performance of G-E VAL with and without\\nchain-of-thoughts (CoT) on the SummEval bench-\\nmark. Table 1 shows that G-E VAL-4 with CoT has\\nhigher correlation than G-E VAL-4 without CoT\\non all dimensions, especially for fluency . This\\nsuggests that CoT can provide more context and\\nguidance for the LLM to evaluate the generated\\ntext, and can also help to explain the evaluation\\nprocess and results. And it is shown that CoT is\\nmore useful on consistency andfluency dimen-\\nsions. We also provide results of G-E VAL with\\na simple prompting baseline on SummEval (only\\nasking GPT-4 to score a summary from 1-5 on\\neach dimension, without detailed task introduction,\\nevaluation criteria and CoT).\\nThe Effect of Probability Normalization We\\ncompare the performance of G-E VAL with and\\nwithout probability normalization on the Sum-\\nmEval benchmark. Table 1 shows that, on Kendall-\\nTau correlation, G-E VAL-4 with probabilities is\\ninferior to G-E VAL-4 without probabilities on Sum-\\nmEval. We believe this is related to the calculation\\nof Kendall-Tau correlation, which is based on the\\nnumber of concordant and discordant pairs. Directscoring without probabilities can lead to many ties,\\nwhich are not counted as either concordant or dis-\\ncordant. This may result in a higher Kendall-Tau\\ncorrelation, but it does not reflect the model’s true\\ncapacity of evaluating the generated texts. On the\\nother hand, probability normalization can obtain\\nmore fine-grained, continuous scores that better\\ncapture the subtle difference between generated\\ntexts. This is reflected by the higher Spearman cor-\\nrelation of G-E VAL-4 with probabilities, which is\\nbased on the rank order of the scores.\\nThe Effect of Different LLMs We compare\\nthe performance of G-E VAL with different LLMs\\non the SummEval and QAGS benchmarks. Ta-\\nble 1 and Table 3 show that G-E VAL-4 has\\nhigher correlation than G-E VAL-3.5 on most di-\\nmensions and datasets, except for engagingness\\nand groundedness on the Topical-Chat bench-\\nmark. This demonstrates that a better LLM can\\nimprove the performance of G-E VAL, especially\\nfor more challenging and complex evaluation tasks,\\nsuch as consistency andrelevance .\\n5 Related Work\\nNgram-based Metrics Ngram-based metrics re-\\nfer to the scores for evaluating the NLG models by\\nmeasuring the lexical overlap between a generated\\ntext and a reference text. BLEU (Papineni et al.,\\n2002) is the most widely used metric for machine\\ntranslation evaluation, which calculates the geomet-\\nric mean of modified n-gram precision and a brevity\\npenalty. ROUGE (Lin, 2004) is a recall-oriented\\nmetric for summarization evaluation, which mea-\\nsures the n-gram overlap between a generated sum-\\nmary and a set of reference summaries. It has been\\nshown that more than 60% of recent papers on\\nNLG only rely on ROUGE or BLEU to evaluate\\ntheir systems (Kasai et al., 2022). However, these\\nmetrics fail to measure content quality (Reiter and\\nBelz, 2009) or capture syntactic errors (Stent et al.,\\n2005), and therefore do not reflect the reliability of\\nNLG systems accurately.\\nEmbedding-based Metrics Embedding-based\\nmetrics refer to the scores for evaluating the NLG\\nmodels by measuring the semantic similarity be-\\ntween a generated text and a reference text based\\non the word or sentence embeddings. WMD (Kus-\\nner et al., 2015) is a metric that measures the dis-\\ntance between two texts based on the word embed-\\ndings. BERTScore (Zhang et al., 2019) measures2517 the similarity between two texts based on the con-\\ntextualized embedding from BERT (Devlin et al.,\\n2019). MoverScore (Zhao et al., 2019) improves\\nBERTScore by adding soft alignments and new\\naggregation methods to obtain a more robust simi-\\nlarity measure. (Clark et al., 2019) propose a metric\\nthat evaluates multi-sentence texts by computing\\nthe similarity between the generated text and the\\nreference text based on the sentence embeddings.\\nTask-specific Evaluators Task-specific metrics\\nrefer to the scores for evaluating the NLG mod-\\nels by measuring the quality of the generated texts\\nbased on the specific task requirements. For ex-\\nample, summarization tasks need to assess the\\nconsistency of the generated summaries (Kry ´s-\\nci´nski et al., 2020; Wang et al., 2020; Cao et al.,\\n2020; Durmus et al., 2020), and dialogue response\\ngeneration tasks need to assess the coherence of\\nthe generated responses (Dziri et al., 2019; Ye et al.,\\n2021; Ghazarian et al., 2019). However, these met-\\nrics are not generalizable to other NLG tasks, and\\nthey are not able to measure the overall quality of\\nthe generated texts.\\nUnified Evaluators Recently, some evaluators\\nhave been developed to assess text quality from\\nmultiple dimensions by varying the input and out-\\nput contents (Yuan et al., 2021) or the model vari-\\nants (Mehri and Eskenazi, 2020) they use. UniEval\\n(Zhong et al., 2022) is a unified evaluator that can\\nevaluate different aspects of text generation as QA\\ntasks. By changing the question format, it can han-\\ndle different evaluation tasks.\\nLLM-based Evaluators Fu et al. (2023) propose\\nGPTScore, a new framework that evaluated texts\\nwith generative pre-training models like GPT-3. It\\nassumes that a generative pre-training model will\\nassign a higher probability of high-quality gener-\\nated text following a given instruction and context.\\nWang et al. (2023a) conduct a preliminary survey\\nof using ChatGPT as a NLG evaluator. Kocmi and\\nFedermann (2023); Lu et al. (2023) proposed to\\nuse GPT models for evaluating machine translation\\ntasks. Very recently, Wang et al. (2023b) investi-\\ngated the problem of unfairness when using large\\nmodels in evaluating dialogue responses.\\n6 Conclusion\\nIn this paper, we propose G-E VAL, a framework of\\nusing LLM with chain-of-thoughts (CoT) to eval-\\nuate the quality of generated texts. We conductextensive experiments on two NLG tasks, text sum-\\nmarization and dialogue generation, and show that\\nG-E VAL can outperform state-of-the-art evaluators\\nand achieve higher human correspondence. We\\nalso propose preliminary analysis on the behavior\\nof LLM-based evaluators, and highlight the poten-\\ntial issue of LLM-based evaluator having a bias\\ntowards the LLM-generated texts. We hope our\\nwork can inspire more research on using LLMs for\\nNLG evaluation, and also raise awareness of the\\npotential risks and challenges of using LLMs as\\nevaluators.\\nLimitations\\nG-E VALis a framework that uses LLMs to evaluate\\nthe quality of generated texts. However, it also has\\nsome limitations that need to be addressed in future\\nwork.\\n1.As we already discussed in the paper, G-E VAL\\nmay have a bias towards the LLM-generated\\ntexts. This may lead to the self-reinforcement\\nof LLMs if the evaluation score is used as\\na reward signal for further tuning. And this\\ncould result in the over-fitting of the LLMs to\\ntheir own evaluation criteria, rather than the\\ntrue evaluation criteria of the NLG tasks.\\n2.G-E VAL is limited by the availability and ac-\\ncessibility of LLMs. Currently, most LLMs\\nare not publicly available, and require special\\naccess or payment to use. This may limit the\\napplicability and reproducibility of G-E VAL.\\nMoreover, the LLMs are constantly updated,\\nwhich may lead to inconsistent evaluation re-\\nsults across different versions of the LLMs.\\n3.We meta-evaluate G-E VALon two NLG tasks,\\ntext summarization and dialogue generation.\\nHowever, there are some emerging NLG tasks\\nin the LLM era where users prompt with free-\\nform natural language instructions. In this\\ncase, the evaluation criteria may need to be\\nmore flexible and adaptive to the user’s inten-\\ntion and preference. Therefore, more research\\nis needed to explore how to use G-E VAL for\\nevaluating these new types of NLG tasks.\\nEthics Statement\\nTheG-E VAL framework we proposed is designed\\nto offer a more effective and reliable method for\\nassessing natural language generation systems. Its2518 purpose is to aid researchers, developers, and other\\ninterested parties in evaluating the quality of text\\nproduced by NLG systems. Possible risks could\\nexist if G-E VAL is unable to precisely evaluate the\\nquality of produced texts or shows a preference for\\nLLM-created texts. This could lead to developers\\noverestimating the performance of their systems\\nor unintentionally reinforcing biases in their mod-\\nels. Furthermore, users depending on the generated\\nmaterial may receive low-quality or biased infor-\\nmation.\\nReferences\\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\\nautomatic metric for mt evaluation with improved cor-\\nrelation with human judgments. In Proceedings of\\nthe acl workshop on intrinsic and extrinsic evaluation\\nmeasures for machine translation and/or summariza-\\ntion, pages 65–72.\\nMeng Cao, Yue Dong, Jiapeng Wu, and Jackie Chi Kit\\nCheung. 2020. Factual error correction for abstrac-\\ntive summarization models. In Proceedings of the\\n2020 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) , pages 6251–6258.\\nZiqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018.\\nFaithful to the original: Fact aware neural abstractive\\nsummarization. In thirty-second AAAI conference on\\nartificial intelligence .\\nElizabeth Clark, Asli Celikyilmaz, and Noah A Smith.\\n2019. Sentence mover’s similarity: Automatic evalu-\\nation for multi-sentence texts. In Proceedings of the\\n57th Annual Meeting of the Association for Compu-\\ntational Linguistics , pages 2748–2760.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. Bert: Pre-training of deep\\nbidirectional transformers for language understand-\\ning. In Proceedings of the 2019 Conference of the\\nNorth American Chapter of the Association for Com-\\nputational Linguistics: Human Language Technolo-\\ngies, Volume 1 (Long and Short Papers) , pages 4171–\\n4186.\\nEsin Durmus, He He, and Mona Diab. 2020. Feqa: A\\nquestion answering evaluation framework for faith-\\nfulness assessment in abstractive summarization. In\\nProceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics , pages 5055–\\n5070.\\nNouha Dziri, Ehsan Kamalloo, Kory Mathewson, and\\nOsmar R Zaiane. 2019. Evaluating coherence in di-\\nalogue systems using entailment. In Proceedings of\\nthe 2019 Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies, Volume 1 (Long and\\nShort Papers) , pages 3806–3812.Alexander R Fabbri, Wojciech Kry ´sci´nski, Bryan Mc-\\nCann, Caiming Xiong, Richard Socher, and Dragomir\\nRadev. 2021. Summeval: Re-evaluating summariza-\\ntion evaluation. Transactions of the Association for\\nComputational Linguistics , 9:391–409.\\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\\nLiu. 2023. Gptscore: Evaluate as you desire. arXiv\\npreprint arXiv:2302.04166 .\\nSarik Ghazarian, Johnny Wei, Aram Galstyan, and\\nNanyun Peng. 2019. Better automatic evaluation\\nof open-domain dialogue systems with contextual-\\nized embeddings. In Proceedings of the Workshop\\non Methods for Optimizing and Evaluating Neural\\nLanguage Generation , pages 82–89, Minneapolis,\\nMinnesota. Association for Computational Linguis-\\ntics.\\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\\nand Phil Blunsom. 2015. Teaching machines to read\\nand comprehend. Advances in neural information\\nprocessing systems , 28.\\nJungo Kasai, Keisuke Sakaguchi, Ronan Le Bras,\\nLavinia Dunagan, Jacob Morrison, Alexander Fabbri,\\nYejin Choi, and Noah A. Smith. 2022. Bidimensional\\nleaderboards: Generate and evaluate language hand\\nin hand. In Proceedings of the 2022 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies , pages 3540–3557, Seattle, United States.\\nAssociation for Computational Linguistics.\\nTom Kocmi and Christian Federmann. 2023. Large\\nlanguage models are state-of-the-art evaluators of\\ntranslation quality. arXiv preprint arXiv:2302.14520 .\\nWojciech Kry ´sci´nski, Bryan McCann, Caiming Xiong,\\nand Richard Socher. 2020. Evaluating the factual\\nconsistency of abstractive text summarization. In\\nProceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP) ,\\npages 9332–9346.\\nMatt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Wein-\\nberger. 2015. From word embeddings to document\\ndistances. In International conference on machine\\nlearning , pages 957–966. PMLR.\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\\nBART: denoising sequence-to-sequence pre-training\\nfor natural language generation, translation, and com-\\nprehension. In Proceedings of the 58th Annual Meet-\\ning of the Association for Computational Linguistics,\\nACL 2020, Online, July 5-10, 2020 , pages 7871–7880.\\nAssociation for Computational Linguistics.\\nChin-Yew Lin. 2004. Rouge: A package for automatic\\nevaluation of summaries. In Text summarization\\nbranches out , pages 74–81.2519 Qingyu Lu, Baopu Qiu, Liang Ding, Liping Xie, and\\nDacheng Tao. 2023. Error analysis prompting en-\\nables human-like translation evaluation in large lan-\\nguage models: A case study on chatgpt. arXiv\\npreprint arXiv:2303.13809 .\\nShikib Mehri and Maxine Eskenazi. 2020. USR: An\\nunsupervised and reference free evaluation metric\\nfor dialog generation. In Proceedings of the 58th\\nAnnual Meeting of the Association for Computational\\nLinguistics , pages 681–707, Online. Association for\\nComputational Linguistics.\\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\\n2018. Don’t give me the details, just the summary!\\ntopic-aware convolutional neural networks for ex-\\ntreme summarization. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Lan-\\nguage Processing , pages 1797–1807.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\\n2022. Training language models to follow instruc-\\ntions with human feedback. Advances in Neural\\nInformation Processing Systems , 35:27730–27744.\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\\nJing Zhu. 2002. Bleu: a method for automatic evalu-\\nation of machine translation. In Proceedings of the\\n40th annual meeting of the Association for Computa-\\ntional Linguistics , pages 311–318.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J Liu. 2020. Exploring the limits\\nof transfer learning with a unified text-to-text trans-\\nformer. Journal of Machine Learning Research , 21:1–\\n67.\\nEhud Reiter and Anja Belz. 2009. An investigation into\\nthe validity of some metrics for automatically evalu-\\nating natural language generation systems. Computa-\\ntional Linguistics , 35(4):529–558.\\nAmanda Stent, Matthew Marge, and Mohit Singhai.\\n2005. Evaluating evaluation methods for generation\\nin the presence of variation. In Proceedings of the 6th\\ninternational conference on Computational Linguis-\\ntics and Intelligent Text Processing , pages 341–351.\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\\nAn instruction-following llama model. https://\\ngithub.com/tatsu-lab/stanford_alpaca .\\nAlex Wang, Kyunghyun Cho, and Mike Lewis. 2020.\\nAsking and answering questions to evaluate the fac-\\ntual consistency of summaries. In Proceedings of the\\n58th Annual Meeting of the Association for Compu-\\ntational Linguistics , pages 5008–5020.\\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\\n2023a. Is chatgpt a good nlg evaluator? a preliminary\\nstudy. arXiv preprint arXiv:2303.04048 .Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai\\nLin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\\n2023b. Large language models are not fair evaluators.\\narXiv preprint arXiv:2305.17926 .\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\\nChain of thought prompting elicits reasoning in large\\nlanguage models. Advances in neural information\\nprocessing systems , 28.\\nZheng Ye, Liucun Lu, Lishan Huang, Liang Lin, and\\nXiaodan Liang. 2021. Towards quantifiable dialogue\\ncoherence evaluation. In Proceedings of the 59th An-\\nnual Meeting of the Association for Computational\\nLinguistics and the 11th International Joint Confer-\\nence on Natural Language Processing (Volume 1:\\nLong Papers) , pages 2718–2729.\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\\nBartscore: Evaluating generated text as text gener-\\nation. Advances in Neural Information Processing\\nSystems , 34.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\\nuating text generation with bert. arXiv preprint\\narXiv:1904.09675 .\\nTianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang,\\nKathleen McKeown, and Tatsunori B. Hashimoto.\\n2023. Benchmarking large language models for news\\nsummarization.\\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\\ntian M Meyer, and Steffen Eger. 2019. Moverscore:\\nText generation evaluating with contextualized em-\\nbeddings and earth mover distance. In Proceedings\\nof the 2019 Conference on Empirical Methods in Nat-\\nural Language Processing and the 9th International\\nJoint Conference on Natural Language Processing\\n(EMNLP-IJCNLP) , pages 563–578.\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\\nllm-as-a-judge with mt-bench and chatbot arena.\\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu\\nJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\\nJiawei Han. 2022. Towards a unified multi-\\ndimensional evaluator for text generation. In Pro-\\nceedings of the 2022 Conference on Empirical Meth-\\nods in Natural Language Processing , pages 2023–\\n2038, Abu Dhabi, United Arab Emirates.\\nA Example Prompts\\nEvaluate Coherence in the Summarization Task\\nYou will be given one summary written\\nfor a news article.2520 Your task is to rate the summary on one\\nmetric.\\nPlease make sure you read and under-\\nstand these instructions carefully. Please\\nkeep this document open while reviewing,\\nand refer to it as needed.\\nEvaluation Criteria:\\nCoherence (1-5) - the collective quality\\nof all sentences. We align this dimension\\nwith the DUC quality question of\\nstructure and coherence whereby \"the\\nsummary should be well-structured and\\nwell-organized. The summary should not\\njust be a heap of related information, but\\nshould build from sentence to sentence\\nto a coherent body of information about\\na topic.\"\\nEvaluation Steps:\\n1. Read the news article carefully and\\nidentify the main topic and key points.\\n2. Read the summary and compare it to\\nthe news article. Check if the summary\\ncovers the main topic and key points of\\nthe news article, and if it presents them\\nin a clear and logical order.\\n3. Assign a score for coherence on a\\nscale of 1 to 5, where 1 is the lowest and\\n5 is the highest based on the Evaluation\\nCriteria.\\nExample:\\nSource Text:\\n{{Document}}\\nSummary:\\n{{Summary}}\\nEvaluation Form (scores ONLY):\\n- Coherence:\\nEvaluate Engagingness in the Dialogue Genera-\\ntion Task\\nYou will be given a conversation between\\ntwo individuals. You will then be given\\none potential response for the next turn\\nin the conversation. The response con-\\ncerns an interesting fact, which will be\\nprovided as well.Your task is to rate the responses on one\\nmetric.\\nPlease make sure you read and under-\\nstand these instructions carefully. Please\\nkeep this document open while reviewing,\\nand refer to it as needed.\\nEvaluation Crieteria:\\nEngagingness (1-3) Is the response\\ndull/interesting?\\n- A score of 1 (dull) means that the re-\\nsponse is generic and dull.\\n- A score of 2 (somewhat interesting)\\nmeans the response is somewhat inter-\\nesting and could engage you in the con-\\nversation (e.g., an opinion, thought)\\n- A score of 3 (interesting) means the\\nresponse is very interesting or presents\\nan interesting fact\\nEvaluation Steps:\\n1. Read the conversation, the correspond-\\ning fact and the response carefully.\\n2. Rate the response on a scale of 1-3 for\\nengagingness, according to the criteria\\nabove.\\n3. Provide a brief explanation for your\\nrating, referring to specific aspects of\\nthe response and the conversation.\\nExample:\\nConversation History:\\n{{Document}}\\nCorresponding Fact:\\n{{Fact}}\\nResponse:\\n{{Response}}\\nEvaluation Form (scores ONLY):\\n- Engagingness:\\nEvaluate Hallucinations\\nHuman Evaluation of Text Summariza-\\ntion Systems:\\nFactual Consistency: Does the\\nsummary untruthful or misleading facts2521 that are not supported by the source text?\\nSource Text:\\n{{Document}}\\nSummary:\\n{{Summary}}\\nDoes the summary contain factual\\ninconsistency?\\nAnswer:2522\\'\\'\\''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_summary = prompt_candidates[0] + \"\\n\" + \"'''\"+paper_contents+\"'''\"\n",
    "prompt_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper \"G-E VAL: NLG Evaluation using GPT-4 with Better Human Alignment\" presents a framework for evaluating the quality of texts generated by natural language generation (NLG) systems. The framework, G-E VAL, utilizes large language models (LLMs) with chain-of-thoughts (CoT) and a form-filling paradigm to assess NLG outputs. The study focuses on two generation tasks: text summarization and dialogue generation. The main findings include:\\n1. G-E VAL outperforms reference-based and reference-free baseline metrics in correlating with human quality judgments, especially for open-ended and creative NLG tasks.\\n2. Automatic chain-of-thoughts improve the performance of LLM-based evaluators by providing more context and guidance.\\n3. Re-weighting the discrete scores by their respective token probabilities provides a more fine-grained continuous score for G-EVAL.\\n4. Analysis suggests a potential bias of LLM-based evaluators towards LLM-generated texts over human-written texts, raising concerns about self-reinforcement of LLMs.\\n\\nThe methodology involves using prompts for NLG evaluation, generating chain-of-thoughts for detailed evaluation steps, and a scoring function that calculates scores based on token probabilities. The experiments conducted on various benchmarks show that G-E VAL, especially with GPT-4 as the backbone model, outperforms existing NLG evaluators in terms of correlation with human evaluations. The paper also discusses the limitations and ethical considerations of using LLMs for NLG evaluation.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_output = summarize_paper(prompt_summary)\n",
    "summary_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SummaryScore(relevance_score=5.0, coherence_score=5.0, consistency_score=5.0, fluency_score=5.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_score = eval_summary_output(paper_contents, summary_output)\n",
    "eval_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance: 5.0\n",
      "Coherence: 5.0\n",
      "Consistency: 5.0\n",
      "Fluency: 5.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Relevance: {eval_score.relevance_score}\")\n",
    "print(f\"Coherence: {eval_score.coherence_score}\")\n",
    "print(f\"Consistency: {eval_score.consistency_score}\")\n",
    "print(f\"Fluency: {eval_score.fluency_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh Nice! Great initial results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store the results for each prompt in an organized table to keep track of their performance so we can select the best candidate by comparing their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_candidate</th>\n",
       "      <th>summary-score-avg</th>\n",
       "      <th>relevance-score</th>\n",
       "      <th>coherence-score</th>\n",
       "      <th>consistency-score</th>\n",
       "      <th>fluency-score</th>\n",
       "      <th>model-summary</th>\n",
       "      <th>prompt-token-count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [prompt_candidate, summary-score-avg, relevance-score, coherence-score, consistency-score, fluency-score, model-summary, prompt-token-count]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "def setup_prompt_summary(prompt_candidate: str, paper_contents: str) -> str:\n",
    "    prompt_summary = prompt_candidate + \"\\n\" + \"'''\"+paper_contents+\"'''\"\n",
    "    return prompt_summary\n",
    "\n",
    "def calc_summary_score_avg(eval_scores: List):\n",
    "    return sum([score for score in eval_scores])/len(eval_scores)\n",
    "\n",
    "\n",
    "def get_num_tokens(prompt, model):\n",
    "    \"\"\"Calculates the number of tokens in a text prompt\"\"\"\n",
    "    enc = tiktoken.encoding_for_model(MODEL_SUMMARY)\n",
    "    return len(enc.encode(prompt))\n",
    "\n",
    "df = pd.DataFrame(columns=['prompt_candidate', 'summary-score-avg', 'relevance-score', 'coherence-score', 'consistency-score', 'fluency-score', 'model-summary', 'prompt-token-count'])\n",
    "df.head()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_candidate</th>\n",
       "      <th>summary-score-avg</th>\n",
       "      <th>relevance-score</th>\n",
       "      <th>coherence-score</th>\n",
       "      <th>consistency-score</th>\n",
       "      <th>fluency-score</th>\n",
       "      <th>model-summary</th>\n",
       "      <th>prompt-token-count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summarize the main findings and methodology of...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Provide a concise summary of the key findings ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Outline the principal results and the methods ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Summarize the core findings and the research m...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Detail the main outcomes and the approach used...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    prompt_candidate  summary-score-avg  \\\n",
       "0  Summarize the main findings and methodology of...                5.0   \n",
       "1  Provide a concise summary of the key findings ...                5.0   \n",
       "2  Outline the principal results and the methods ...                5.0   \n",
       "3  Summarize the core findings and the research m...                4.5   \n",
       "4  Detail the main outcomes and the approach used...                5.0   \n",
       "\n",
       "   relevance-score  coherence-score  consistency-score  fluency-score  \\\n",
       "0              5.0              5.0                5.0            5.0   \n",
       "1              5.0              5.0                5.0            5.0   \n",
       "2              5.0              5.0                5.0            5.0   \n",
       "3              4.0              5.0                4.0            5.0   \n",
       "4              5.0              5.0                5.0            5.0   \n",
       "\n",
       "        model-summary  prompt-token-count  \n",
       "0  gpt-3.5-turbo-0125                  12  \n",
       "1  gpt-3.5-turbo-0125                  16  \n",
       "2  gpt-3.5-turbo-0125                  16  \n",
       "3  gpt-3.5-turbo-0125                  14  \n",
       "4  gpt-3.5-turbo-0125                  13  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for prompt in prompt_candidates:\n",
    "    prompt_candidate_token_count = get_num_tokens(prompt, MODEL_SUMMARY)\n",
    "    prompt_summary = setup_prompt_summary(prompt, paper_contents)\n",
    "    summary_output = summarize_paper(prompt_summary)\n",
    "    eval_score = eval_summary_output(prompt_summary, summary_output)\n",
    "    eval_scores_list = [eval_score.relevance_score, eval_score.coherence_score, eval_score.consistency_score, eval_score.fluency_score] \n",
    "    summary_score_avg = calc_summary_score_avg(eval_scores_list)\n",
    "    df.loc[len(df)] = [prompt, summary_score_avg, eval_score.relevance_score, eval_score.coherence_score, eval_score.consistency_score, eval_score.fluency_score,MODEL_SUMMARY,prompt_candidate_token_count]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Summarize the main findings and methodology of the paper.',\n",
       " 'Provide a concise summary of the key findings and the methodology used in the paper.',\n",
       " 'Outline the principal results and the methods employed in the study described in the paper.',\n",
       " 'Detail the main outcomes and the approach used in the research paper.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score = df['summary-score-avg'].max()\n",
    "best_prompts = df[df['summary-score-avg'] == best_score]['prompt_candidate'].tolist()\n",
    "best_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we have a tie between these prompts (all of them got the best score), we can settle this tie by using the \n",
    "token count as the criteria for defining the absolute best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Summarize the main findings and methodology of the paper.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowest_token_count_prompt = df[df[\"prompt_candidate\"].isin(best_prompts)].sort_values(\"prompt-token-count\").iloc[0][\"prompt_candidate\"]\n",
    "lowest_token_count_prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-prompt-eng",
   "language": "python",
   "name": "oreilly-prompt-eng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
