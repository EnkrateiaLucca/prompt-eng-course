{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install openai\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def get_response(prompt_question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and programming assistant\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "get_response(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "\n",
    "\n",
    "LangChain is an orchestration framework to build LLM applications leveraging a modularized infrastructure of chains and \n",
    "components like models, prompt templates and output parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. \"Flippin' Fantastic Pancakes\" - A pancake delivery service that guarantees perfectly flipped pancakes every time, with a comical twist on the traditional pancake flipping technique.\n",
       "\n",
       "2. \"Pancake Palooza\" - A pancake restaurant that offers a variety of outrageous pancake toppings and flavors, such as bacon-infused pancake batter or rainbow sprinkle pancakes. Customers can also participate in pancake eating competitions for a chance to win free pancakes for life.\n",
       "\n",
       "3. \"Pancake Pals\" - A subscription box service that delivers pancake mix and fun pancake accessories to your door every month. Each box includes a new pancake recipe and a surprise pancake-related gift, such as pancake molds in the shape of funny faces or animals."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "template = \"\"\"Give me a list of 3 funny ideas for a company that makes {product}.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "Markdown(chain.invoke({\"product\": \"pancakes\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pydantic\n",
    "\n",
    "Pydantic is the most widely used data validation library for Python. With Pydantic, schema validation and serialization are controlled by type annotations; less to learn, less code to write, and integration with your IDE and static analysis tools.\n",
    "Pydantic's core validation logic is written in Rust. As a result, Pydantic is among the fastest data validation libraries for Python. Pydantic models can emit JSON Schema, allowing for easy integration with other tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime.datetime(2020, 1, 2, 3, 4, 5, tzinfo=TzInfo(UTC))\n",
      "(10, 20)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Delivery(BaseModel):\n",
    "    timestamp: datetime\n",
    "    dimensions: Tuple[int, int]\n",
    "\n",
    "\n",
    "m = Delivery(timestamp='2020-01-02T03:04:05Z', dimensions=['10', '20'])\n",
    "print(repr(m.timestamp))\n",
    "#> datetime.datetime(2020, 1, 2, 3, 4, 5, tzinfo=TzInfo(UTC))\n",
    "print(m.dimensions)\n",
    "#> (10, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructor\n",
    "\n",
    "Instructor is a Python library that makes it a breeze to work with structured outputs from large language models (LLMs). Built on top of Pydantic, it provides a simple, transparent, and user-friendly API to manage validation, retries, and streaming responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INV-123456\n",
      "VAT-7890123\n"
     ]
    }
   ],
   "source": [
    "# !pip install instructor\n",
    "import instructor\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "\n",
    "# Patch the OpenAI client\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "class InvoiceData(BaseModel):\n",
    "    \"\"\"Call this function to extract invoice data with specific output structure\"\"\"\n",
    "    invoice_number: str = Field(..., description=\"The invoice number extracted from the PDF\")\n",
    "    vat_registration_number: str = Field(..., description=\"The VAT registration number extracted from the PDF\")\n",
    "\n",
    "fake_invoice_data = \"\"\"\n",
    "Invoice Summary\n",
    "==============\n",
    "Invoice No: INV-123456\n",
    "Date: 2024-02-11\n",
    "Billing To: ACME Corporation\n",
    "Address: 123 Business Rd, Business City, BC1234\n",
    "\n",
    "Product Description       Quantity    Unit Price    Total\n",
    "-----------------------------------------------------------\n",
    "Widget A                  10          $15.00        $150.00\n",
    "Gadget B                  5           $20.00        $100.00\n",
    "\n",
    "Subtotal:                                          $250.00\n",
    "Tax (10%):                                          $25.00\n",
    "Total:                                             $275.00\n",
    "\n",
    "VAT Registration Number: VAT-7890123\n",
    "\n",
    "Thank you for your business!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0125\",\n",
    "    messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and programming assistant\"},\n",
    "              {\"role\": \"user\", \"content\": fake_invoice_data}],\n",
    "    response_model=InvoiceData\n",
    ")\n",
    "\n",
    "print(response.invoice_number)\n",
    "print(response.vat_registration_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPY\n",
    "\n",
    "DSPy is a framework for algorithmically optimizing LM prompts and weights, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system without DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate synthetic examples to tune each step, and (5) use these examples to finetune smaller LMs to cut costs. Currently, this is hard and messy: every time you change your pipeline, your LM, or your data, all prompts (or finetuning steps) may need to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dspy-ai\n",
    "\n",
    "# Suppose you want to build a simple retrieval-augmented generation (RAG) system for question answering. \n",
    "# You can define your own RAG program like this:\n",
    "import dspy\n",
    "\n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(\"context, question -> answer\")\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        answer = self.generate_answer(context=context, question=question)\n",
    "        return answer\n",
    "\n",
    "rag = RAG()  # zero-shot, uncompiled version of RAG\n",
    "rag(\"what is the capital of France?\").answer  # -> \"Paris\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jinja2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Templating engine that allows you to write python-like code then pass data to render a document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Sandbox!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of Brazil is Bras√≠lia. It was officially inaugurated as the capital on April 21, 1960.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install Jinja2\n",
    "from jinja2 import Environment, PackageLoader, select_autoescape\n",
    "# env = Environment(\n",
    "#     loader=PackageLoader(\"yourapp\"),\n",
    "#     autoescape=select_autoescape()\n",
    "# )\n",
    "# template = env.get_template(\"mytemplate.html\")\n",
    "# print(template.render(the=\"variables\", go=\"here\"))\n",
    "from jinja2.sandbox import SandboxedEnvironment\n",
    "env = SandboxedEnvironment()\n",
    "func = lambda: \"Hello, Sandbox!\"\n",
    "print(env.from_string(\"{{ func() }}\").render(func=func))\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def get_response(prompt_question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and programming assistant\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "get_response(\"What is the capital of Canada?\")\n",
    "\n",
    "func = lambda x: get_response(f\"What is the capital of {x}?\")\n",
    "env.from_string(\"{{ func('Brazil') }}\").render(func=func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Promptflow\n",
    "\n",
    "[Promptflow](https://github.com/microsoft/promptflow) is a suite of development tools designed to streamline the end-to-end development cycle of LLM-based AI applications, from ideation, prototyping, testing, evaluation to production deployment and monitoring. It makes prompt engineering much easier and enables you to build LLM apps with production quality. It is based on jinja!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guidance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guidance is a framework for controlling large language models, it allows users to constrain generation (e.g. with regex and CFGs) as well as to interleave control (conditional, loops) and generation seamlessly.\n",
    "\n",
    "it's good for:\n",
    "\n",
    "1. Fine-grained controlled generation with open source models.\n",
    "2. [Granular design for controlled llm generation in context lke rag search over web articles](https://github.com/guidance-ai/guidance/blob/7873d331e3f6b73cb5b6e2c1ed839377286d59d7/notebooks/art_of_prompt_design/rag.ipynb)\n",
    "3. If statements within llm generation logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>system</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>You are a cat expert.</div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>user</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>What are the smallest cats?</div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>assistant</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>The</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> smallest</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> cat</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> breed</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> is</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> the</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> Sing</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>apur</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>a</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>,</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> which</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> typically</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> weighs</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> between</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> </span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>4</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> to</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> </span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>8</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> pounds</span></div></div></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !pip install guidance\n",
    "\n",
    "from guidance import models, gen, system, user, assistant\n",
    "\n",
    "gpt = models.OpenAIChat(\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# append text or generations to the model\n",
    "with system():\n",
    "    lm = gpt + \"You are a cat expert.\"\n",
    "\n",
    "with user():\n",
    "    lm += \"What are the smallest cats?\"\n",
    "\n",
    "with assistant():\n",
    "    lm += gen(\"answer\", stop=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-Prompt-Engineer\n",
    "\n",
    "It is a [github](https://github.com/mshumer/gpt-prompt-engineer) that introduced more structured experimentation\n",
    "to prompt engineering, essentially allowing the user to generate, test and rank their prompts. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marvin \n",
    "\n",
    "[Marvin](https://www.askmarvin.ai/docs/text/transformation/) is a framework that allows you to\n",
    "to convert natural language to native Python types and structured objects. This is one of its simplest but most powerful features, and forms the basis for almost every other tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Location(city='New York', state='New York')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install marvin -U\n",
    "import marvin\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Location(BaseModel):\n",
    "    city: str\n",
    "    state: str\n",
    "\n",
    "marvin.cast(\"the big apple\", target=Location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://doi.org/10.1145/3491101.3519729',\n",
       " 'https://doi.org/10.1145/3491102.3517582',\n",
       " 'https://doi.org/10.1145/3487569',\n",
       " 'https://doi.org/10.1098/rstb.2011.0416',\n",
       " 'https://doi.org/10.1145/3490099.3511105',\n",
       " 'https://doi.org/10.1145/3544548.3581388',\n",
       " 'https://doi.org/10.1145/3543829.3543834',\n",
       " 'https://doi.org/10.1016/j.iheduc.2019.03.002',\n",
       " 'https://doi.org/10.1145/3520312.3534864',\n",
       " 'https://doi.org/10.1080/03057267.2013.847261']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"references.txt\", \"r\") as f:\n",
    "    references = f.read()\n",
    "marvin.extract(references,\n",
    "               instructions=\"Extract the urls from the references in the text above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URLs(urls=['https://doi.org/10.1145/3491101.3519729', 'https://doi.org/10.1145/3491102.3517582', 'https://doi.org/10.1145/3487569', 'https://doi.org/10.1098/rstb.2011.0416', 'https://doi.org/10.1145/3490099.3511105', 'https://doi.org/10.1145/3544548.3581388', 'https://doi.org/10.1145/3543829.3543834', 'https://doi.org/10.1016/j.iheduc.2019.03.002', 'https://doi.org/10.1145/3520312.3534864', 'https://doi.org/10.1080/03057267.2013.847261'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class URLs(BaseModel):\n",
    "    urls: list\n",
    "\n",
    "marvin.cast(references, target=URLs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlines\n",
    "\n",
    "Outlines focuses on structured text generation, offering tools and functions for enhancing Large Language Model (LLM) prompting and generation processes. It provides a set of Python libraries and functions to facilitate complex text generation tasks.\n",
    "\n",
    "For more information, visit [Outlines on GitHub](https://github.com/outlines-dev)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM-Studio\n",
    "\n",
    "Awesome free interface to download and interact with open source LLMs including hosting your own server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-CLI\n",
    "\n",
    "LLM is a command-line tool and Python library for interacting with Large Language Models (LLMs), enabling users to run prompts, store results in SQLite, and generate embeddings. It supports both remote API models and self-hosted models through plugins, making it versatile for various applications.\n",
    "\n",
    "For more information, visit [LLM on GitHub](https://github.com/simonw/llm).\n",
    "\n",
    "# Fabric\n",
    "\n",
    "Fabric is an open-source framework aimed at enhancing human capabilities using AI. It offers a modular approach to tackle specific problems through a crowdsourced collection of AI prompts that are versatile and applicable in various contexts.\n",
    "\n",
    "For more details, visit [Fabric on GitHub](https://github.com/danielmiessler/fabric).\n",
    "\n",
    "# SGLang\n",
    "\n",
    "SGLang is a structured generation language specifically designed for enhancing interactions with large language models. It facilitates faster and more controlled communications by integrating a flexible frontend language with a high-performance runtime system, enabling advanced prompting techniques, parallelism, and external interactions. Similar to guidance and outlines.\n",
    "\n",
    "For more information, visit [SGLang on GitHub](https://github.com/sgl-project/sglang)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- https://huyenchip.com/2024/03/14/ai-oss.html?utm_source=bensbites&utm_medium=newsletter&utm_campaign=daily-digest-apple-and-amazon-are-waking-up"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-prompt-eng",
   "language": "python",
   "name": "oreilly-prompt-eng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
