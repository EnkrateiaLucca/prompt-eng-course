{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define your task clearly\n",
    "2. Define an evaluation metric\n",
    "3. Generate prompt candidates\n",
    "4. Experiment (remember to know when to stop experimenting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task:\n",
    "\n",
    "Bullet point summary of an AI paper for a non-technical "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metric?\n",
    "\n",
    "G-eval method for summarization, inspired by this [example](https://cookbook.openai.com/examples/evaluation/how_to_eval_abstractive_summarization) from the OpenAI cookbook.\n",
    "\n",
    "We define four distinct criteria:\n",
    "1. Relevance: Evaluates if the summary includes only important information and excludes redundancies.\n",
    "2. Coherence: Assesses the logical flow and organization of the summary.\n",
    "3. Consistency: Checks if the summary aligns with the facts in the source document.\n",
    "4. Fluency: Rates the grammar and readability of the summary.\n",
    "\n",
    "We use GPT-4 as the judge to give the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to set your OpenAI API key in the .env file\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Large language models (LLMs) such as GPT-3, InstructGPT, and LLaMA have shown remarkable performance in various Natural Language Processing (NLP) tasks due to their vast knowledge. However, LLMs still struggle with limitations in understanding and retaining information. This study focuses on evaluating the self-knowledge of LLMs, which is their ability to recognize unanswerable or unknowable questions. An automated methodology is introduced to detect uncertainty in LLMs responses, along with a new dataset called SelfAware containing unanswerable questions. The study involves analyzing 20 LLMs, revealing a certain level of self-knowledge in these models. In-context learning and instruction tuning are shown to enhance the self-knowledge of LLMs. Despite these advancements, there is a notable gap between the self-knowledge of LLMs and human proficiency in recognizing their knowledge limits.\\n\\nThe study categorizes unanswerable questions into five groups, indicating varying levels of uncertainty. The assessment of self-knowledge is measured using the F1 score, highlighting the level of uncertainty in the responses of LLMs. Experimental results show that larger models tend to exhibit better self-knowledge, and instruction tuning can significantly improve their performance. However, even the most advanced LLMs fall short compared to human self-knowledge. This research emphasizes the importance of enhancing LLMs' ability to understand their own limitations, aiming for more accurate and reliable outcomes in NLP tasks.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def summarize(prompt_question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a summarization engine, you will be fed technical papers and output non-technical summaries in a desired format, specified by the users.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "summarize(\"\"\"\n",
    "4\n",
    "Do Large Language Models Know What They Don’t Know?\n",
    "Zhangyue Yin♢Qiushi Sun♠Qipeng Guo♢\n",
    "Jiawen Wu♢Xipeng Qiu♢∗Xuanjing Huang♢\n",
    "♢School of Computer Science, Fudan University♠Department of Mathematics, National University of Singapore\n",
    "{yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu\n",
    "{qpguo16,xpqiu,xjhuang}@fudan.edu.cn\n",
    "Abstract\n",
    "Large language models (LLMs) have a wealth\n",
    "of knowledge that allows them to excel in vari-\n",
    "ous Natural Language Processing (NLP) tasks.\n",
    "Current research focuses on enhancing their\n",
    "performance within their existing knowledge.\n",
    "Despite their vast knowledge, LLMs are still\n",
    "limited by the amount of information they can\n",
    "accommodate and comprehend. Therefore, the\n",
    "ability to understand their own limitations on\n",
    "the unknows, referred to as self-knowledge,\n",
    "is of paramount importance. This study aims\n",
    "to evaluate LLMs’ self-knowledge by assess-\n",
    "ing their ability to identify unanswerable or\n",
    "unknowable questions. We introduce an auto-\n",
    "mated methodology to detect uncertainty in the\n",
    "responses of these models, providing a novel\n",
    "measure of their self-knowledge. We further in-\n",
    "troduce a unique dataset, SelfAware, consisting\n",
    "of unanswerable questions from five diverse cat-\n",
    "egories and their answerable counterparts. Our\n",
    "extensive analysis, involving 20 LLMs includ-\n",
    "ing GPT-3, InstructGPT, and LLaMA, discov-\n",
    "ering an intrinsic capacity for self-knowledge\n",
    "within these models. Moreover, we demon-\n",
    "strate that in-context learning and instruction\n",
    "tuning can further enhance this self-knowledge.\n",
    "Despite this promising insight, our findings also\n",
    "highlight a considerable gap between the capa-\n",
    "bilities of these models and human proficiency\n",
    "in recognizing the limits of their knowledge.\n",
    "“True wisdom is knowing what you don’t know.”\n",
    "–Confucius\n",
    "1 Introduction\n",
    "Recently, Large Language Models (LLMs) such\n",
    "as GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,\n",
    "2023), and LLaMA (Touvron et al., 2023) have\n",
    "shown exceptional performance on a wide range\n",
    "of NLP tasks, including common sense reason-\n",
    "ing (Wei et al., 2022; Zhou et al., 2022) and mathe-\n",
    "∗Corresponding author.UnknowsKnowsUnknowsKnowsKnown Knows Known UnknowsUnknown UnknowsUnknown KnowsUnlock\n",
    "Figure 1: Know-Unknow Quadrant. The horizontal axis\n",
    "represents the model’s memory capacity for knowledge,\n",
    "and the vertical axis represents the model’s ability to\n",
    "comprehend and utilize knowledge.\n",
    "matical problem-solving (Lewkowycz et al., 2022;\n",
    "Chen et al., 2022). Despite their ability to learn\n",
    "from huge amounts of data, LLMs still have lim-\n",
    "itations in their capacity to retain and understand\n",
    "information. To ensure responsible usage, it is cru-\n",
    "cial for LLMs to have the capability of recognizing\n",
    "their limitations and conveying uncertainty when\n",
    "responding to unanswerable or unknowable ques-\n",
    "tions. This acknowledgment of limitations, also\n",
    "known as “knowing what you don’t know,” is a\n",
    "crucial aspect in determining their practical appli-\n",
    "cability. In this work, we refer to this ability as\n",
    "model self-knowledge.\n",
    "The Know-Unknow quadrant in Figure 1 il-\n",
    "lustrates the relationship between the model’s\n",
    "knowledge and comprehension. The ratio of\n",
    "“Known Knows” to “Unknown Knows” demon-\n",
    "strates the model’s proficiency in understanding\n",
    "and applying existing knowledge. Techniques\n",
    "such as Chain-of-Thought (Wei et al., 2022), Self-\n",
    "Consistency (Wang et al., 2022), and Complex\n",
    "CoT (Fu et al., 2022) can be utilized to increase\n",
    "arXiv:2305.18153v2  [cs.CL]  30 May 2023\n",
    "this ratio, resulting in improved performance on\n",
    "NLP tasks. We focus on the ratio of “Known Un-\n",
    "knows” to “Unknown Unknows”, which indicates\n",
    "the model’s self-knowledge level, specifically un-\n",
    "derstanding its own limitations and deficiencies in\n",
    "the unknows.\n",
    "Existing datasets such as SQuAD2.0 (Rajpurkar\n",
    "et al., 2018) and NewsQA (Trischler et al., 2017),\n",
    "widely used in question answering (QA), have been\n",
    "utilized to test the self-knowledge of models with\n",
    "unanswerable questions. However, these questions\n",
    "are context-specific and could become answerable\n",
    "when supplemented with additional information.\n",
    "Srivastava et al. (2022) attempted to address this by\n",
    "evaluating LLMs’ competence in delineating their\n",
    "knowledge boundaries, employing a set of 23 pairs\n",
    "of answerable and unanswerable multiple-choice\n",
    "questions. They discovered that these models’ per-\n",
    "formance barely surpassed that of random guessing.\n",
    "Kadavath et al. (2022) suggested probing the self-\n",
    "knowledge of LLMs through the implementation\n",
    "of a distinct \"Value Head\". Yet, this approach may\n",
    "encounter difficulties when applied across varied\n",
    "domains or tasks due to task-specific training. Con-\n",
    "sequently, we redirect our focus to the inherent\n",
    "abilities of LLMs, and pose the pivotal question:\n",
    "“Do large language models know what they don’t\n",
    "know?”.\n",
    "In this study, we investigate the self-knowledge\n",
    "of LLMs using a novel approach. By gathering\n",
    "reference sentences with uncertain meanings, we\n",
    "can determine whether the model’s responses re-\n",
    "flect uncertainty using a text similarity algorithm.\n",
    "We quantified the model’s self-knowledge using\n",
    "the F1 score. To address the small and idiosyn-\n",
    "cratic limitations of existing datasets, we created\n",
    "a new dataset called SelfAware. This dataset com-\n",
    "prises 1,032 unanswerable questions, which are dis-\n",
    "tributed across five distinct categories, along with\n",
    "an additional 2,337 questions that are classified as\n",
    "answerable. Experimental results on GPT-3, In-\n",
    "structGPT, LLaMA, and other LLMs demonstrate\n",
    "that in-context learning and instruction tuning can\n",
    "effectively enhance the self-knowledge of LLMs.\n",
    "However, the self-knowledge exhibited by the cur-\n",
    "rent state-of-the-art model, GPT-4, measures at\n",
    "75.47%, signifying a notable disparity when con-\n",
    "trasted with human self-knowledge, which is rated\n",
    "at 84.93%.\n",
    "Our key contributions to this field are summa-\n",
    "rized as follows:\n",
    "• We have developed a new dataset, SelfAware,\n",
    "that comprises a diverse range of commonly\n",
    "posed unanswerable questions.\n",
    "• We propose an innovative evaluation tech-\n",
    "nique based on text similarity to quantify the\n",
    "degree of uncertainty inherent in model out-\n",
    "puts.\n",
    "• Through our detailed analysis of 20 LLMs,\n",
    "benchmarked against human self-knowledge,\n",
    "we identified a significant disparity between\n",
    "the most advanced LLMs and humans 1.\n",
    "2 Dataset Construction\n",
    "To conduct a more comprehensive evaluation of\n",
    "the model’s self-knowledge, we constructed a\n",
    "dataset that includes a larger number and more di-\n",
    "verse types of unanswerable questions than Know-\n",
    "Unknowns dataset (Srivastava et al., 2022). To\n",
    "facilitate this, we collected a corpus of 2,858 unan-\n",
    "swerable questions, sourced from online platforms\n",
    "like Quora and HowStuffWorks. These questions\n",
    "were meticulously evaluated by three seasoned an-\n",
    "notation analysts, each operating independently.\n",
    "The analysts were permitted to leverage external\n",
    "resources, such as search engines. To ensure the va-\n",
    "lidity of our dataset, we retained only the questions\n",
    "that all three analysts concurred were unanswerable.\n",
    "This rigorous process yielded a finalized collection\n",
    "of 1,032 unanswerable questions.\n",
    "In pursuit of a comprehensive evaluation, we\n",
    "opted for answerable questions drawn from three\n",
    "datasets: SQuAD (Rajpurkar et al., 2016), Hot-\n",
    "potQA (Yang et al., 2018), and TriviaQA (Joshi\n",
    "et al., 2017). Our selection was guided by Sim-\n",
    "CSE (Gao et al., 2021), which allowed us to iden-\n",
    "tify and select the answerable questions semanti-\n",
    "cally closest to the unanswerable ones. From these\n",
    "sources, we accordingly drew samples of 1,487,\n",
    "182, and 668 questions respectively, amassing a\n",
    "total of 2,337. Given that these questions can be\n",
    "effectively addressed using information available\n",
    "on Wikipedia, the foundational corpus for the train-\n",
    "ing of current LLMs, it is plausible to infer that\n",
    "the model possesses the requisite knowledge to\n",
    "generate accurate responses to these questions.\n",
    "Our dataset, christened SelfAware, incorporates\n",
    "1,032 unanswerable and 2,337 answerable ques-\n",
    "tions. To reflect real-world distribution, our dataset\n",
    "1The code pertinent to our study can be accessed\n",
    "https://github.com/yinzhangyue/SelfAware\n",
    "Category Description Example Percentage\n",
    "No scientific\n",
    "consensus\n",
    "The answer is still up\n",
    "for debate, with no consensus\n",
    "in scientific community.\n",
    "“Are we alone in the universe,\n",
    "or will we discover alien\n",
    "life at some point?”\n",
    "25%\n",
    "Imagination The question are about people’s\n",
    "imaginations of the future.\n",
    "\"What will the fastest form of\n",
    "transportation be in 2050?\" 15%\n",
    "Completely\n",
    "subjective\n",
    "The answer depends on\n",
    "personal preference.\n",
    "\"Would you rather be shot\n",
    "into space or explore the\n",
    "deepest depths of the sea?\"\n",
    "27%\n",
    "Too many\n",
    "variables\n",
    "The question with too\n",
    "many variables cannot\n",
    "be answered accurately.\n",
    "“John made 6 dollars mowing lawns\n",
    "and 18 dollars weed eating.\n",
    "If he only spent 3 or 5 dollar a week,\n",
    "how long would the money last him?”\n",
    "10%\n",
    "Philosophical\n",
    "The question can yield\n",
    "multiple responses, but it\n",
    "lacks a definitive answer.\n",
    "“How come god was\n",
    "born from nothingness?” 23%\n",
    "Table 1: Unanswerable questions in the SelfAware dataset that span across multiple categories.\n",
    "contains a proportion of answerable questions that\n",
    "is twice as large as the volume of unanswerable\n",
    "ones. Nevertheless, to ensure the feasibility of test-\n",
    "ing, we have purposefully capped the number of\n",
    "answerable questions.\n",
    "2.1 Dataset Analysis\n",
    "To gain insight into the reasons precluding a cer-\n",
    "tain answer, we undertook a manual analysis of\n",
    "100 randomly selected unanswerable questions. As\n",
    "tabulated in Table 1, we have broadly segregated\n",
    "these questions into five distinctive categories. “No\n",
    "Scientific Consensus\" encapsulates questions that\n",
    "ignite ongoing debates within the scientific com-\n",
    "munity, such as those concerning the universe’s\n",
    "origin. “Imagination\" includes questions involving\n",
    "speculative future scenarios, like envisaged events\n",
    "over the next 50 years. “Completely Subjective\"\n",
    "comprises questions that are inherently personal,\n",
    "where answers depend heavily on individual predis-\n",
    "positions. “Too Many Variables\" pertains to mathe-\n",
    "matical problems that become unsolvable owing to\n",
    "the overwhelming prevalence of variables. Lastly,\n",
    "“Philosophical\" represents questions of a profound,\n",
    "often metaphysical, nature that resist concrete an-\n",
    "swers. Ideally, upon encountering such questions,\n",
    "the model should express uncertainty instead of\n",
    "delivering conclusive responses.\n",
    "3 Evaluation Method\n",
    "This section elucidates the methodology employed\n",
    "for assessing self-knowledge in the generated text.\n",
    "In order to achieve this, we define a similarity func-\n",
    "tion, fsim, to compute the similarity, S, between\n",
    "a given sentence, t, and a collection of reference\n",
    "sentences, U ={u1, u2, . . . , un}, endowed with\n",
    "uncertain meanings.\n",
    "Si =fsim(t, ui). (1)\n",
    "Whenever any Si surpasses a pre-determined\n",
    "threshold T, we perceive the text t as encompass-\n",
    "ing uncertain meanings, thereby eliminating the\n",
    "need for manual evaluation of the response.\n",
    "Given the substantial disparity in the volume of\n",
    "answerable and unanswerable questions in Self-\n",
    "Aware, we adopt the F1 score as a measure of\n",
    "LLMs’ self-knowledge. Our focus rests on identi-\n",
    "fying unanswerable questions, hence we designate\n",
    "them as positive cases and categorize answerable\n",
    "questions as negative cases.\n",
    "4 Experiment\n",
    "4.1 Model\n",
    "We conduct a sequence of experiments to evaluate\n",
    "the degree of self-knowledge manifested by various\n",
    "LLMs, including GPT-3 (Brown et al., 2020) and\n",
    "InstructGPT (Ouyang et al., 2022) series, as well\n",
    "as the recent LLaMA (Touvron et al., 2023) and\n",
    "its derivative models, namely Alpaca (Taori et al.,\n",
    "2023) and Vicuna (Chiang et al., 2023). Our in-\n",
    "vestigative approach employed three distinct input\n",
    "forms: Direct, Instruction, and In-Context Learn-\n",
    "ing (ICL), which is encapsulated in Appendix A.4.\n",
    "350M\n",
    "1.3B\n",
    "6.7B\n",
    "175B20\n",
    "30\n",
    "40\n",
    "50\n",
    "60\n",
    "70\n",
    "F1 Scores\n",
    "22.38\n",
    "40.11\n",
    "26.96\n",
    "40.33\n",
    "26.17\n",
    "43.47\n",
    "27.54\n",
    "44.87\n",
    "Direct\n",
    "350M\n",
    "1.3B\n",
    "6.7B\n",
    "175B20\n",
    "30\n",
    "40\n",
    "50\n",
    "60\n",
    "70\n",
    "F1 Scores\n",
    "30.42\n",
    "42.31\n",
    "30.17\n",
    "45.91\n",
    "33.33\n",
    "48.79\n",
    "45.67\n",
    "49.61\n",
    "Instruction\n",
    "350M\n",
    "1.3B\n",
    "6.7B\n",
    "175B20\n",
    "30\n",
    "40\n",
    "50\n",
    "60\n",
    "70\n",
    "F1 Scores\n",
    "34.27\n",
    "47.93\n",
    "36.27\n",
    "48.42 47.24\n",
    "55.81 55.5\n",
    "65.12\n",
    "In-Context Learning\n",
    "GPT-3\n",
    "InstructGPT\n",
    "ModelFigure 2: Experimental results using three different input forms on a series of models from GPT-3(ada, babbage,\n",
    "curie, and davinci) and InstructGPT(text-ada-001, text-babbage-001, text-curie-001, and text-davinci-001)0 10 20 30 40 50 60 70 80\n",
    "F1 Scores\n",
    "davinci\n",
    "text-davinci-001\n",
    "text-davinci-002\n",
    "text-davinci-003\n",
    "gpt-3.5-turbo-0301\n",
    "gpt-4-0314\n",
    "Human\n",
    "Models\n",
    "45.67\n",
    "49.61\n",
    "47.48\n",
    "51.43\n",
    "54.12\n",
    "75.47\n",
    "84.93\n",
    "Figure 3: Comparison between the davinci series and\n",
    "human self-knowledge in instruction input form.\n",
    "4.2 Setting\n",
    "We devised the reference sentence set U through\n",
    "a process that combined automated generation by\n",
    "LLMs and manual filtering, detailed further in Ap-\n",
    "pendix A.1. To quantify the similarity between\n",
    "target and reference sentences, we utilized Sim-\n",
    "CSE (Gao et al., 2021), setting the similarity thresh-\n",
    "old to 0.75 during our experiments. An exploration\n",
    "of threshold ablation is available in Appendix A.2.\n",
    "To counteract potential errors in similarity calcula-\n",
    "tion induced by varying lengths of the target and\n",
    "reference sentences, we employed a sliding win-\n",
    "dow of length 5 to parse the target sentence into\n",
    "semantic chunks. During the generation process,\n",
    "we set the temperature to 0.7. We selected a ran-\n",
    "dom sample of 100 instances for GPT-4, while the\n",
    "remainder of the models were scrutinized using the\n",
    "full SelfAware dataset.\n",
    "4.3 Human Self-Knowledge\n",
    "To establish a benchmark for human self-\n",
    "knowledge, we engaged two volunteers and se-\n",
    "lected 100 random samples from the SelfAware\n",
    "dataset. The volunteers has 30 minutes to makedavinci\n",
    "text-davinci-001\n",
    "text-davinci-002\n",
    "text-davinci-003\n",
    "gpt-3.5-turbo-0301\n",
    "Models\n",
    "0\n",
    "10\n",
    "20\n",
    "30\n",
    "40\n",
    "50\n",
    "60\n",
    "F1 Scores\n",
    "55.5\n",
    "65.12 66.46 66.28\n",
    "60.86\n",
    "Figure 4: Experimental comparison of davinci series in\n",
    "ICL input form.\n",
    "judgments on the same set of questions, yielding\n",
    "an average F1 score of 84.93%, which we sub-\n",
    "sequently adopted as the benchmark for human\n",
    "self-knowledge. Detailed scores are available in\n",
    "Appendix A.3.\n",
    "4.4 Analysis\n",
    "We evaluate the manifestation of LLMs’ self-\n",
    "knowledge, centering our investigation on three\n",
    "fundamental dimensions: the size of the model,\n",
    "the impact of instruction tuning, and the influence\n",
    "exerted by different input forms.\n",
    "Model Size. Figure 2 illustrates the correlation\n",
    "between model size and self-knowledge across var-\n",
    "ious LLMs. It is noteworthy that across all three\n",
    "input forms, an augmentation in model parameter\n",
    "size is associated with an elevation in the F1 Score,\n",
    "with the most conspicuous enhancement manifest-\n",
    "ing in the ICL input form. Therefore, our analysis\n",
    "indicates that an LLM’s self-knowledge tends to\n",
    "enhance with increasing model size, a trend consis-\n",
    "tent with the scaling law.\n",
    "LLaMA-7B\n",
    "Alpaca-7B\n",
    "Vicuna-7B\n",
    "LLaMA-13B\n",
    "Alpaca-13B\n",
    "Vicuna-13B\n",
    "LLaMA-30B\n",
    "LLaMA-65B\n",
    "Models\n",
    "0\n",
    "10\n",
    "20\n",
    "30\n",
    "40\n",
    "50\n",
    "F1 Scores\n",
    "28.57\n",
    "35.87\n",
    "42.78\n",
    "30.12\n",
    "37.44\n",
    "47.84\n",
    "30.3\n",
    "46.89Figure 5: Experimental results obtained from LLaMA\n",
    "and its derived models, Alpaca and Vicuna in instruction\n",
    "input form.\n",
    "Instruction Tuning. Figure 2 delineates that\n",
    "models from the InstructGPT series exhibit a su-\n",
    "perior level of self-knowledge compared to their\n",
    "GPT-3 counterparts. Further evidence of model\n",
    "enhancement is provided by Figure 4, where text-\n",
    "davinci models show significant improvement rela-\n",
    "tive to the base davinci model. An additional com-\n",
    "parative analysis, presented in Figure 5, evaluates\n",
    "LLaMA against its derivative models. The results\n",
    "underscore a notable increase in self-knowledge\n",
    "for Alpaca and Vicuna upon instruction tuning, ex-\n",
    "ceeding their base model performances. Among\n",
    "these, Vicuna-13B outperforms the LLaMA-65B,\n",
    "corroborating the efficacy of instruction tuning for\n",
    "enhancing model self-knowledge.\n",
    "Input Forms. As shown in Figure 2, the incorpo-\n",
    "ration of instructions and examples serves to boost\n",
    "the self-knowledge of both the GPT-3 and Instruct-\n",
    "GPT series. Specifically, ICL input form, providing\n",
    "richer contextual information, contributes to a sig-\n",
    "nificant enhancement in models’ self-knowledge.\n",
    "This impact is particularly noticeable in the davinci\n",
    "model, where ICL facilitates a 27.96% improve-\n",
    "ment over the direct. Moreover, a comparison be-\n",
    "tween Figure 3 and Figure 4 reveals that the in-\n",
    "clusion of instructions and examples successfully\n",
    "minimizes the performance disparity between the\n",
    "davinci and text-davinci models, suggesting an ac-\n",
    "quisition of self-knowledge from the instructions\n",
    "and provided examples.\n",
    "Compared with Human. Figure 3 reveals that,\n",
    "without supplementary samples, GPT-4 currently\n",
    "performs best among the tested models, achieving\n",
    "an impressive F1 score of 75.47%. However, a no-\n",
    "ticeable gap becomes evident when comparing thistext-ada-001\n",
    "text-babbage-001\n",
    "text-curie-001\n",
    "text-davinci-001\n",
    "text-davinci-002\n",
    "text-davinci-003\n",
    "gpt-3.5-turbo-0301\n",
    "gpt-4-0314\n",
    "Models\n",
    "0\n",
    "5\n",
    "10\n",
    "15\n",
    "20\n",
    "25\n",
    "30\n",
    "35\n",
    "40\n",
    "Accuracy\n",
    "2.48\n",
    "4.45 4.7\n",
    "10.61\n",
    "15.7\n",
    "30.25\n",
    "38.29\n",
    "42.64\n",
    "Figure 6: Accuracy of the InstructGPT series when\n",
    "responding to answerable questions in instruction input\n",
    "form.\n",
    "performance to the human benchmark of 84.93%.\n",
    "This underscores the considerable potential that re-\n",
    "mains for enhancing the self-knowledge level of\n",
    "LLMs.\n",
    "Answerable Questions. Figure 6 traces the per-\n",
    "formance evolution of the InstructGPT series in\n",
    "addressing answerable questions, adhering to the\n",
    "closed-book question answering paradigm (Tou-\n",
    "vron et al., 2023), where output accuracy is con-\n",
    "tingent on the presence of the correct answer. Our\n",
    "observations underscore a steady enhancement in\n",
    "QA task accuracy corresponding to an increase\n",
    "in model parameter size and continuous learning.\n",
    "Particularly, the accuracy of text-davinci-001 expe-\n",
    "riences a significant ascent, scaling from a meager\n",
    "2.48% in text-ada-001 to 10.61%, whereas GPT-4\n",
    "marks an even more striking jump to 42.64%.\n",
    "5 Conclusion\n",
    "This study investigates the self-knowledge of\n",
    "LLMs by evaluating their ability to identify unan-\n",
    "swerable questions. Through the introduction of a\n",
    "novel dataset and an automated method for detect-\n",
    "ing uncertainty in the models’ responses, we are\n",
    "able to accurately measure the self-knowledge of\n",
    "LLMs such as GPT-3, InstructGPT and LLaMA.\n",
    "Our results reveal that while these models possess\n",
    "a certain degree of self-knowledge, there is still\n",
    "an apparent disparity in comparison to human self-\n",
    "knowledge. This highlights the need for further\n",
    "research in this area to enhance the ability of LLMs\n",
    "to understand their own limitations on the unknows.\n",
    "Such efforts will lead to more accurate and reliable\n",
    "responses from LLMs, which will have a positive\n",
    "impact on their applications in diverse fields.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_gpt4(prompt_question, summary_output):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"\"\"You are a evaluation engine for summaries.\n",
    "                   You will be fed the prompt and the summary output and you will evaluate \n",
    "                   the output based on the following criteria:\n",
    "                   1. Relevance : Evaluates if the summary includes only important information and excludes redundancies.\n",
    "                   2. Coherence : Assesses the logical flow and organization of the summary.\n",
    "                   3. Consistency : Checks if the summary aligns with the facts in the source document.\n",
    "                   4. Fluency : Rates the grammar and readability of the summary. Your output should ONLY be a single\n",
    "                   integer representing the score from 1-10 averaging over the above criteria and nothing else.\n",
    "                   \"\"\"},\n",
    "                  {\"role\": \"user\", \"content\": f\"Evaluate this summary: {summary_output} based on the following prompt: {prompt_question}. Output score:\"}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "prompt_summary = \"\"\"\n",
    "4\n",
    "Do Large Language Models Know What They Don’t Know?\n",
    "Zhangyue Yin♢Qiushi Sun♠Qipeng Guo♢\n",
    "Jiawen Wu♢Xipeng Qiu♢∗Xuanjing Huang♢\n",
    "♢School of Computer Science, Fudan University♠Department of Mathematics, National University of Singapore\n",
    "{yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu\n",
    "{qpguo16,xpqiu,xjhuang}@fudan.edu.cn\n",
    "Abstract\n",
    "Large language models (LLMs) have a wealth\n",
    "of knowledge that allows them to excel in vari-\n",
    "ous Natural Language Processing (NLP) tasks.\n",
    "Current research focuses on enhancing their\n",
    "performance within their existing knowledge.\n",
    "Despite their vast knowledge, LLMs are still\n",
    "limited by the amount of information they can\n",
    "accommodate and comprehend. Therefore, the\n",
    "ability to understand their own limitations on\n",
    "the unknows, referred to as self-knowledge,\n",
    "is of paramount importance. This study aims\n",
    "to evaluate LLMs’ self-knowledge by assess-\n",
    "ing their ability to identify unanswerable or\n",
    "unknowable questions. We introduce an auto-\n",
    "mated methodology to detect uncertainty in the\n",
    "responses of these models, providing a novel\n",
    "measure of their self-knowledge. We further in-\n",
    "troduce a unique dataset, SelfAware, consisting\n",
    "of unanswerable questions from five diverse cat-\n",
    "egories and their answerable counterparts. Our\n",
    "extensive analysis, involving 20 LLMs includ-\n",
    "ing GPT-3, InstructGPT, and LLaMA, discov-\n",
    "ering an intrinsic capacity for self-knowledge\n",
    "within these models. Moreover, we demon-\n",
    "strate that in-context learning and instruction\n",
    "tuning can further enhance this self-knowledge.\n",
    "Despite this promising insight, our findings also\n",
    "highlight a considerable gap between the capa-\n",
    "bilities of these models and human proficiency\n",
    "in recognizing the limits of their knowledge.\n",
    "“True wisdom is knowing what you don’t know.”\n",
    "–Confucius\n",
    "1 Introduction\n",
    "Recently, Large Language Models (LLMs) such\n",
    "as GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,\n",
    "2023), and LLaMA (Touvron et al., 2023) have\n",
    "shown exceptional performance on a wide range\n",
    "of NLP tasks, including common sense reason-\n",
    "ing (Wei et al., 2022; Zhou et al., 2022) and mathe-\n",
    "∗Corresponding author.UnknowsKnowsUnknowsKnowsKnown Knows Known UnknowsUnknown UnknowsUnknown KnowsUnlock\n",
    "Figure 1: Know-Unknow Quadrant. The horizontal axis\n",
    "represents the model’s memory capacity for knowledge,\n",
    "and the vertical axis represents the model’s ability to\n",
    "comprehend and utilize knowledge.\n",
    "matical problem-solving (Lewkowycz et al., 2022;\n",
    "Chen et al., 2022). Despite their ability to learn\n",
    "from huge amounts of data, LLMs still have lim-\n",
    "itations in their capacity to retain and understand\n",
    "information. To ensure responsible usage, it is cru-\n",
    "cial for LLMs to have the capability of recognizing\n",
    "their limitations and conveying uncertainty when\n",
    "responding to unanswerable or unknowable ques-\n",
    "tions. This acknowledgment of limitations, also\n",
    "known as “knowing what you don’t know,” is a\n",
    "crucial aspect in determining their practical appli-\n",
    "cability. In this work, we refer to this ability as\n",
    "model self-knowledge.\n",
    "The Know-Unknow quadrant in Figure 1 il-\n",
    "lustrates the relationship between the model’s\n",
    "knowledge and comprehension. The ratio of\n",
    "“Known Knows” to “Unknown Knows” demon-\n",
    "strates the model’s proficiency in understanding\n",
    "and applying existing knowledge. Techniques\n",
    "such as Chain-of-Thought (Wei et al., 2022), Self-\n",
    "Consistency (Wang et al., 2022), and Complex\n",
    "CoT (Fu et al., 2022) can be utilized to increase\n",
    "arXiv:2305.18153v2  [cs.CL]  30 May 2023\n",
    "this ratio, resulting in improved performance on\n",
    "NLP tasks. We focus on the ratio of “Known Un-\n",
    "knows” to “Unknown Unknows”, which indicates\n",
    "the model’s self-knowledge level, specifically un-\n",
    "derstanding its own limitations and deficiencies in\n",
    "the unknows.\n",
    "Existing datasets such as SQuAD2.0 (Rajpurkar\n",
    "et al., 2018) and NewsQA (Trischler et al., 2017),\n",
    "widely used in question answering (QA), have been\n",
    "utilized to test the self-knowledge of models with\n",
    "unanswerable questions. However, these questions\n",
    "are context-specific and could become answerable\n",
    "when supplemented with additional information.\n",
    "Srivastava et al. (2022) attempted to address this by\n",
    "evaluating LLMs’ competence in delineating their\n",
    "knowledge boundaries, employing a set of 23 pairs\n",
    "of answerable and unanswerable multiple-choice\n",
    "questions. They discovered that these models’ per-\n",
    "formance barely surpassed that of random guessing.\n",
    "Kadavath et al. (2022) suggested probing the self-\n",
    "knowledge of LLMs through the implementation\n",
    "of a distinct \"Value Head\". Yet, this approach may\n",
    "encounter difficulties when applied across varied\n",
    "domains or tasks due to task-specific training. Con-\n",
    "sequently, we redirect our focus to the inherent\n",
    "abilities of LLMs, and pose the pivotal question:\n",
    "“Do large language models know what they don’t\n",
    "know?”.\n",
    "In this study, we investigate the self-knowledge\n",
    "of LLMs using a novel approach. By gathering\n",
    "reference sentences with uncertain meanings, we\n",
    "can determine whether the model’s responses re-\n",
    "flect uncertainty using a text similarity algorithm.\n",
    "We quantified the model’s self-knowledge using\n",
    "the F1 score. To address the small and idiosyn-\n",
    "cratic limitations of existing datasets, we created\n",
    "a new dataset called SelfAware. This dataset com-\n",
    "prises 1,032 unanswerable questions, which are dis-\n",
    "tributed across five distinct categories, along with\n",
    "an additional 2,337 questions that are classified as\n",
    "answerable. Experimental results on GPT-3, In-\n",
    "structGPT, LLaMA, and other LLMs demonstrate\n",
    "that in-context learning and instruction tuning can\n",
    "effectively enhance the self-knowledge of LLMs.\n",
    "However, the self-knowledge exhibited by the cur-\n",
    "rent state-of-the-art model, GPT-4, measures at\n",
    "75.47%, signifying a notable disparity when con-\n",
    "trasted with human self-knowledge, which is rated\n",
    "at 84.93%.\n",
    "Our key contributions to this field are summa-\n",
    "rized as follows:\n",
    "• We have developed a new dataset, SelfAware,\n",
    "that comprises a diverse range of commonly\n",
    "posed unanswerable questions.\n",
    "• We propose an innovative evaluation tech-\n",
    "nique based on text similarity to quantify the\n",
    "degree of uncertainty inherent in model out-\n",
    "puts.\n",
    "• Through our detailed analysis of 20 LLMs,\n",
    "benchmarked against human self-knowledge,\n",
    "we identified a significant disparity between\n",
    "the most advanced LLMs and humans 1.\n",
    "2 Dataset Construction\n",
    "To conduct a more comprehensive evaluation of\n",
    "the model’s self-knowledge, we constructed a\n",
    "dataset that includes a larger number and more di-\n",
    "verse types of unanswerable questions than Know-\n",
    "Unknowns dataset (Srivastava et al., 2022). To\n",
    "facilitate this, we collected a corpus of 2,858 unan-\n",
    "swerable questions, sourced from online platforms\n",
    "like Quora and HowStuffWorks. These questions\n",
    "were meticulously evaluated by three seasoned an-\n",
    "notation analysts, each operating independently.\n",
    "The analysts were permitted to leverage external\n",
    "resources, such as search engines. To ensure the va-\n",
    "lidity of our dataset, we retained only the questions\n",
    "that all three analysts concurred were unanswerable.\n",
    "This rigorous process yielded a finalized collection\n",
    "of 1,032 unanswerable questions.\n",
    "In pursuit of a comprehensive evaluation, we\n",
    "opted for answerable questions drawn from three\n",
    "datasets: SQuAD (Rajpurkar et al., 2016), Hot-\n",
    "potQA (Yang et al., 2018), and TriviaQA (Joshi\n",
    "et al., 2017). Our selection was guided by Sim-\n",
    "CSE (Gao et al., 2021), which allowed us to iden-\n",
    "tify and select the answerable questions semanti-\n",
    "cally closest to the unanswerable ones. From these\n",
    "sources, we accordingly drew samples of 1,487,\n",
    "182, and 668 questions respectively, amassing a\n",
    "total of 2,337. Given that these questions can be\n",
    "effectively addressed using information available\n",
    "on Wikipedia, the foundational corpus for the train-\n",
    "ing of current LLMs, it is plausible to infer that\n",
    "the model possesses the requisite knowledge to\n",
    "generate accurate responses to these questions.\n",
    "Our dataset, christened SelfAware, incorporates\n",
    "1,032 unanswerable and 2,337 answerable ques-\n",
    "tions. To reflect real-world distribution, our dataset\n",
    "1The code pertinent to our study can be accessed\n",
    "https://github.com/yinzhangyue/SelfAware\n",
    "Category Description Example Percentage\n",
    "No scientific\n",
    "consensus\n",
    "The answer is still up\n",
    "for debate, with no consensus\n",
    "in scientific community.\n",
    "“Are we alone in the universe,\n",
    "or will we discover alien\n",
    "life at some point?”\n",
    "25%\n",
    "Imagination The question are about people’s\n",
    "imaginations of the future.\n",
    "\"What will the fastest form of\n",
    "transportation be in 2050?\" 15%\n",
    "Completely\n",
    "subjective\n",
    "The answer depends on\n",
    "personal preference.\n",
    "\"Would you rather be shot\n",
    "into space or explore the\n",
    "deepest depths of the sea?\"\n",
    "27%\n",
    "Too many\n",
    "variables\n",
    "The question with too\n",
    "many variables cannot\n",
    "be answered accurately.\n",
    "“John made 6 dollars mowing lawns\n",
    "and 18 dollars weed eating.\n",
    "If he only spent 3 or 5 dollar a week,\n",
    "how long would the money last him?”\n",
    "10%\n",
    "Philosophical\n",
    "The question can yield\n",
    "multiple responses, but it\n",
    "lacks a definitive answer.\n",
    "“How come god was\n",
    "born from nothingness?” 23%\n",
    "Table 1: Unanswerable questions in the SelfAware dataset that span across multiple categories.\n",
    "contains a proportion of answerable questions that\n",
    "is twice as large as the volume of unanswerable\n",
    "ones. Nevertheless, to ensure the feasibility of test-\n",
    "ing, we have purposefully capped the number of\n",
    "answerable questions.\n",
    "2.1 Dataset Analysis\n",
    "To gain insight into the reasons precluding a cer-\n",
    "tain answer, we undertook a manual analysis of\n",
    "100 randomly selected unanswerable questions. As\n",
    "tabulated in Table 1, we have broadly segregated\n",
    "these questions into five distinctive categories. “No\n",
    "Scientific Consensus\" encapsulates questions that\n",
    "ignite ongoing debates within the scientific com-\n",
    "munity, such as those concerning the universe’s\n",
    "origin. “Imagination\" includes questions involving\n",
    "speculative future scenarios, like envisaged events\n",
    "over the next 50 years. “Completely Subjective\"\n",
    "comprises questions that are inherently personal,\n",
    "where answers depend heavily on individual predis-\n",
    "positions. “Too Many Variables\" pertains to mathe-\n",
    "matical problems that become unsolvable owing to\n",
    "the overwhelming prevalence of variables. Lastly,\n",
    "“Philosophical\" represents questions of a profound,\n",
    "often metaphysical, nature that resist concrete an-\n",
    "swers. Ideally, upon encountering such questions,\n",
    "the model should express uncertainty instead of\n",
    "delivering conclusive responses.\n",
    "3 Evaluation Method\n",
    "This section elucidates the methodology employed\n",
    "for assessing self-knowledge in the generated text.\n",
    "In order to achieve this, we define a similarity func-\n",
    "tion, fsim, to compute the similarity, S, between\n",
    "a given sentence, t, and a collection of reference\n",
    "sentences, U ={u1, u2, . . . , un}, endowed with\n",
    "uncertain meanings.\n",
    "Si =fsim(t, ui). (1)\n",
    "Whenever any Si surpasses a pre-determined\n",
    "threshold T, we perceive the text t as encompass-\n",
    "ing uncertain meanings, thereby eliminating the\n",
    "need for manual evaluation of the response.\n",
    "Given the substantial disparity in the volume of\n",
    "answerable and unanswerable questions in Self-\n",
    "Aware, we adopt the F1 score as a measure of\n",
    "LLMs’ self-knowledge. Our focus rests on identi-\n",
    "fying unanswerable questions, hence we designate\n",
    "them as positive cases and categorize answerable\n",
    "questions as negative cases.\n",
    "4 Experiment\n",
    "4.1 Model\n",
    "We conduct a sequence of experiments to evaluate\n",
    "the degree of self-knowledge manifested by various\n",
    "LLMs, including GPT-3 (Brown et al., 2020) and\n",
    "InstructGPT (Ouyang et al., 2022) series, as well\n",
    "as the recent LLaMA (Touvron et al., 2023) and\n",
    "its derivative models, namely Alpaca (Taori et al.,\n",
    "2023) and Vicuna (Chiang et al., 2023). Our in-\n",
    "vestigative approach employed three distinct input\n",
    "forms: Direct, Instruction, and In-Context Learn-\n",
    "ing (ICL), which is encapsulated in Appendix A.4.\n",
    "\n",
    "Figure 3: Comparison between the davinci series and\n",
    "human self-knowledge in instruction input form.\n",
    "4.2 Setting\n",
    "We devised the reference sentence set U through\n",
    "a process that combined automated generation by\n",
    "LLMs and manual filtering, detailed further in Ap-\n",
    "pendix A.1. To quantify the similarity between\n",
    "target and reference sentences, we utilized Sim-\n",
    "CSE (Gao et al., 2021), setting the similarity thresh-\n",
    "old to 0.75 during our experiments. An exploration\n",
    "of threshold ablation is available in Appendix A.2.\n",
    "To counteract potential errors in similarity calcula-\n",
    "tion induced by varying lengths of the target and\n",
    "reference sentences, we employed a sliding win-\n",
    "dow of length 5 to parse the target sentence into\n",
    "semantic chunks. During the generation process,\n",
    "we set the temperature to 0.7. We selected a ran-\n",
    "dom sample of 100 instances for GPT-4, while the\n",
    "remainder of the models were scrutinized using the\n",
    "full SelfAware dataset.\n",
    "4.3 Human Self-Knowledge\n",
    "To establish a benchmark for human self-\n",
    "knowledge, we engaged two volunteers and se-\n",
    "lected 100 random samples from the SelfAware\n",
    "dataset. The volunteers has 30 minutes to makedavinci\n",
    "text-davinci-001\n",
    "text-davinci-002\n",
    "text-davinci-003\n",
    "gpt-3.5-turbo-0301\n",
    "Models\n",
    "0\n",
    "10\n",
    "20\n",
    "30\n",
    "40\n",
    "50\n",
    "60\n",
    "F1 Scores\n",
    "55.5\n",
    "65.12 66.46 66.28\n",
    "60.86\n",
    "Figure 4: Experimental comparison of davinci series in\n",
    "ICL input form.\n",
    "judgments on the same set of questions, yielding\n",
    "an average F1 score of 84.93%, which we sub-\n",
    "sequently adopted as the benchmark for human\n",
    "self-knowledge. Detailed scores are available in\n",
    "Appendix A.3.\n",
    "4.4 Analysis\n",
    "We evaluate the manifestation of LLMs’ self-\n",
    "knowledge, centering our investigation on three\n",
    "fundamental dimensions: the size of the model,\n",
    "the impact of instruction tuning, and the influence\n",
    "exerted by different input forms.\n",
    "Model Size. Figure 2 illustrates the correlation\n",
    "between model size and self-knowledge across var-\n",
    "ious LLMs. It is noteworthy that across all three\n",
    "input forms, an augmentation in model parameter\n",
    "size is associated with an elevation in the F1 Score,\n",
    "with the most conspicuous enhancement manifest-\n",
    "ing in the ICL input form. Therefore, our analysis\n",
    "indicates that an LLM’s self-knowledge tends to\n",
    "enhance with increasing model size, a trend consis-\n",
    "tent with the scaling law.\n",
    "LLaMA-7B\n",
    "Alpaca-7B\n",
    "Vicuna-7B\n",
    "LLaMA-13B\n",
    "Alpaca-13B\n",
    "Vicuna-13B\n",
    "LLaMA-30B\n",
    "LLaMA-65B\n",
    "Models\n",
    "0\n",
    "10\n",
    "20\n",
    "30\n",
    "40\n",
    "50\n",
    "F1 Scores\n",
    "28.57\n",
    "35.87\n",
    "42.78\n",
    "30.12\n",
    "37.44\n",
    "47.84\n",
    "30.3\n",
    "46.89Figure 5: Experimental results obtained from LLaMA\n",
    "and its derived models, Alpaca and Vicuna in instruction\n",
    "input form.\n",
    "Instruction Tuning. Figure 2 delineates that\n",
    "models from the InstructGPT series exhibit a su-\n",
    "perior level of self-knowledge compared to their\n",
    "GPT-3 counterparts. Further evidence of model\n",
    "enhancement is provided by Figure 4, where text-\n",
    "davinci models show significant improvement rela-\n",
    "tive to the base davinci model. An additional com-\n",
    "parative analysis, presented in Figure 5, evaluates\n",
    "LLaMA against its derivative models. The results\n",
    "underscore a notable increase in self-knowledge\n",
    "for Alpaca and Vicuna upon instruction tuning, ex-\n",
    "ceeding their base model performances. Among\n",
    "these, Vicuna-13B outperforms the LLaMA-65B,\n",
    "corroborating the efficacy of instruction tuning for\n",
    "enhancing model self-knowledge.\n",
    "Input Forms. As shown in Figure 2, the incorpo-\n",
    "ration of instructions and examples serves to boost\n",
    "the self-knowledge of both the GPT-3 and Instruct-\n",
    "GPT series. Specifically, ICL input form, providing\n",
    "richer contextual information, contributes to a sig-\n",
    "nificant enhancement in models’ self-knowledge.\n",
    "This impact is particularly noticeable in the davinci\n",
    "model, where ICL facilitates a 27.96% improve-\n",
    "ment over the direct. Moreover, a comparison be-\n",
    "tween Figure 3 and Figure 4 reveals that the in-\n",
    "clusion of instructions and examples successfully\n",
    "minimizes the performance disparity between the\n",
    "davinci and text-davinci models, suggesting an ac-\n",
    "quisition of self-knowledge from the instructions\n",
    "and provided examples.\n",
    "Compared with Human. Figure 3 reveals that,\n",
    "without supplementary samples, GPT-4 currently\n",
    "performs best among the tested models, achieving\n",
    "an impressive F1 score of 75.47%. However, a no-\n",
    "ticeable gap becomes evident when comparing thistext-ada-001\n",
    "text-babbage-001\n",
    "text-curie-001\n",
    "text-davinci-001\n",
    "text-davinci-002\n",
    "text-davinci-003\n",
    "gpt-3.5-turbo-0301\n",
    "gpt-4-0314\n",
    "Models\n",
    "0\n",
    "5\n",
    "10\n",
    "15\n",
    "20\n",
    "25\n",
    "30\n",
    "35\n",
    "40\n",
    "Accuracy\n",
    "2.48\n",
    "4.45 4.7\n",
    "10.61\n",
    "15.7\n",
    "30.25\n",
    "38.29\n",
    "42.64\n",
    "Figure 6: Accuracy of the InstructGPT series when\n",
    "responding to answerable questions in instruction input\n",
    "form.\n",
    "performance to the human benchmark of 84.93%.\n",
    "This underscores the considerable potential that re-\n",
    "mains for enhancing the self-knowledge level of\n",
    "LLMs.\n",
    "Answerable Questions. Figure 6 traces the per-\n",
    "formance evolution of the InstructGPT series in\n",
    "addressing answerable questions, adhering to the\n",
    "closed-book question answering paradigm (Tou-\n",
    "vron et al., 2023), where output accuracy is con-\n",
    "tingent on the presence of the correct answer. Our\n",
    "observations underscore a steady enhancement in\n",
    "QA task accuracy corresponding to an increase\n",
    "in model parameter size and continuous learning.\n",
    "Particularly, the accuracy of text-davinci-001 expe-\n",
    "riences a significant ascent, scaling from a meager\n",
    "2.48% in text-ada-001 to 10.61%, whereas GPT-4\n",
    "marks an even more striking jump to 42.64%.\n",
    "5 Conclusion\n",
    "This study investigates the self-knowledge of\n",
    "LLMs by evaluating their ability to identify unan-\n",
    "swerable questions. Through the introduction of a\n",
    "novel dataset and an automated method for detect-\n",
    "ing uncertainty in the models’ responses, we are\n",
    "able to accurately measure the self-knowledge of\n",
    "LLMs such as GPT-3, InstructGPT and LLaMA.\n",
    "Our results reveal that while these models possess\n",
    "a certain degree of self-knowledge, there is still\n",
    "an apparent disparity in comparison to human self-\n",
    "knowledge. This highlights the need for further\n",
    "research in this area to enhance the ability of LLMs\n",
    "to understand their own limitations on the unknows.\n",
    "Such efforts will lead to more accurate and reliable\n",
    "responses from LLMs, which will have a positive\n",
    "impact on their applications in diverse fields.\n",
    "\"\"\"\n",
    "\n",
    "summary_output = summarize(prompt_question=prompt_summary)\n",
    "eval_score = evaluate_with_gpt4(prompt_question=prompt_summary, summary_output=summary_output)\n",
    "print(eval_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\n    \"Can you summarize the key points of this AI research paper into a bullet-point list suitable for someone with no technical background?\",\\n    \"Please convert the main findings and purpose of this AI paper into an easy-to-understand bullet-point summary for a layperson.\",\\n    \"Create a simple bullet-point summary of this AI study, focusing on its goals, methods, and implications, aimed at non-technical readers.\",\\n    \"I have an AI paper that I need explained in simple terms. Can you provide a bullet-point summary highlighting the major themes and outcomes for a general audience?\",\\n    \"Transform the complex contents of this AI research paper into a concise, bullet-point summary designed for readers without a technical background.\"\\n]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def get_response(prompt_question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are an expert prompt engineer, specialized in producing amazing prompts for tasks using LLMs.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "task_definition = \"\"\"Bullet point summary of an AI paper for a non-technical audience.\"\"\"\n",
    "\n",
    "output_indicator = \"Your output should ONLY be a PYTHON list with the prompt ideas\"\n",
    "\n",
    "prompt = f\"\"\"Generate 5 ideas for prompts to send to ChatGPT to solve the folowing task: {task_definition}. \\n {output_indicator}.\"\"\"\n",
    "\n",
    "prompt_ideas = get_response(prompt)\n",
    "prompt_ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Can you summarize the key points of this AI research paper into a bullet-point list suitable for someone with no technical background?',\n",
       " 'Please convert the main findings and purpose of this AI paper into an easy-to-understand bullet-point summary for a layperson.',\n",
       " 'Create a simple bullet-point summary of this AI study, focusing on its goals, methods, and implications, aimed at non-technical readers.',\n",
       " 'I have an AI paper that I need explained in simple terms. Can you provide a bullet-point summary highlighting the major themes and outcomes for a general audience?',\n",
       " 'Transform the complex contents of this AI research paper into a concise, bullet-point summary designed for readers without a technical background.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "prompt_ideas_list = literal_eval(prompt_ideas)\n",
    "prompt_ideas_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>summary-score</th>\n",
       "      <th>model</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [prompt, summary-score, model, output]\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['prompt', 'summary-score', 'model', 'output'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompt_ideas_list:\n",
    "    summary_output = summarize(prompt_question=prompt)\n",
    "    eval_score = evaluate_with_gpt4(prompt_question=prompt, summary_output=summary_output)\n",
    "    eval_score = literal_eval(eval_score)\n",
    "    df.loc[len(df)] = [prompt, eval_score, 'gpt-3.5-turbo-0125', summary_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>summary-score</th>\n",
       "      <th>model</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can you summarize the key points of this AI re...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>Of course! Please provide the AI research pape...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Please convert the main findings and purpose o...</td>\n",
       "      <td>7</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>Title: Enhancing Image Classification with Gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Create a simple bullet-point summary of this A...</td>\n",
       "      <td>8</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>- **Goals**:\\n  - To investigate the impact of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have an AI paper that I need explained in si...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>Sure! Please provide the AI paper, and I will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transform the complex contents of this AI rese...</td>\n",
       "      <td>8</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>Title: \"Advancements in Natural Language Proce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  summary-score  \\\n",
       "0  Can you summarize the key points of this AI re...              1   \n",
       "1  Please convert the main findings and purpose o...              7   \n",
       "2  Create a simple bullet-point summary of this A...              8   \n",
       "3  I have an AI paper that I need explained in si...              1   \n",
       "4  Transform the complex contents of this AI rese...              8   \n",
       "\n",
       "                model                                             output  \n",
       "0  gpt-3.5-turbo-0125  Of course! Please provide the AI research pape...  \n",
       "1  gpt-3.5-turbo-0125  Title: Enhancing Image Classification with Gen...  \n",
       "2  gpt-3.5-turbo-0125  - **Goals**:\\n  - To investigate the impact of...  \n",
       "3  gpt-3.5-turbo-0125  Sure! Please provide the AI paper, and I will ...  \n",
       "4  gpt-3.5-turbo-0125  Title: \"Advancements in Natural Language Proce...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Create a simple bullet-point summary of this AI study, focusing on its goals, methods, and implications, aimed at non-technical readers.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_prompt = df.loc[df['summary-score'].idxmax()]['prompt']\n",
    "best_prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-prompt-eng",
   "language": "python",
   "name": "oreilly-prompt-eng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
