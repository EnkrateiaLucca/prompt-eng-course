{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to set your OpenAI API key in the .env file\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Fun Use Case! \n",
    "\n",
    "## __Understanding Papers with Prompt Engineering__\n",
    "\n",
    "### The Five High-Level Questions\n",
    "\n",
    "1. What problem is the paper trying to solve?\n",
    "2. Why is the problem interesting?\n",
    "3. What is the primary contribution?\n",
    "4. How did they do it?\n",
    "5. What are the key take-aways?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def get_response(prompt_question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and programming assistant\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper \"Towards A Human-in-the-Loop LLM Approach to Collaborative Discourse Analysis\" primarily explores the integration of a human-in-the-loop (HITL) approach utilizing a large language model (LLM) for the analysis of collaborative discourse, specifically within the context of students engaging in synergistic learning processes in STEM+C (Science, Technology, Engineering, Mathematics, and Computing) education.\\n\\nThe authors present an exploratory study using the GPT-4-Turbo model to summarize and categorize the synergistic learning observed during students\\' collaborative discourse, particularly focusing on how they integrate physics and computing concepts in their dialogues during problem-solving tasks. Such an approach seeks to address the labor-intensive nature of manually analyzing student interactions, aiming for a more automated, yet insightful, analysis that could ideally match human evaluators in effectiveness.\\n\\nThe study includes the design and implementation of a method combining CoT (Chain-of-Thought Prompting) and Active Learning techniques. The paper details how the discourse is segmented and analyzed, utilizing the LLM to generate summaries that are compared against those produced by human experts to evaluate the LLM\\'s effectiveness and pinpoint its strengths and limitations.\\n\\nThe initial findings, based on a limited dataset and exploratory analysis, suggest that the LLM could potentially perform comparably to human evaluators in recognizing and summarizing the synergistic learning aspects of student discourse. The hope is that such technology could eventually assist educators by providing actionable insights that are derived in a more efficient manner, thereby enhancing educational outcomes without the extensive manual effort currently required.\\n\\nIn sum, the main idea of the paper is the investigation and development of a human-in-the-loop approach using an LLM to improve the analysis and understanding of synergistic learning in collaborative student discourse within STEM+C education, paving the way for more automated systems that can support educators in real-time learning environments.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_contents = \"\"\"\n",
    "\n",
    "1\n",
    "Towards A Human-in-the-Loop LLM Approach to\n",
    "Collaborative Discourse Analysis\n",
    "Clayton Cohn1[0000−0003−0856−9587], Caitlin Snyder1[0000−0002−3341−0490], Justin\n",
    "Montenegro2, and Gautam Biswas1[0000−0002−2752−3878]\n",
    "1 Vanderbilt University, Nashville, TN 37240, USA\n",
    "clayton.a.cohn@vanderbilt.edu2 Martin Luther King, Jr. Academic Magnet High School, Nashville, TN 37203, USA\n",
    "Abstract. LLMs have demonstrated proficiency in contextualizing their\n",
    "outputs using human input, often matching or beating human-level per-\n",
    "formance on a variety of tasks. However, LLMs have not yet been used to\n",
    "characterize synergistic learning in students’ collaborative discourse. In\n",
    "this exploratory work, we take a first step towards adopting a human-in-\n",
    "the-loop prompt engineering approach with GPT-4-Turbo to summarize\n",
    "and categorize students’ synergistic learning during collaborative dis-\n",
    "course. Our preliminary findings suggest GPT-4-Turbo may be able to\n",
    "characterize students’ synergistic learning in a manner comparable to\n",
    "humans and that our approach warrants further investigation.\n",
    "Keywords: LLM ·Collaborative Learning ·Human-in-the-Loop ·Dis-\n",
    "course Analysis ·K12 STEM.\n",
    "1 Introduction\n",
    "Computational modeling of scientific processes has been shown to effectively\n",
    "foster students’ Science, Technology, Engineering, Mathematics, and Comput-\n",
    "ing (STEM+C) learning [5], but task success necessitates synergistic learning\n",
    "(i.e., the simultaneous development and application of science and computing\n",
    "knowledge to address modeling tasks), which can lead to student difficulties\n",
    "[1]. Research has shown that problem-solving environments promoting synergis-\n",
    "tic learning in domains such as physics and computing often facilitate a bet-\n",
    "ter understanding of physics and computing concepts and practices when com-\n",
    "pared to students taught via a traditional curriculum [5]. Analyzing students’\n",
    "collaborative discourse offers valuable insights into their application of both\n",
    "domains’ concepts as they construct computational models [8]. Unfortunately,\n",
    "manually analyzing students’ discourse to identify their synergistic processes is\n",
    "time-consuming, and programmatic approaches are needed.\n",
    "In this paper, we take an exploratory first step towards adopting a human-in-\n",
    "the-loop LLM approach from previous work called Chain-of-Thought Prompting\n",
    "+ Active Learning [3] (detailed in Section 3) to characterize the synergistic con-\n",
    "tent in students’ collaborative discourse. We use a large language model (LLM)\n",
    "to summarize conversation segments in terms of how physics and computing\n",
    "arXiv:2405.03677v1  [cs.CL]  6 May 2024\n",
    "2 C. Cohn et al.\n",
    "concepts are interwoven to support students’ model building and debugging\n",
    "tasks. We evaluate our approach by comparing the LLM’s summaries to human-\n",
    "produced ones (using an expert human evaluator to rank them) and by qualita-\n",
    "tively analyzing the summaries to discern the LLM’s strengths and weaknesses\n",
    "alongside a physics and computer science teacher (the Educator) with experi-\n",
    "ence teaching the C2STEM curriculum (see Section 3.1). Within this framework,\n",
    "we analyze data from high school students working in pairs to build kinematics\n",
    "models and answer the following research questions: RQ1) How does the quality\n",
    "of human- and LLM-generated summaries and synergistic learning characteri-\n",
    "zations of collaborative student discourse compare?, and RQ2) What are the\n",
    "LLM’s strengths, and where does it struggle, in summarizing and characterizing\n",
    "synergistic learning in physics and computing?\n",
    "As this work is exploratory, due to the small sample size, we aim not to\n",
    "present generalizable findings but hope that our results will inform subsequent\n",
    "research as we work towards forging a human-AI partnership by providing teach-\n",
    "ers with actionable, LLM-generated feedback and recommendations to help them\n",
    "guide students in their synergistic learning.\n",
    "2 Background\n",
    "Roschelle and Teasley [7] define collaboration as “a coordinated, synchronous ac-\n",
    "tivity that is a result of a continuous attempt to construct and maintain a shared\n",
    "conception of a problem.” This development of a shared conceptual understand-\n",
    "ing necessitates multi-faceted collaborative discourse across multiple dimensions:\n",
    "social (e.g., navigating the social intricacies of forming a consensus [12]), cogni-\n",
    "tive (e.g., the development of context-specific knowledge [8]), and metacognitive\n",
    "(e.g., socially shared regulation [4]). Researchers have developed and leveraged\n",
    "frameworks situated within learning theory to classify and analyze collaborative\n",
    "problem solving (CPS) both broadly (i.e., across dimensions [6]) and narrowly\n",
    "(i.e., by focusing on one CPS aspect to gain in-depth insight, e.g., argumentative\n",
    "knowledge construction [12]). In this paper, we focus on one dimension of CPS\n",
    "that is particularly important to the context of STEM+C learning: students’\n",
    "cognitive integration of synergistic domains.\n",
    "Leveraging CPS frameworks to classify student discourse has traditionally\n",
    "been done through hand-coding utterances. However, this is time-consuming\n",
    "and laborious, leading researchers to leverage automated classification methods\n",
    "such as rule-based approaches, supervised machine learning methods, and (more\n",
    "recently) LLMs [10]. Utilizing LLMs can help extend previous work on classifying\n",
    "synergistic learning discourse, which has primarily relied on the frequency counts\n",
    "of domain-specific concept codes [8,5]. In particular, the use of LLMs can help\n",
    "address the following difficulties encountered while employing traditional meth-\n",
    "ods: (1) concept codes are difficult to identify programmatically, as rule-based\n",
    "approaches like regular expressions (regex) have difficulties with misspellings\n",
    "and homonyms; (2) the presence or absence of concept codes is not analyzed in\n",
    "a conversational context; and (3) the presence of cross-domain concept codes is\n",
    "Towards A HITL LLM Approach to Collaborative Discourse Analysis 3\n",
    "not necessarily indicative of synergistic learning, as synergistic learning requires\n",
    "students to form connections between concepts in both domains.\n",
    "Recent advances in LLM performance capabilities have allowed researchers\n",
    "to find new and creative ways to apply these powerful models to education using\n",
    "in-context learning (ICL) [2] (i.e., providing the LLM with labeled instances dur-\n",
    "ing inference) in lieu of traditional training that requires expensive parameter\n",
    "updates. One prominent extension of ICL is chain-of-thought reasoning (CoT)\n",
    "[11], which augments the labeled instances with “reasoning chains” that explain\n",
    "the rationale behind the correct answer and help guide the LLM towards the cor-\n",
    "rect solution. Recent work has found success in leveraging CoT towards scoring\n",
    "and explaining students’ formative assessment responses in the Earth Science\n",
    "domain [3]. In this work, we investigate this approach as a means to summarize\n",
    "and characterize synergistic learning in students’ collaborative discourse.\n",
    "3 Methods\n",
    "This paper extends the previous work of 1) Snyder et al. on log-segmented dis-\n",
    "course summarization defined by students’ model building segments extracted\n",
    "from their activity logs [9], and 2) Cohn et al. on a human-in-the-loop prompt\n",
    "engineering approach called Chain-of-Thought Prompting + Active Learning [3]\n",
    "(the Method) for scoring and explaining students’ science formative assessment\n",
    "responses. The original Method is a three-step process: 1) Response Scoring,\n",
    "where two human reviewers manually label a sample of students’ formative as-\n",
    "sessment responses and identify disagreements (i.e., sticking points) the LLM\n",
    "may similarly struggle with; 2) Prompt Development, which employs few-shot\n",
    "CoT prompting to address the sticking points and help align the LLM with\n",
    "the humans’ scoring consensus; and 3) Active Learning, where a knowledgeable\n",
    "human (e.g., a domain expert, researcher, or instructor) acts as an “oracle” and\n",
    "identifies the LLM’s reasoning errors on a validation set, then appends additional\n",
    "few-shot instances that the LLM struggled with to the prompt and uses CoT\n",
    "reasoning to help correct the LLM’s misconceptions. We illustrate the Method\n",
    "in Figure 1. For a complete description of the Method, please see [3].\n",
    "In this work, we combine log-based discourse segmentation [9] and CoT\n",
    "prompting [3] to generate more contextualized summaries of students’ discourse\n",
    "segments to study students’ synergistic learning processes by linking their model\n",
    "construction and debugging activities with their conversations during each probl-\n",
    "em-solving segment. We provide Supplementary Materials3that include 1) addi-\n",
    "tional information about the learning environment, 2) method application details\n",
    "(including our final prompt and few-shot example selection methodology), 3) a\n",
    "more in depth look at our conversation with the Educator, and 4) a more detailed\n",
    "analysis of the LLM’s strengths and weaknesses while applying the Method.\n",
    "3https://github.com/oele-isis-vanderbilt/AIED24_LBR\n",
    "4 C. Cohn et al.\n",
    "3.1 STEM+C Learning Environment, Curriculum, and Data\n",
    "Our work in this paper centers on the C2STEM learning environment [5], where\n",
    "students learn kinematics by building computational models of the 1- and 2-D\n",
    "motion of objects. C2STEM combines block-based programming with domain-\n",
    "specific modeling blocks to support the development and integration of science\n",
    "and computing knowledge as students create partial or complete models that\n",
    "simulate behaviors governed by scientific principles. This paper focuses on the\n",
    "1-D Truck Task, where students use their knowledge of kinematic equations to\n",
    "model the motion of a truck that starts from rest, accelerates to a speed limit,\n",
    "cruises at that speed, then decelerates to come to a stop at a stop sign.\n",
    "Our study, approved by our university Institutional Review Board, included\n",
    "26 consented high school students (aged 14-15) who completed the C2STEM\n",
    "kinematics curriculum. Students’ demographic information was not collected as\n",
    "part of this study (we began collecting it in later studies). Data collection in-\n",
    "cluded logged actions in the C2STEM environment, saved project files, and video\n",
    "and audio data (collected using laptop webcams and OBS software). Our data\n",
    "analysis included 9 dyads (one group had a student who did not consent to\n",
    "data collection, so we did not analyze that group; and we had technical issues\n",
    "with audio data from other groups). The dataset includes 9 hours of discourse\n",
    "transcripts and over 2,000 logged actions collected during one day of the study.\n",
    "Student discourse was transcribed using Otter.ai and edited for accuracy.\n",
    "3.2 Approach\n",
    "We extend the Method, previously used for formative assessment scoring and\n",
    "feedback, to prompt GPT-4-Turbo to summarize segments of students’ discourse\n",
    "and identify the Discourse Category (defined momentarily) by answering the fol-\n",
    "lowing question: “Given a discourse segment, and its environment task context\n",
    "and actions, is the students’ conversation best characterized as physics-focused\n",
    "(i.e., the conversation is primarily focused on the physics domain), computing-\n",
    "focused (i.e., the conversation is primarily focused on the computing domain),\n",
    "physics-and-computing-synergistic (i.e., students discuss concepts from both do-\n",
    "mains, interleaving them throughout the conversation, and making connections\n",
    "Fig. 1. Chain-of-Thought Prompting + Active Learning, identified by the green box,\n",
    "where each blue diamond is a step in the Method. Yellow boxes represent the process’s\n",
    "application to the classroom detailed in prior work [3].\n",
    "Towards A HITL LLM Approach to Collaborative Discourse Analysis 5\n",
    "between them), or physics-and-computing-separate (i.e., students discuss both\n",
    "domains but do so separately without interleaving)?” We use the recently re-\n",
    "leased GPT-4-Turbo LLM (gpt-4-0125-preview) because it provides an extended\n",
    "context window (128,000 tokens).\n",
    "We selected 10 training instances and 12 testing instances (10 additional\n",
    "segments were used as a validation set to perform Active Learning) prior to\n",
    "Response Scoring, using stratified sampling to approximate a uniform distribu-\n",
    "tion across Discourse Categories for both the train and test sets. Note that the\n",
    "student discourse was segmented based on which element of the model the stu-\n",
    "dents were working on (identified automatically via log data). During Response\n",
    "Scoring, the first two authors of this paper (Reviewers R1 and R2, respectively)\n",
    "independently evaluated the training set segments, classifying each segment as\n",
    "belonging to one of the four Discourse Categories. For each segment the Re-\n",
    "viewers disagreed on, the reason for disagreement was noted as a sticking point,\n",
    "and the segment was discussed until a consensus was reached on the specific\n",
    "Discourse Category for that segment. R1 and R2 initially struggled to agree on\n",
    "segments’ Discourse Categories (Cohen’s k = 0.315). This is because segments\n",
    "often contained concepts from both domains that may or may not have been\n",
    "interwoven, so it was not always clear which Discourse Category a segment be-\n",
    "longed to. Because of this, the Reviewers ultimately opted to label all segments\n",
    "via consensus coding.\n",
    "During Prompt Development, we provided the LLM with explicit task instruc-\n",
    "tions, curricular and environment context, and general guidelines (e.g., instruct-\n",
    "ing the LLM to cite evidence directly from the students’ discourse to support\n",
    "its summary decisions and Discourse Category choice). We supplemented the\n",
    "prompt with extensive contextual information not found in previous work [9],\n",
    "including the Discourse Categories, C2STEM variables and their values, physics\n",
    "and computing concepts and their definitions, and students’ actions in the learn-\n",
    "ing environment (derived from environment logs). Four labeled instances were\n",
    "initially appended to the prompt as few-shot examples (one per Discourse Cate-\n",
    "gory). Active Learning was performed for a total of two rounds over 10 validation\n",
    "set instances, at the end of which one additional few-shot instance was added.\n",
    "Before testing, R1 wrote summaries (and labeled Discourse Categories) for\n",
    "the 12 test instances. R2 then compared the human-generated summaries to two\n",
    "LLMs’ summaries: GPT-4-Turbo and GPT-4. We compare GPT-4 to GPT-4-\n",
    "Turbo to see which LLM is most promising for use in future work. To evaluate\n",
    "RQ1, R2 used “ranked choice” to rank the three summaries from best to worst\n",
    "for each test set instance without knowledge of whether the summaries were gen-\n",
    "erated by a human, GPT-4-Turbo, or GPT-4 (the Competitors). Three rankings\n",
    "were used for the scoring: (1) Wins (the number of times each Competitor was\n",
    "ranked higher than another Competitor across all instances, i.e., the best Com-\n",
    "petitor for an individual segment receives two “wins” for outranking the other\n",
    "two Competitors for that segment); (2) Best (the number of instances each Com-\n",
    "petitor was selected as the best choice); and (3) Worst (the number of instances\n",
    "each Competitor was selected as the worst choice). To answer RQ1, we used\n",
    "\"\"\"\n",
    "\n",
    "get_response(f\"What is the main idea of the following paper: '''{paper_contents}'''\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Consider this paper: '''\\n\\n1\\nTowards A Human-in-the-Loop LLM Approach to\\nCollaborative Discourse Analysis\\nClayton Cohn1[0000−0003−0856−9587], Caitlin Snyder1[0000−0002−3341−0490], Justin\\nMontenegro2, and Gautam Biswas1[0000−0002−2752−3878]\\n1 Vanderbilt University, Nashville, TN 37240, USA\\nclayton.a.cohn@vanderbilt.edu2 Martin Luther King, Jr. Academic Magnet High School, Nashville, TN 37203, USA\\nAbstract. LLMs have demonstrated proficiency in contextualizing their\\noutputs using human input, often matching or beating human-level per-\\nformance on a variety of tasks. However, LLMs have not yet been used to\\ncharacterize synergistic learning in students’ collaborative discourse. In\\nthis exploratory work, we take a first step towards adopting a human-in-\\nthe-loop prompt engineering approach with GPT-4-Turbo to summarize\\nand categorize students’ synergistic learning during collaborative dis-\\ncourse. Our preliminary findings suggest GPT-4-Turbo may be able to\\ncharacterize students’ synergistic learning in a manner comparable to\\nhumans and that our approach warrants further investigation.\\nKeywords: LLM ·Collaborative Learning ·Human-in-the-Loop ·Dis-\\ncourse Analysis ·K12 STEM.\\n1 Introduction\\nComputational modeling of scientific processes has been shown to effectively\\nfoster students’ Science, Technology, Engineering, Mathematics, and Comput-\\ning (STEM+C) learning [5], but task success necessitates synergistic learning\\n(i.e., the simultaneous development and application of science and computing\\nknowledge to address modeling tasks), which can lead to student difficulties\\n[1]. Research has shown that problem-solving environments promoting synergis-\\ntic learning in domains such as physics and computing often facilitate a bet-\\nter understanding of physics and computing concepts and practices when com-\\npared to students taught via a traditional curriculum [5]. Analyzing students’\\ncollaborative discourse offers valuable insights into their application of both\\ndomains’ concepts as they construct computational models [8]. Unfortunately,\\nmanually analyzing students’ discourse to identify their synergistic processes is\\ntime-consuming, and programmatic approaches are needed.\\nIn this paper, we take an exploratory first step towards adopting a human-in-\\nthe-loop LLM approach from previous work called Chain-of-Thought Prompting\\n+ Active Learning [3] (detailed in Section 3) to characterize the synergistic con-\\ntent in students’ collaborative discourse. We use a large language model (LLM)\\nto summarize conversation segments in terms of how physics and computing\\narXiv:2405.03677v1  [cs.CL]  6 May 2024\\n2 C. Cohn et al.\\nconcepts are interwoven to support students’ model building and debugging\\ntasks. We evaluate our approach by comparing the LLM’s summaries to human-\\nproduced ones (using an expert human evaluator to rank them) and by qualita-\\ntively analyzing the summaries to discern the LLM’s strengths and weaknesses\\nalongside a physics and computer science teacher (the Educator) with experi-\\nence teaching the C2STEM curriculum (see Section 3.1). Within this framework,\\nwe analyze data from high school students working in pairs to build kinematics\\nmodels and answer the following research questions: RQ1) How does the quality\\nof human- and LLM-generated summaries and synergistic learning characteri-\\nzations of collaborative student discourse compare?, and RQ2) What are the\\nLLM’s strengths, and where does it struggle, in summarizing and characterizing\\nsynergistic learning in physics and computing?\\nAs this work is exploratory, due to the small sample size, we aim not to\\npresent generalizable findings but hope that our results will inform subsequent\\nresearch as we work towards forging a human-AI partnership by providing teach-\\ners with actionable, LLM-generated feedback and recommendations to help them\\nguide students in their synergistic learning.\\n2 Background\\nRoschelle and Teasley [7] define collaboration as “a coordinated, synchronous ac-\\ntivity that is a result of a continuous attempt to construct and maintain a shared\\nconception of a problem.” This development of a shared conceptual understand-\\ning necessitates multi-faceted collaborative discourse across multiple dimensions:\\nsocial (e.g., navigating the social intricacies of forming a consensus [12]), cogni-\\ntive (e.g., the development of context-specific knowledge [8]), and metacognitive\\n(e.g., socially shared regulation [4]). Researchers have developed and leveraged\\nframeworks situated within learning theory to classify and analyze collaborative\\nproblem solving (CPS) both broadly (i.e., across dimensions [6]) and narrowly\\n(i.e., by focusing on one CPS aspect to gain in-depth insight, e.g., argumentative\\nknowledge construction [12]). In this paper, we focus on one dimension of CPS\\nthat is particularly important to the context of STEM+C learning: students’\\ncognitive integration of synergistic domains.\\nLeveraging CPS frameworks to classify student discourse has traditionally\\nbeen done through hand-coding utterances. However, this is time-consuming\\nand laborious, leading researchers to leverage automated classification methods\\nsuch as rule-based approaches, supervised machine learning methods, and (more\\nrecently) LLMs [10]. Utilizing LLMs can help extend previous work on classifying\\nsynergistic learning discourse, which has primarily relied on the frequency counts\\nof domain-specific concept codes [8,5]. In particular, the use of LLMs can help\\naddress the following difficulties encountered while employing traditional meth-\\nods: (1) concept codes are difficult to identify programmatically, as rule-based\\napproaches like regular expressions (regex) have difficulties with misspellings\\nand homonyms; (2) the presence or absence of concept codes is not analyzed in\\na conversational context; and (3) the presence of cross-domain concept codes is\\nTowards A HITL LLM Approach to Collaborative Discourse Analysis 3\\nnot necessarily indicative of synergistic learning, as synergistic learning requires\\nstudents to form connections between concepts in both domains.\\nRecent advances in LLM performance capabilities have allowed researchers\\nto find new and creative ways to apply these powerful models to education using\\nin-context learning (ICL) [2] (i.e., providing the LLM with labeled instances dur-\\ning inference) in lieu of traditional training that requires expensive parameter\\nupdates. One prominent extension of ICL is chain-of-thought reasoning (CoT)\\n[11], which augments the labeled instances with “reasoning chains” that explain\\nthe rationale behind the correct answer and help guide the LLM towards the cor-\\nrect solution. Recent work has found success in leveraging CoT towards scoring\\nand explaining students’ formative assessment responses in the Earth Science\\ndomain [3]. In this work, we investigate this approach as a means to summarize\\nand characterize synergistic learning in students’ collaborative discourse.\\n3 Methods\\nThis paper extends the previous work of 1) Snyder et al. on log-segmented dis-\\ncourse summarization defined by students’ model building segments extracted\\nfrom their activity logs [9], and 2) Cohn et al. on a human-in-the-loop prompt\\nengineering approach called Chain-of-Thought Prompting + Active Learning [3]\\n(the Method) for scoring and explaining students’ science formative assessment\\nresponses. The original Method is a three-step process: 1) Response Scoring,\\nwhere two human reviewers manually label a sample of students’ formative as-\\nsessment responses and identify disagreements (i.e., sticking points) the LLM\\nmay similarly struggle with; 2) Prompt Development, which employs few-shot\\nCoT prompting to address the sticking points and help align the LLM with\\nthe humans’ scoring consensus; and 3) Active Learning, where a knowledgeable\\nhuman (e.g., a domain expert, researcher, or instructor) acts as an “oracle” and\\nidentifies the LLM’s reasoning errors on a validation set, then appends additional\\nfew-shot instances that the LLM struggled with to the prompt and uses CoT\\nreasoning to help correct the LLM’s misconceptions. We illustrate the Method\\nin Figure 1. For a complete description of the Method, please see [3].\\nIn this work, we combine log-based discourse segmentation [9] and CoT\\nprompting [3] to generate more contextualized summaries of students’ discourse\\nsegments to study students’ synergistic learning processes by linking their model\\nconstruction and debugging activities with their conversations during each probl-\\nem-solving segment. We provide Supplementary Materials3that include 1) addi-\\ntional information about the learning environment, 2) method application details\\n(including our final prompt and few-shot example selection methodology), 3) a\\nmore in depth look at our conversation with the Educator, and 4) a more detailed\\nanalysis of the LLM’s strengths and weaknesses while applying the Method.\\n3https://github.com/oele-isis-vanderbilt/AIED24_LBR\\n4 C. Cohn et al.\\n3.1 STEM+C Learning Environment, Curriculum, and Data\\nOur work in this paper centers on the C2STEM learning environment [5], where\\nstudents learn kinematics by building computational models of the 1- and 2-D\\nmotion of objects. C2STEM combines block-based programming with domain-\\nspecific modeling blocks to support the development and integration of science\\nand computing knowledge as students create partial or complete models that\\nsimulate behaviors governed by scientific principles. This paper focuses on the\\n1-D Truck Task, where students use their knowledge of kinematic equations to\\nmodel the motion of a truck that starts from rest, accelerates to a speed limit,\\ncruises at that speed, then decelerates to come to a stop at a stop sign.\\nOur study, approved by our university Institutional Review Board, included\\n26 consented high school students (aged 14-15) who completed the C2STEM\\nkinematics curriculum. Students’ demographic information was not collected as\\npart of this study (we began collecting it in later studies). Data collection in-\\ncluded logged actions in the C2STEM environment, saved project files, and video\\nand audio data (collected using laptop webcams and OBS software). Our data\\nanalysis included 9 dyads (one group had a student who did not consent to\\ndata collection, so we did not analyze that group; and we had technical issues\\nwith audio data from other groups). The dataset includes 9 hours of discourse\\ntranscripts and over 2,000 logged actions collected during one day of the study.\\nStudent discourse was transcribed using Otter.ai and edited for accuracy.\\n3.2 Approach\\nWe extend the Method, previously used for formative assessment scoring and\\nfeedback, to prompt GPT-4-Turbo to summarize segments of students’ discourse\\nand identify the Discourse Category (defined momentarily) by answering the fol-\\nlowing question: “Given a discourse segment, and its environment task context\\nand actions, is the students’ conversation best characterized as physics-focused\\n(i.e., the conversation is primarily focused on the physics domain), computing-\\nfocused (i.e., the conversation is primarily focused on the computing domain),\\nphysics-and-computing-synergistic (i.e., students discuss concepts from both do-\\nmains, interleaving them throughout the conversation, and making connections\\nFig. 1. Chain-of-Thought Prompting + Active Learning, identified by the green box,\\nwhere each blue diamond is a step in the Method. Yellow boxes represent the process’s\\napplication to the classroom detailed in prior work [3].\\nTowards A HITL LLM Approach to Collaborative Discourse Analysis 5\\nbetween them), or physics-and-computing-separate (i.e., students discuss both\\ndomains but do so separately without interleaving)?” We use the recently re-\\nleased GPT-4-Turbo LLM (gpt-4-0125-preview) because it provides an extended\\ncontext window (128,000 tokens).\\nWe selected 10 training instances and 12 testing instances (10 additional\\nsegments were used as a validation set to perform Active Learning) prior to\\nResponse Scoring, using stratified sampling to approximate a uniform distribu-\\ntion across Discourse Categories for both the train and test sets. Note that the\\nstudent discourse was segmented based on which element of the model the stu-\\ndents were working on (identified automatically via log data). During Response\\nScoring, the first two authors of this paper (Reviewers R1 and R2, respectively)\\nindependently evaluated the training set segments, classifying each segment as\\nbelonging to one of the four Discourse Categories. For each segment the Re-\\nviewers disagreed on, the reason for disagreement was noted as a sticking point,\\nand the segment was discussed until a consensus was reached on the specific\\nDiscourse Category for that segment. R1 and R2 initially struggled to agree on\\nsegments’ Discourse Categories (Cohen’s k = 0.315). This is because segments\\noften contained concepts from both domains that may or may not have been\\ninterwoven, so it was not always clear which Discourse Category a segment be-\\nlonged to. Because of this, the Reviewers ultimately opted to label all segments\\nvia consensus coding.\\nDuring Prompt Development, we provided the LLM with explicit task instruc-\\ntions, curricular and environment context, and general guidelines (e.g., instruct-\\ning the LLM to cite evidence directly from the students’ discourse to support\\nits summary decisions and Discourse Category choice). We supplemented the\\nprompt with extensive contextual information not found in previous work [9],\\nincluding the Discourse Categories, C2STEM variables and their values, physics\\nand computing concepts and their definitions, and students’ actions in the learn-\\ning environment (derived from environment logs). Four labeled instances were\\ninitially appended to the prompt as few-shot examples (one per Discourse Cate-\\ngory). Active Learning was performed for a total of two rounds over 10 validation\\nset instances, at the end of which one additional few-shot instance was added.\\nBefore testing, R1 wrote summaries (and labeled Discourse Categories) for\\nthe 12 test instances. R2 then compared the human-generated summaries to two\\nLLMs’ summaries: GPT-4-Turbo and GPT-4. We compare GPT-4 to GPT-4-\\nTurbo to see which LLM is most promising for use in future work. To evaluate\\nRQ1, R2 used “ranked choice” to rank the three summaries from best to worst\\nfor each test set instance without knowledge of whether the summaries were gen-\\nerated by a human, GPT-4-Turbo, or GPT-4 (the Competitors). Three rankings\\nwere used for the scoring: (1) Wins (the number of times each Competitor was\\nranked higher than another Competitor across all instances, i.e., the best Com-\\npetitor for an individual segment receives two “wins” for outranking the other\\ntwo Competitors for that segment); (2) Best (the number of instances each Com-\\npetitor was selected as the best choice); and (3) Worst (the number of instances\\neach Competitor was selected as the worst choice). To answer RQ1, we used\\n'''. \\n What problem is the paper trying to solve?\",\n",
       " \"Consider this paper: '''\\n\\n1\\nTowards A Human-in-the-Loop LLM Approach to\\nCollaborative Discourse Analysis\\nClayton Cohn1[0000−0003−0856−9587], Caitlin Snyder1[0000−0002−3341−0490], Justin\\nMontenegro2, and Gautam Biswas1[0000−0002−2752−3878]\\n1 Vanderbilt University, Nashville, TN 37240, USA\\nclayton.a.cohn@vanderbilt.edu2 Martin Luther King, Jr. Academic Magnet High School, Nashville, TN 37203, USA\\nAbstract. LLMs have demonstrated proficiency in contextualizing their\\noutputs using human input, often matching or beating human-level per-\\nformance on a variety of tasks. However, LLMs have not yet been used to\\ncharacterize synergistic learning in students’ collaborative discourse. In\\nthis exploratory work, we take a first step towards adopting a human-in-\\nthe-loop prompt engineering approach with GPT-4-Turbo to summarize\\nand categorize students’ synergistic learning during collaborative dis-\\ncourse. Our preliminary findings suggest GPT-4-Turbo may be able to\\ncharacterize students’ synergistic learning in a manner comparable to\\nhumans and that our approach warrants further investigation.\\nKeywords: LLM ·Collaborative Learning ·Human-in-the-Loop ·Dis-\\ncourse Analysis ·K12 STEM.\\n1 Introduction\\nComputational modeling of scientific processes has been shown to effectively\\nfoster students’ Science, Technology, Engineering, Mathematics, and Comput-\\ning (STEM+C) learning [5], but task success necessitates synergistic learning\\n(i.e., the simultaneous development and application of science and computing\\nknowledge to address modeling tasks), which can lead to student difficulties\\n[1]. Research has shown that problem-solving environments promoting synergis-\\ntic learning in domains such as physics and computing often facilitate a bet-\\nter understanding of physics and computing concepts and practices when com-\\npared to students taught via a traditional curriculum [5]. Analyzing students’\\ncollaborative discourse offers valuable insights into their application of both\\ndomains’ concepts as they construct computational models [8]. Unfortunately,\\nmanually analyzing students’ discourse to identify their synergistic processes is\\ntime-consuming, and programmatic approaches are needed.\\nIn this paper, we take an exploratory first step towards adopting a human-in-\\nthe-loop LLM approach from previous work called Chain-of-Thought Prompting\\n+ Active Learning [3] (detailed in Section 3) to characterize the synergistic con-\\ntent in students’ collaborative discourse. We use a large language model (LLM)\\nto summarize conversation segments in terms of how physics and computing\\narXiv:2405.03677v1  [cs.CL]  6 May 2024\\n2 C. Cohn et al.\\nconcepts are interwoven to support students’ model building and debugging\\ntasks. We evaluate our approach by comparing the LLM’s summaries to human-\\nproduced ones (using an expert human evaluator to rank them) and by qualita-\\ntively analyzing the summaries to discern the LLM’s strengths and weaknesses\\nalongside a physics and computer science teacher (the Educator) with experi-\\nence teaching the C2STEM curriculum (see Section 3.1). Within this framework,\\nwe analyze data from high school students working in pairs to build kinematics\\nmodels and answer the following research questions: RQ1) How does the quality\\nof human- and LLM-generated summaries and synergistic learning characteri-\\nzations of collaborative student discourse compare?, and RQ2) What are the\\nLLM’s strengths, and where does it struggle, in summarizing and characterizing\\nsynergistic learning in physics and computing?\\nAs this work is exploratory, due to the small sample size, we aim not to\\npresent generalizable findings but hope that our results will inform subsequent\\nresearch as we work towards forging a human-AI partnership by providing teach-\\ners with actionable, LLM-generated feedback and recommendations to help them\\nguide students in their synergistic learning.\\n2 Background\\nRoschelle and Teasley [7] define collaboration as “a coordinated, synchronous ac-\\ntivity that is a result of a continuous attempt to construct and maintain a shared\\nconception of a problem.” This development of a shared conceptual understand-\\ning necessitates multi-faceted collaborative discourse across multiple dimensions:\\nsocial (e.g., navigating the social intricacies of forming a consensus [12]), cogni-\\ntive (e.g., the development of context-specific knowledge [8]), and metacognitive\\n(e.g., socially shared regulation [4]). Researchers have developed and leveraged\\nframeworks situated within learning theory to classify and analyze collaborative\\nproblem solving (CPS) both broadly (i.e., across dimensions [6]) and narrowly\\n(i.e., by focusing on one CPS aspect to gain in-depth insight, e.g., argumentative\\nknowledge construction [12]). In this paper, we focus on one dimension of CPS\\nthat is particularly important to the context of STEM+C learning: students’\\ncognitive integration of synergistic domains.\\nLeveraging CPS frameworks to classify student discourse has traditionally\\nbeen done through hand-coding utterances. However, this is time-consuming\\nand laborious, leading researchers to leverage automated classification methods\\nsuch as rule-based approaches, supervised machine learning methods, and (more\\nrecently) LLMs [10]. Utilizing LLMs can help extend previous work on classifying\\nsynergistic learning discourse, which has primarily relied on the frequency counts\\nof domain-specific concept codes [8,5]. In particular, the use of LLMs can help\\naddress the following difficulties encountered while employing traditional meth-\\nods: (1) concept codes are difficult to identify programmatically, as rule-based\\napproaches like regular expressions (regex) have difficulties with misspellings\\nand homonyms; (2) the presence or absence of concept codes is not analyzed in\\na conversational context; and (3) the presence of cross-domain concept codes is\\nTowards A HITL LLM Approach to Collaborative Discourse Analysis 3\\nnot necessarily indicative of synergistic learning, as synergistic learning requires\\nstudents to form connections between concepts in both domains.\\nRecent advances in LLM performance capabilities have allowed researchers\\nto find new and creative ways to apply these powerful models to education using\\nin-context learning (ICL) [2] (i.e., providing the LLM with labeled instances dur-\\ning inference) in lieu of traditional training that requires expensive parameter\\nupdates. One prominent extension of ICL is chain-of-thought reasoning (CoT)\\n[11], which augments the labeled instances with “reasoning chains” that explain\\nthe rationale behind the correct answer and help guide the LLM towards the cor-\\nrect solution. Recent work has found success in leveraging CoT towards scoring\\nand explaining students’ formative assessment responses in the Earth Science\\ndomain [3]. In this work, we investigate this approach as a means to summarize\\nand characterize synergistic learning in students’ collaborative discourse.\\n3 Methods\\nThis paper extends the previous work of 1) Snyder et al. on log-segmented dis-\\ncourse summarization defined by students’ model building segments extracted\\nfrom their activity logs [9], and 2) Cohn et al. on a human-in-the-loop prompt\\nengineering approach called Chain-of-Thought Prompting + Active Learning [3]\\n(the Method) for scoring and explaining students’ science formative assessment\\nresponses. The original Method is a three-step process: 1) Response Scoring,\\nwhere two human reviewers manually label a sample of students’ formative as-\\nsessment responses and identify disagreements (i.e., sticking points) the LLM\\nmay similarly struggle with; 2) Prompt Development, which employs few-shot\\nCoT prompting to address the sticking points and help align the LLM with\\nthe humans’ scoring consensus; and 3) Active Learning, where a knowledgeable\\nhuman (e.g., a domain expert, researcher, or instructor) acts as an “oracle” and\\nidentifies the LLM’s reasoning errors on a validation set, then appends additional\\nfew-shot instances that the LLM struggled with to the prompt and uses CoT\\nreasoning to help correct the LLM’s misconceptions. We illustrate the Method\\nin Figure 1. For a complete description of the Method, please see [3].\\nIn this work, we combine log-based discourse segmentation [9] and CoT\\nprompting [3] to generate more contextualized summaries of students’ discourse\\nsegments to study students’ synergistic learning processes by linking their model\\nconstruction and debugging activities with their conversations during each probl-\\nem-solving segment. We provide Supplementary Materials3that include 1) addi-\\ntional information about the learning environment, 2) method application details\\n(including our final prompt and few-shot example selection methodology), 3) a\\nmore in depth look at our conversation with the Educator, and 4) a more detailed\\nanalysis of the LLM’s strengths and weaknesses while applying the Method.\\n3https://github.com/oele-isis-vanderbilt/AIED24_LBR\\n4 C. Cohn et al.\\n3.1 STEM+C Learning Environment, Curriculum, and Data\\nOur work in this paper centers on the C2STEM learning environment [5], where\\nstudents learn kinematics by building computational models of the 1- and 2-D\\nmotion of objects. C2STEM combines block-based programming with domain-\\nspecific modeling blocks to support the development and integration of science\\nand computing knowledge as students create partial or complete models that\\nsimulate behaviors governed by scientific principles. This paper focuses on the\\n1-D Truck Task, where students use their knowledge of kinematic equations to\\nmodel the motion of a truck that starts from rest, accelerates to a speed limit,\\ncruises at that speed, then decelerates to come to a stop at a stop sign.\\nOur study, approved by our university Institutional Review Board, included\\n26 consented high school students (aged 14-15) who completed the C2STEM\\nkinematics curriculum. Students’ demographic information was not collected as\\npart of this study (we began collecting it in later studies). Data collection in-\\ncluded logged actions in the C2STEM environment, saved project files, and video\\nand audio data (collected using laptop webcams and OBS software). Our data\\nanalysis included 9 dyads (one group had a student who did not consent to\\ndata collection, so we did not analyze that group; and we had technical issues\\nwith audio data from other groups). The dataset includes 9 hours of discourse\\ntranscripts and over 2,000 logged actions collected during one day of the study.\\nStudent discourse was transcribed using Otter.ai and edited for accuracy.\\n3.2 Approach\\nWe extend the Method, previously used for formative assessment scoring and\\nfeedback, to prompt GPT-4-Turbo to summarize segments of students’ discourse\\nand identify the Discourse Category (defined momentarily) by answering the fol-\\nlowing question: “Given a discourse segment, and its environment task context\\nand actions, is the students’ conversation best characterized as physics-focused\\n(i.e., the conversation is primarily focused on the physics domain), computing-\\nfocused (i.e., the conversation is primarily focused on the computing domain),\\nphysics-and-computing-synergistic (i.e., students discuss concepts from both do-\\nmains, interleaving them throughout the conversation, and making connections\\nFig. 1. Chain-of-Thought Prompting + Active Learning, identified by the green box,\\nwhere each blue diamond is a step in the Method. Yellow boxes represent the process’s\\napplication to the classroom detailed in prior work [3].\\nTowards A HITL LLM Approach to Collaborative Discourse Analysis 5\\nbetween them), or physics-and-computing-separate (i.e., students discuss both\\ndomains but do so separately without interleaving)?” We use the recently re-\\nleased GPT-4-Turbo LLM (gpt-4-0125-preview) because it provides an extended\\ncontext window (128,000 tokens).\\nWe selected 10 training instances and 12 testing instances (10 additional\\nsegments were used as a validation set to perform Active Learning) prior to\\nResponse Scoring, using stratified sampling to approximate a uniform distribu-\\ntion across Discourse Categories for both the train and test sets. Note that the\\nstudent discourse was segmented based on which element of the model the stu-\\ndents were working on (identified automatically via log data). During Response\\nScoring, the first two authors of this paper (Reviewers R1 and R2, respectively)\\nindependently evaluated the training set segments, classifying each segment as\\nbelonging to one of the four Discourse Categories. For each segment the Re-\\nviewers disagreed on, the reason for disagreement was noted as a sticking point,\\nand the segment was discussed until a consensus was reached on the specific\\nDiscourse Category for that segment. R1 and R2 initially struggled to agree on\\nsegments’ Discourse Categories (Cohen’s k = 0.315). This is because segments\\noften contained concepts from both domains that may or may not have been\\ninterwoven, so it was not always clear which Discourse Category a segment be-\\nlonged to. Because of this, the Reviewers ultimately opted to label all segments\\nvia consensus coding.\\nDuring Prompt Development, we provided the LLM with explicit task instruc-\\ntions, curricular and environment context, and general guidelines (e.g., instruct-\\ning the LLM to cite evidence directly from the students’ discourse to support\\nits summary decisions and Discourse Category choice). We supplemented the\\nprompt with extensive contextual information not found in previous work [9],\\nincluding the Discourse Categories, C2STEM variables and their values, physics\\nand computing concepts and their definitions, and students’ actions in the learn-\\ning environment (derived from environment logs). Four labeled instances were\\ninitially appended to the prompt as few-shot examples (one per Discourse Cate-\\ngory). Active Learning was performed for a total of two rounds over 10 validation\\nset instances, at the end of which one additional few-shot instance was added.\\nBefore testing, R1 wrote summaries (and labeled Discourse Categories) for\\nthe 12 test instances. R2 then compared the human-generated summaries to two\\nLLMs’ summaries: GPT-4-Turbo and GPT-4. We compare GPT-4 to GPT-4-\\nTurbo to see which LLM is most promising for use in future work. To evaluate\\nRQ1, R2 used “ranked choice” to rank the three summaries from best to worst\\nfor each test set instance without knowledge of whether the summaries were gen-\\nerated by a human, GPT-4-Turbo, or GPT-4 (the Competitors). Three rankings\\nwere used for the scoring: (1) Wins (the number of times each Competitor was\\nranked higher than another Competitor across all instances, i.e., the best Com-\\npetitor for an individual segment receives two “wins” for outranking the other\\ntwo Competitors for that segment); (2) Best (the number of instances each Com-\\npetitor was selected as the best choice); and (3) Worst (the number of instances\\neach Competitor was selected as the worst choice). To answer RQ1, we used\\n'''. \\n Why is the problem interesting?\",\n",
       " \"Consider this paper: '''\\n\\n1\\nTowards A Human-in-the-Loop LLM Approach to\\nCollaborative Discourse Analysis\\nClayton Cohn1[0000−0003−0856−9587], Caitlin Snyder1[0000−0002−3341−0490], Justin\\nMontenegro2, and Gautam Biswas1[0000−0002−2752−3878]\\n1 Vanderbilt University, Nashville, TN 37240, USA\\nclayton.a.cohn@vanderbilt.edu2 Martin Luther King, Jr. Academic Magnet High School, Nashville, TN 37203, USA\\nAbstract. LLMs have demonstrated proficiency in contextualizing their\\noutputs using human input, often matching or beating human-level per-\\nformance on a variety of tasks. However, LLMs have not yet been used to\\ncharacterize synergistic learning in students’ collaborative discourse. In\\nthis exploratory work, we take a first step towards adopting a human-in-\\nthe-loop prompt engineering approach with GPT-4-Turbo to summarize\\nand categorize students’ synergistic learning during collaborative dis-\\ncourse. Our preliminary findings suggest GPT-4-Turbo may be able to\\ncharacterize students’ synergistic learning in a manner comparable to\\nhumans and that our approach warrants further investigation.\\nKeywords: LLM ·Collaborative Learning ·Human-in-the-Loop ·Dis-\\ncourse Analysis ·K12 STEM.\\n1 Introduction\\nComputational modeling of scientific processes has been shown to effectively\\nfoster students’ Science, Technology, Engineering, Mathematics, and Comput-\\ning (STEM+C) learning [5], but task success necessitates synergistic learning\\n(i.e., the simultaneous development and application of science and computing\\nknowledge to address modeling tasks), which can lead to student difficulties\\n[1]. Research has shown that problem-solving environments promoting synergis-\\ntic learning in domains such as physics and computing often facilitate a bet-\\nter understanding of physics and computing concepts and practices when com-\\npared to students taught via a traditional curriculum [5]. Analyzing students’\\ncollaborative discourse offers valuable insights into their application of both\\ndomains’ concepts as they construct computational models [8]. Unfortunately,\\nmanually analyzing students’ discourse to identify their synergistic processes is\\ntime-consuming, and programmatic approaches are needed.\\nIn this paper, we take an exploratory first step towards adopting a human-in-\\nthe-loop LLM approach from previous work called Chain-of-Thought Prompting\\n+ Active Learning [3] (detailed in Section 3) to characterize the synergistic con-\\ntent in students’ collaborative discourse. We use a large language model (LLM)\\nto summarize conversation segments in terms of how physics and computing\\narXiv:2405.03677v1  [cs.CL]  6 May 2024\\n2 C. Cohn et al.\\nconcepts are interwoven to support students’ model building and debugging\\ntasks. We evaluate our approach by comparing the LLM’s summaries to human-\\nproduced ones (using an expert human evaluator to rank them) and by qualita-\\ntively analyzing the summaries to discern the LLM’s strengths and weaknesses\\nalongside a physics and computer science teacher (the Educator) with experi-\\nence teaching the C2STEM curriculum (see Section 3.1). Within this framework,\\nwe analyze data from high school students working in pairs to build kinematics\\nmodels and answer the following research questions: RQ1) How does the quality\\nof human- and LLM-generated summaries and synergistic learning characteri-\\nzations of collaborative student discourse compare?, and RQ2) What are the\\nLLM’s strengths, and where does it struggle, in summarizing and characterizing\\nsynergistic learning in physics and computing?\\nAs this work is exploratory, due to the small sample size, we aim not to\\npresent generalizable findings but hope that our results will inform subsequent\\nresearch as we work towards forging a human-AI partnership by providing teach-\\ners with actionable, LLM-generated feedback and recommendations to help them\\nguide students in their synergistic learning.\\n2 Background\\nRoschelle and Teasley [7] define collaboration as “a coordinated, synchronous ac-\\ntivity that is a result of a continuous attempt to construct and maintain a shared\\nconception of a problem.” This development of a shared conceptual understand-\\ning necessitates multi-faceted collaborative discourse across multiple dimensions:\\nsocial (e.g., navigating the social intricacies of forming a consensus [12]), cogni-\\ntive (e.g., the development of context-specific knowledge [8]), and metacognitive\\n(e.g., socially shared regulation [4]). Researchers have developed and leveraged\\nframeworks situated within learning theory to classify and analyze collaborative\\nproblem solving (CPS) both broadly (i.e., across dimensions [6]) and narrowly\\n(i.e., by focusing on one CPS aspect to gain in-depth insight, e.g., argumentative\\nknowledge construction [12]). In this paper, we focus on one dimension of CPS\\nthat is particularly important to the context of STEM+C learning: students’\\ncognitive integration of synergistic domains.\\nLeveraging CPS frameworks to classify student discourse has traditionally\\nbeen done through hand-coding utterances. However, this is time-consuming\\nand laborious, leading researchers to leverage automated classification methods\\nsuch as rule-based approaches, supervised machine learning methods, and (more\\nrecently) LLMs [10]. Utilizing LLMs can help extend previous work on classifying\\nsynergistic learning discourse, which has primarily relied on the frequency counts\\nof domain-specific concept codes [8,5]. In particular, the use of LLMs can help\\naddress the following difficulties encountered while employing traditional meth-\\nods: (1) concept codes are difficult to identify programmatically, as rule-based\\napproaches like regular expressions (regex) have difficulties with misspellings\\nand homonyms; (2) the presence or absence of concept codes is not analyzed in\\na conversational context; and (3) the presence of cross-domain concept codes is\\nTowards A HITL LLM Approach to Collaborative Discourse Analysis 3\\nnot necessarily indicative of synergistic learning, as synergistic learning requires\\nstudents to form connections between concepts in both domains.\\nRecent advances in LLM performance capabilities have allowed researchers\\nto find new and creative ways to apply these powerful models to education using\\nin-context learning (ICL) [2] (i.e., providing the LLM with labeled instances dur-\\ning inference) in lieu of traditional training that requires expensive parameter\\nupdates. One prominent extension of ICL is chain-of-thought reasoning (CoT)\\n[11], which augments the labeled instances with “reasoning chains” that explain\\nthe rationale behind the correct answer and help guide the LLM towards the cor-\\nrect solution. Recent work has found success in leveraging CoT towards scoring\\nand explaining students’ formative assessment responses in the Earth Science\\ndomain [3]. In this work, we investigate this approach as a means to summarize\\nand characterize synergistic learning in students’ collaborative discourse.\\n3 Methods\\nThis paper extends the previous work of 1) Snyder et al. on log-segmented dis-\\ncourse summarization defined by students’ model building segments extracted\\nfrom their activity logs [9], and 2) Cohn et al. on a human-in-the-loop prompt\\nengineering approach called Chain-of-Thought Prompting + Active Learning [3]\\n(the Method) for scoring and explaining students’ science formative assessment\\nresponses. The original Method is a three-step process: 1) Response Scoring,\\nwhere two human reviewers manually label a sample of students’ formative as-\\nsessment responses and identify disagreements (i.e., sticking points) the LLM\\nmay similarly struggle with; 2) Prompt Development, which employs few-shot\\nCoT prompting to address the sticking points and help align the LLM with\\nthe humans’ scoring consensus; and 3) Active Learning, where a knowledgeable\\nhuman (e.g., a domain expert, researcher, or instructor) acts as an “oracle” and\\nidentifies the LLM’s reasoning errors on a validation set, then appends additional\\nfew-shot instances that the LLM struggled with to the prompt and uses CoT\\nreasoning to help correct the LLM’s misconceptions. We illustrate the Method\\nin Figure 1. For a complete description of the Method, please see [3].\\nIn this work, we combine log-based discourse segmentation [9] and CoT\\nprompting [3] to generate more contextualized summaries of students’ discourse\\nsegments to study students’ synergistic learning processes by linking their model\\nconstruction and debugging activities with their conversations during each probl-\\nem-solving segment. We provide Supplementary Materials3that include 1) addi-\\ntional information about the learning environment, 2) method application details\\n(including our final prompt and few-shot example selection methodology), 3) a\\nmore in depth look at our conversation with the Educator, and 4) a more detailed\\nanalysis of the LLM’s strengths and weaknesses while applying the Method.\\n3https://github.com/oele-isis-vanderbilt/AIED24_LBR\\n4 C. Cohn et al.\\n3.1 STEM+C Learning Environment, Curriculum, and Data\\nOur work in this paper centers on the C2STEM learning environment [5], where\\nstudents learn kinematics by building computational models of the 1- and 2-D\\nmotion of objects. C2STEM combines block-based programming with domain-\\nspecific modeling blocks to support the development and integration of science\\nand computing knowledge as students create partial or complete models that\\nsimulate behaviors governed by scientific principles. This paper focuses on the\\n1-D Truck Task, where students use their knowledge of kinematic equations to\\nmodel the motion of a truck that starts from rest, accelerates to a speed limit,\\ncruises at that speed, then decelerates to come to a stop at a stop sign.\\nOur study, approved by our university Institutional Review Board, included\\n26 consented high school students (aged 14-15) who completed the C2STEM\\nkinematics curriculum. Students’ demographic information was not collected as\\npart of this study (we began collecting it in later studies). Data collection in-\\ncluded logged actions in the C2STEM environment, saved project files, and video\\nand audio data (collected using laptop webcams and OBS software). Our data\\nanalysis included 9 dyads (one group had a student who did not consent to\\ndata collection, so we did not analyze that group; and we had technical issues\\nwith audio data from other groups). The dataset includes 9 hours of discourse\\ntranscripts and over 2,000 logged actions collected during one day of the study.\\nStudent discourse was transcribed using Otter.ai and edited for accuracy.\\n3.2 Approach\\nWe extend the Method, previously used for formative assessment scoring and\\nfeedback, to prompt GPT-4-Turbo to summarize segments of students’ discourse\\nand identify the Discourse Category (defined momentarily) by answering the fol-\\nlowing question: “Given a discourse segment, and its environment task context\\nand actions, is the students’ conversation best characterized as physics-focused\\n(i.e., the conversation is primarily focused on the physics domain), computing-\\nfocused (i.e., the conversation is primarily focused on the computing domain),\\nphysics-and-computing-synergistic (i.e., students discuss concepts from both do-\\nmains, interleaving them throughout the conversation, and making connections\\nFig. 1. Chain-of-Thought Prompting + Active Learning, identified by the green box,\\nwhere each blue diamond is a step in the Method. Yellow boxes represent the process’s\\napplication to the classroom detailed in prior work [3].\\nTowards A HITL LLM Approach to Collaborative Discourse Analysis 5\\nbetween them), or physics-and-computing-separate (i.e., students discuss both\\ndomains but do so separately without interleaving)?” We use the recently re-\\nleased GPT-4-Turbo LLM (gpt-4-0125-preview) because it provides an extended\\ncontext window (128,000 tokens).\\nWe selected 10 training instances and 12 testing instances (10 additional\\nsegments were used as a validation set to perform Active Learning) prior to\\nResponse Scoring, using stratified sampling to approximate a uniform distribu-\\ntion across Discourse Categories for both the train and test sets. Note that the\\nstudent discourse was segmented based on which element of the model the stu-\\ndents were working on (identified automatically via log data). During Response\\nScoring, the first two authors of this paper (Reviewers R1 and R2, respectively)\\nindependently evaluated the training set segments, classifying each segment as\\nbelonging to one of the four Discourse Categories. For each segment the Re-\\nviewers disagreed on, the reason for disagreement was noted as a sticking point,\\nand the segment was discussed until a consensus was reached on the specific\\nDiscourse Category for that segment. R1 and R2 initially struggled to agree on\\nsegments’ Discourse Categories (Cohen’s k = 0.315). This is because segments\\noften contained concepts from both domains that may or may not have been\\ninterwoven, so it was not always clear which Discourse Category a segment be-\\nlonged to. Because of this, the Reviewers ultimately opted to label all segments\\nvia consensus coding.\\nDuring Prompt Development, we provided the LLM with explicit task instruc-\\ntions, curricular and environment context, and general guidelines (e.g., instruct-\\ning the LLM to cite evidence directly from the students’ discourse to support\\nits summary decisions and Discourse Category choice). We supplemented the\\nprompt with extensive contextual information not found in previous work [9],\\nincluding the Discourse Categories, C2STEM variables and their values, physics\\nand computing concepts and their definitions, and students’ actions in the learn-\\ning environment (derived from environment logs). Four labeled instances were\\ninitially appended to the prompt as few-shot examples (one per Discourse Cate-\\ngory). Active Learning was performed for a total of two rounds over 10 validation\\nset instances, at the end of which one additional few-shot instance was added.\\nBefore testing, R1 wrote summaries (and labeled Discourse Categories) for\\nthe 12 test instances. R2 then compared the human-generated summaries to two\\nLLMs’ summaries: GPT-4-Turbo and GPT-4. We compare GPT-4 to GPT-4-\\nTurbo to see which LLM is most promising for use in future work. To evaluate\\nRQ1, R2 used “ranked choice” to rank the three summaries from best to worst\\nfor each test set instance without knowledge of whether the summaries were gen-\\nerated by a human, GPT-4-Turbo, or GPT-4 (the Competitors). Three rankings\\nwere used for the scoring: (1) Wins (the number of times each Competitor was\\nranked higher than another Competitor across all instances, i.e., the best Com-\\npetitor for an individual segment receives two “wins” for outranking the other\\ntwo Competitors for that segment); (2) Best (the number of instances each Com-\\npetitor was selected as the best choice); and (3) Worst (the number of instances\\neach Competitor was selected as the worst choice). To answer RQ1, we used\\n'''. \\n What is the primary contribution?\",\n",
       " \"Consider this paper: '''\\n\\n1\\nTowards A Human-in-the-Loop LLM Approach to\\nCollaborative Discourse Analysis\\nClayton Cohn1[0000−0003−0856−9587], Caitlin Snyder1[0000−0002−3341−0490], Justin\\nMontenegro2, and Gautam Biswas1[0000−0002−2752−3878]\\n1 Vanderbilt University, Nashville, TN 37240, USA\\nclayton.a.cohn@vanderbilt.edu2 Martin Luther King, Jr. Academic Magnet High School, Nashville, TN 37203, USA\\nAbstract. LLMs have demonstrated proficiency in contextualizing their\\noutputs using human input, often matching or beating human-level per-\\nformance on a variety of tasks. However, LLMs have not yet been used to\\ncharacterize synergistic learning in students’ collaborative discourse. In\\nthis exploratory work, we take a first step towards adopting a human-in-\\nthe-loop prompt engineering approach with GPT-4-Turbo to summarize\\nand categorize students’ synergistic learning during collaborative dis-\\ncourse. Our preliminary findings suggest GPT-4-Turbo may be able to\\ncharacterize students’ synergistic learning in a manner comparable to\\nhumans and that our approach warrants further investigation.\\nKeywords: LLM ·Collaborative Learning ·Human-in-the-Loop ·Dis-\\ncourse Analysis ·K12 STEM.\\n1 Introduction\\nComputational modeling of scientific processes has been shown to effectively\\nfoster students’ Science, Technology, Engineering, Mathematics, and Comput-\\ning (STEM+C) learning [5], but task success necessitates synergistic learning\\n(i.e., the simultaneous development and application of science and computing\\nknowledge to address modeling tasks), which can lead to student difficulties\\n[1]. Research has shown that problem-solving environments promoting synergis-\\ntic learning in domains such as physics and computing often facilitate a bet-\\nter understanding of physics and computing concepts and practices when com-\\npared to students taught via a traditional curriculum [5]. Analyzing students’\\ncollaborative discourse offers valuable insights into their application of both\\ndomains’ concepts as they construct computational models [8]. Unfortunately,\\nmanually analyzing students’ discourse to identify their synergistic processes is\\ntime-consuming, and programmatic approaches are needed.\\nIn this paper, we take an exploratory first step towards adopting a human-in-\\nthe-loop LLM approach from previous work called Chain-of-Thought Prompting\\n+ Active Learning [3] (detailed in Section 3) to characterize the synergistic con-\\ntent in students’ collaborative discourse. We use a large language model (LLM)\\nto summarize conversation segments in terms of how physics and computing\\narXiv:2405.03677v1  [cs.CL]  6 May 2024\\n2 C. Cohn et al.\\nconcepts are interwoven to support students’ model building and debugging\\ntasks. We evaluate our approach by comparing the LLM’s summaries to human-\\nproduced ones (using an expert human evaluator to rank them) and by qualita-\\ntively analyzing the summaries to discern the LLM’s strengths and weaknesses\\nalongside a physics and computer science teacher (the Educator) with experi-\\nence teaching the C2STEM curriculum (see Section 3.1). Within this framework,\\nwe analyze data from high school students working in pairs to build kinematics\\nmodels and answer the following research questions: RQ1) How does the quality\\nof human- and LLM-generated summaries and synergistic learning characteri-\\nzations of collaborative student discourse compare?, and RQ2) What are the\\nLLM’s strengths, and where does it struggle, in summarizing and characterizing\\nsynergistic learning in physics and computing?\\nAs this work is exploratory, due to the small sample size, we aim not to\\npresent generalizable findings but hope that our results will inform subsequent\\nresearch as we work towards forging a human-AI partnership by providing teach-\\ners with actionable, LLM-generated feedback and recommendations to help them\\nguide students in their synergistic learning.\\n2 Background\\nRoschelle and Teasley [7] define collaboration as “a coordinated, synchronous ac-\\ntivity that is a result of a continuous attempt to construct and maintain a shared\\nconception of a problem.” This development of a shared conceptual understand-\\ning necessitates multi-faceted collaborative discourse across multiple dimensions:\\nsocial (e.g., navigating the social intricacies of forming a consensus [12]), cogni-\\ntive (e.g., the development of context-specific knowledge [8]), and metacognitive\\n(e.g., socially shared regulation [4]). Researchers have developed and leveraged\\nframeworks situated within learning theory to classify and analyze collaborative\\nproblem solving (CPS) both broadly (i.e., across dimensions [6]) and narrowly\\n(i.e., by focusing on one CPS aspect to gain in-depth insight, e.g., argumentative\\nknowledge construction [12]). In this paper, we focus on one dimension of CPS\\nthat is particularly important to the context of STEM+C learning: students’\\ncognitive integration of synergistic domains.\\nLeveraging CPS frameworks to classify student discourse has traditionally\\nbeen done through hand-coding utterances. However, this is time-consuming\\nand laborious, leading researchers to leverage automated classification methods\\nsuch as rule-based approaches, supervised machine learning methods, and (more\\nrecently) LLMs [10]. Utilizing LLMs can help extend previous work on classifying\\nsynergistic learning discourse, which has primarily relied on the frequency counts\\nof domain-specific concept codes [8,5]. In particular, the use of LLMs can help\\naddress the following difficulties encountered while employing traditional meth-\\nods: (1) concept codes are difficult to identify programmatically, as rule-based\\napproaches like regular expressions (regex) have difficulties with misspellings\\nand homonyms; (2) the presence or absence of concept codes is not analyzed in\\na conversational context; and (3) the presence of cross-domain concept codes is\\nTowards A HITL LLM Approach to Collaborative Discourse Analysis 3\\nnot necessarily indicative of synergistic learning, as synergistic learning requires\\nstudents to form connections between concepts in both domains.\\nRecent advances in LLM performance capabilities have allowed researchers\\nto find new and creative ways to apply these powerful models to education using\\nin-context learning (ICL) [2] (i.e., providing the LLM with labeled instances dur-\\ning inference) in lieu of traditional training that requires expensive parameter\\nupdates. One prominent extension of ICL is chain-of-thought reasoning (CoT)\\n[11], which augments the labeled instances with “reasoning chains” that explain\\nthe rationale behind the correct answer and help guide the LLM towards the cor-\\nrect solution. Recent work has found success in leveraging CoT towards scoring\\nand explaining students’ formative assessment responses in the Earth Science\\ndomain [3]. In this work, we investigate this approach as a means to summarize\\nand characterize synergistic learning in students’ collaborative discourse.\\n3 Methods\\nThis paper extends the previous work of 1) Snyder et al. on log-segmented dis-\\ncourse summarization defined by students’ model building segments extracted\\nfrom their activity logs [9], and 2) Cohn et al. on a human-in-the-loop prompt\\nengineering approach called Chain-of-Thought Prompting + Active Learning [3]\\n(the Method) for scoring and explaining students’ science formative assessment\\nresponses. The original Method is a three-step process: 1) Response Scoring,\\nwhere two human reviewers manually label a sample of students’ formative as-\\nsessment responses and identify disagreements (i.e., sticking points) the LLM\\nmay similarly struggle with; 2) Prompt Development, which employs few-shot\\nCoT prompting to address the sticking points and help align the LLM with\\nthe humans’ scoring consensus; and 3) Active Learning, where a knowledgeable\\nhuman (e.g., a domain expert, researcher, or instructor) acts as an “oracle” and\\nidentifies the LLM’s reasoning errors on a validation set, then appends additional\\nfew-shot instances that the LLM struggled with to the prompt and uses CoT\\nreasoning to help correct the LLM’s misconceptions. We illustrate the Method\\nin Figure 1. For a complete description of the Method, please see [3].\\nIn this work, we combine log-based discourse segmentation [9] and CoT\\nprompting [3] to generate more contextualized summaries of students’ discourse\\nsegments to study students’ synergistic learning processes by linking their model\\nconstruction and debugging activities with their conversations during each probl-\\nem-solving segment. We provide Supplementary Materials3that include 1) addi-\\ntional information about the learning environment, 2) method application details\\n(including our final prompt and few-shot example selection methodology), 3) a\\nmore in depth look at our conversation with the Educator, and 4) a more detailed\\nanalysis of the LLM’s strengths and weaknesses while applying the Method.\\n3https://github.com/oele-isis-vanderbilt/AIED24_LBR\\n4 C. Cohn et al.\\n3.1 STEM+C Learning Environment, Curriculum, and Data\\nOur work in this paper centers on the C2STEM learning environment [5], where\\nstudents learn kinematics by building computational models of the 1- and 2-D\\nmotion of objects. C2STEM combines block-based programming with domain-\\nspecific modeling blocks to support the development and integration of science\\nand computing knowledge as students create partial or complete models that\\nsimulate behaviors governed by scientific principles. This paper focuses on the\\n1-D Truck Task, where students use their knowledge of kinematic equations to\\nmodel the motion of a truck that starts from rest, accelerates to a speed limit,\\ncruises at that speed, then decelerates to come to a stop at a stop sign.\\nOur study, approved by our university Institutional Review Board, included\\n26 consented high school students (aged 14-15) who completed the C2STEM\\nkinematics curriculum. Students’ demographic information was not collected as\\npart of this study (we began collecting it in later studies). Data collection in-\\ncluded logged actions in the C2STEM environment, saved project files, and video\\nand audio data (collected using laptop webcams and OBS software). Our data\\nanalysis included 9 dyads (one group had a student who did not consent to\\ndata collection, so we did not analyze that group; and we had technical issues\\nwith audio data from other groups). The dataset includes 9 hours of discourse\\ntranscripts and over 2,000 logged actions collected during one day of the study.\\nStudent discourse was transcribed using Otter.ai and edited for accuracy.\\n3.2 Approach\\nWe extend the Method, previously used for formative assessment scoring and\\nfeedback, to prompt GPT-4-Turbo to summarize segments of students’ discourse\\nand identify the Discourse Category (defined momentarily) by answering the fol-\\nlowing question: “Given a discourse segment, and its environment task context\\nand actions, is the students’ conversation best characterized as physics-focused\\n(i.e., the conversation is primarily focused on the physics domain), computing-\\nfocused (i.e., the conversation is primarily focused on the computing domain),\\nphysics-and-computing-synergistic (i.e., students discuss concepts from both do-\\nmains, interleaving them throughout the conversation, and making connections\\nFig. 1. Chain-of-Thought Prompting + Active Learning, identified by the green box,\\nwhere each blue diamond is a step in the Method. Yellow boxes represent the process’s\\napplication to the classroom detailed in prior work [3].\\nTowards A HITL LLM Approach to Collaborative Discourse Analysis 5\\nbetween them), or physics-and-computing-separate (i.e., students discuss both\\ndomains but do so separately without interleaving)?” We use the recently re-\\nleased GPT-4-Turbo LLM (gpt-4-0125-preview) because it provides an extended\\ncontext window (128,000 tokens).\\nWe selected 10 training instances and 12 testing instances (10 additional\\nsegments were used as a validation set to perform Active Learning) prior to\\nResponse Scoring, using stratified sampling to approximate a uniform distribu-\\ntion across Discourse Categories for both the train and test sets. Note that the\\nstudent discourse was segmented based on which element of the model the stu-\\ndents were working on (identified automatically via log data). During Response\\nScoring, the first two authors of this paper (Reviewers R1 and R2, respectively)\\nindependently evaluated the training set segments, classifying each segment as\\nbelonging to one of the four Discourse Categories. For each segment the Re-\\nviewers disagreed on, the reason for disagreement was noted as a sticking point,\\nand the segment was discussed until a consensus was reached on the specific\\nDiscourse Category for that segment. R1 and R2 initially struggled to agree on\\nsegments’ Discourse Categories (Cohen’s k = 0.315). This is because segments\\noften contained concepts from both domains that may or may not have been\\ninterwoven, so it was not always clear which Discourse Category a segment be-\\nlonged to. Because of this, the Reviewers ultimately opted to label all segments\\nvia consensus coding.\\nDuring Prompt Development, we provided the LLM with explicit task instruc-\\ntions, curricular and environment context, and general guidelines (e.g., instruct-\\ning the LLM to cite evidence directly from the students’ discourse to support\\nits summary decisions and Discourse Category choice). We supplemented the\\nprompt with extensive contextual information not found in previous work [9],\\nincluding the Discourse Categories, C2STEM variables and their values, physics\\nand computing concepts and their definitions, and students’ actions in the learn-\\ning environment (derived from environment logs). Four labeled instances were\\ninitially appended to the prompt as few-shot examples (one per Discourse Cate-\\ngory). Active Learning was performed for a total of two rounds over 10 validation\\nset instances, at the end of which one additional few-shot instance was added.\\nBefore testing, R1 wrote summaries (and labeled Discourse Categories) for\\nthe 12 test instances. R2 then compared the human-generated summaries to two\\nLLMs’ summaries: GPT-4-Turbo and GPT-4. We compare GPT-4 to GPT-4-\\nTurbo to see which LLM is most promising for use in future work. To evaluate\\nRQ1, R2 used “ranked choice” to rank the three summaries from best to worst\\nfor each test set instance without knowledge of whether the summaries were gen-\\nerated by a human, GPT-4-Turbo, or GPT-4 (the Competitors). Three rankings\\nwere used for the scoring: (1) Wins (the number of times each Competitor was\\nranked higher than another Competitor across all instances, i.e., the best Com-\\npetitor for an individual segment receives two “wins” for outranking the other\\ntwo Competitors for that segment); (2) Best (the number of instances each Com-\\npetitor was selected as the best choice); and (3) Worst (the number of instances\\neach Competitor was selected as the worst choice). To answer RQ1, we used\\n'''. \\n How did they do it?\",\n",
       " \"Consider this paper: '''\\n\\n1\\nTowards A Human-in-the-Loop LLM Approach to\\nCollaborative Discourse Analysis\\nClayton Cohn1[0000−0003−0856−9587], Caitlin Snyder1[0000−0002−3341−0490], Justin\\nMontenegro2, and Gautam Biswas1[0000−0002−2752−3878]\\n1 Vanderbilt University, Nashville, TN 37240, USA\\nclayton.a.cohn@vanderbilt.edu2 Martin Luther King, Jr. Academic Magnet High School, Nashville, TN 37203, USA\\nAbstract. LLMs have demonstrated proficiency in contextualizing their\\noutputs using human input, often matching or beating human-level per-\\nformance on a variety of tasks. However, LLMs have not yet been used to\\ncharacterize synergistic learning in students’ collaborative discourse. In\\nthis exploratory work, we take a first step towards adopting a human-in-\\nthe-loop prompt engineering approach with GPT-4-Turbo to summarize\\nand categorize students’ synergistic learning during collaborative dis-\\ncourse. Our preliminary findings suggest GPT-4-Turbo may be able to\\ncharacterize students’ synergistic learning in a manner comparable to\\nhumans and that our approach warrants further investigation.\\nKeywords: LLM ·Collaborative Learning ·Human-in-the-Loop ·Dis-\\ncourse Analysis ·K12 STEM.\\n1 Introduction\\nComputational modeling of scientific processes has been shown to effectively\\nfoster students’ Science, Technology, Engineering, Mathematics, and Comput-\\ning (STEM+C) learning [5], but task success necessitates synergistic learning\\n(i.e., the simultaneous development and application of science and computing\\nknowledge to address modeling tasks), which can lead to student difficulties\\n[1]. Research has shown that problem-solving environments promoting synergis-\\ntic learning in domains such as physics and computing often facilitate a bet-\\nter understanding of physics and computing concepts and practices when com-\\npared to students taught via a traditional curriculum [5]. Analyzing students’\\ncollaborative discourse offers valuable insights into their application of both\\ndomains’ concepts as they construct computational models [8]. Unfortunately,\\nmanually analyzing students’ discourse to identify their synergistic processes is\\ntime-consuming, and programmatic approaches are needed.\\nIn this paper, we take an exploratory first step towards adopting a human-in-\\nthe-loop LLM approach from previous work called Chain-of-Thought Prompting\\n+ Active Learning [3] (detailed in Section 3) to characterize the synergistic con-\\ntent in students’ collaborative discourse. We use a large language model (LLM)\\nto summarize conversation segments in terms of how physics and computing\\narXiv:2405.03677v1  [cs.CL]  6 May 2024\\n2 C. Cohn et al.\\nconcepts are interwoven to support students’ model building and debugging\\ntasks. We evaluate our approach by comparing the LLM’s summaries to human-\\nproduced ones (using an expert human evaluator to rank them) and by qualita-\\ntively analyzing the summaries to discern the LLM’s strengths and weaknesses\\nalongside a physics and computer science teacher (the Educator) with experi-\\nence teaching the C2STEM curriculum (see Section 3.1). Within this framework,\\nwe analyze data from high school students working in pairs to build kinematics\\nmodels and answer the following research questions: RQ1) How does the quality\\nof human- and LLM-generated summaries and synergistic learning characteri-\\nzations of collaborative student discourse compare?, and RQ2) What are the\\nLLM’s strengths, and where does it struggle, in summarizing and characterizing\\nsynergistic learning in physics and computing?\\nAs this work is exploratory, due to the small sample size, we aim not to\\npresent generalizable findings but hope that our results will inform subsequent\\nresearch as we work towards forging a human-AI partnership by providing teach-\\ners with actionable, LLM-generated feedback and recommendations to help them\\nguide students in their synergistic learning.\\n2 Background\\nRoschelle and Teasley [7] define collaboration as “a coordinated, synchronous ac-\\ntivity that is a result of a continuous attempt to construct and maintain a shared\\nconception of a problem.” This development of a shared conceptual understand-\\ning necessitates multi-faceted collaborative discourse across multiple dimensions:\\nsocial (e.g., navigating the social intricacies of forming a consensus [12]), cogni-\\ntive (e.g., the development of context-specific knowledge [8]), and metacognitive\\n(e.g., socially shared regulation [4]). Researchers have developed and leveraged\\nframeworks situated within learning theory to classify and analyze collaborative\\nproblem solving (CPS) both broadly (i.e., across dimensions [6]) and narrowly\\n(i.e., by focusing on one CPS aspect to gain in-depth insight, e.g., argumentative\\nknowledge construction [12]). In this paper, we focus on one dimension of CPS\\nthat is particularly important to the context of STEM+C learning: students’\\ncognitive integration of synergistic domains.\\nLeveraging CPS frameworks to classify student discourse has traditionally\\nbeen done through hand-coding utterances. However, this is time-consuming\\nand laborious, leading researchers to leverage automated classification methods\\nsuch as rule-based approaches, supervised machine learning methods, and (more\\nrecently) LLMs [10]. Utilizing LLMs can help extend previous work on classifying\\nsynergistic learning discourse, which has primarily relied on the frequency counts\\nof domain-specific concept codes [8,5]. In particular, the use of LLMs can help\\naddress the following difficulties encountered while employing traditional meth-\\nods: (1) concept codes are difficult to identify programmatically, as rule-based\\napproaches like regular expressions (regex) have difficulties with misspellings\\nand homonyms; (2) the presence or absence of concept codes is not analyzed in\\na conversational context; and (3) the presence of cross-domain concept codes is\\nTowards A HITL LLM Approach to Collaborative Discourse Analysis 3\\nnot necessarily indicative of synergistic learning, as synergistic learning requires\\nstudents to form connections between concepts in both domains.\\nRecent advances in LLM performance capabilities have allowed researchers\\nto find new and creative ways to apply these powerful models to education using\\nin-context learning (ICL) [2] (i.e., providing the LLM with labeled instances dur-\\ning inference) in lieu of traditional training that requires expensive parameter\\nupdates. One prominent extension of ICL is chain-of-thought reasoning (CoT)\\n[11], which augments the labeled instances with “reasoning chains” that explain\\nthe rationale behind the correct answer and help guide the LLM towards the cor-\\nrect solution. Recent work has found success in leveraging CoT towards scoring\\nand explaining students’ formative assessment responses in the Earth Science\\ndomain [3]. In this work, we investigate this approach as a means to summarize\\nand characterize synergistic learning in students’ collaborative discourse.\\n3 Methods\\nThis paper extends the previous work of 1) Snyder et al. on log-segmented dis-\\ncourse summarization defined by students’ model building segments extracted\\nfrom their activity logs [9], and 2) Cohn et al. on a human-in-the-loop prompt\\nengineering approach called Chain-of-Thought Prompting + Active Learning [3]\\n(the Method) for scoring and explaining students’ science formative assessment\\nresponses. The original Method is a three-step process: 1) Response Scoring,\\nwhere two human reviewers manually label a sample of students’ formative as-\\nsessment responses and identify disagreements (i.e., sticking points) the LLM\\nmay similarly struggle with; 2) Prompt Development, which employs few-shot\\nCoT prompting to address the sticking points and help align the LLM with\\nthe humans’ scoring consensus; and 3) Active Learning, where a knowledgeable\\nhuman (e.g., a domain expert, researcher, or instructor) acts as an “oracle” and\\nidentifies the LLM’s reasoning errors on a validation set, then appends additional\\nfew-shot instances that the LLM struggled with to the prompt and uses CoT\\nreasoning to help correct the LLM’s misconceptions. We illustrate the Method\\nin Figure 1. For a complete description of the Method, please see [3].\\nIn this work, we combine log-based discourse segmentation [9] and CoT\\nprompting [3] to generate more contextualized summaries of students’ discourse\\nsegments to study students’ synergistic learning processes by linking their model\\nconstruction and debugging activities with their conversations during each probl-\\nem-solving segment. We provide Supplementary Materials3that include 1) addi-\\ntional information about the learning environment, 2) method application details\\n(including our final prompt and few-shot example selection methodology), 3) a\\nmore in depth look at our conversation with the Educator, and 4) a more detailed\\nanalysis of the LLM’s strengths and weaknesses while applying the Method.\\n3https://github.com/oele-isis-vanderbilt/AIED24_LBR\\n4 C. Cohn et al.\\n3.1 STEM+C Learning Environment, Curriculum, and Data\\nOur work in this paper centers on the C2STEM learning environment [5], where\\nstudents learn kinematics by building computational models of the 1- and 2-D\\nmotion of objects. C2STEM combines block-based programming with domain-\\nspecific modeling blocks to support the development and integration of science\\nand computing knowledge as students create partial or complete models that\\nsimulate behaviors governed by scientific principles. This paper focuses on the\\n1-D Truck Task, where students use their knowledge of kinematic equations to\\nmodel the motion of a truck that starts from rest, accelerates to a speed limit,\\ncruises at that speed, then decelerates to come to a stop at a stop sign.\\nOur study, approved by our university Institutional Review Board, included\\n26 consented high school students (aged 14-15) who completed the C2STEM\\nkinematics curriculum. Students’ demographic information was not collected as\\npart of this study (we began collecting it in later studies). Data collection in-\\ncluded logged actions in the C2STEM environment, saved project files, and video\\nand audio data (collected using laptop webcams and OBS software). Our data\\nanalysis included 9 dyads (one group had a student who did not consent to\\ndata collection, so we did not analyze that group; and we had technical issues\\nwith audio data from other groups). The dataset includes 9 hours of discourse\\ntranscripts and over 2,000 logged actions collected during one day of the study.\\nStudent discourse was transcribed using Otter.ai and edited for accuracy.\\n3.2 Approach\\nWe extend the Method, previously used for formative assessment scoring and\\nfeedback, to prompt GPT-4-Turbo to summarize segments of students’ discourse\\nand identify the Discourse Category (defined momentarily) by answering the fol-\\nlowing question: “Given a discourse segment, and its environment task context\\nand actions, is the students’ conversation best characterized as physics-focused\\n(i.e., the conversation is primarily focused on the physics domain), computing-\\nfocused (i.e., the conversation is primarily focused on the computing domain),\\nphysics-and-computing-synergistic (i.e., students discuss concepts from both do-\\nmains, interleaving them throughout the conversation, and making connections\\nFig. 1. Chain-of-Thought Prompting + Active Learning, identified by the green box,\\nwhere each blue diamond is a step in the Method. Yellow boxes represent the process’s\\napplication to the classroom detailed in prior work [3].\\nTowards A HITL LLM Approach to Collaborative Discourse Analysis 5\\nbetween them), or physics-and-computing-separate (i.e., students discuss both\\ndomains but do so separately without interleaving)?” We use the recently re-\\nleased GPT-4-Turbo LLM (gpt-4-0125-preview) because it provides an extended\\ncontext window (128,000 tokens).\\nWe selected 10 training instances and 12 testing instances (10 additional\\nsegments were used as a validation set to perform Active Learning) prior to\\nResponse Scoring, using stratified sampling to approximate a uniform distribu-\\ntion across Discourse Categories for both the train and test sets. Note that the\\nstudent discourse was segmented based on which element of the model the stu-\\ndents were working on (identified automatically via log data). During Response\\nScoring, the first two authors of this paper (Reviewers R1 and R2, respectively)\\nindependently evaluated the training set segments, classifying each segment as\\nbelonging to one of the four Discourse Categories. For each segment the Re-\\nviewers disagreed on, the reason for disagreement was noted as a sticking point,\\nand the segment was discussed until a consensus was reached on the specific\\nDiscourse Category for that segment. R1 and R2 initially struggled to agree on\\nsegments’ Discourse Categories (Cohen’s k = 0.315). This is because segments\\noften contained concepts from both domains that may or may not have been\\ninterwoven, so it was not always clear which Discourse Category a segment be-\\nlonged to. Because of this, the Reviewers ultimately opted to label all segments\\nvia consensus coding.\\nDuring Prompt Development, we provided the LLM with explicit task instruc-\\ntions, curricular and environment context, and general guidelines (e.g., instruct-\\ning the LLM to cite evidence directly from the students’ discourse to support\\nits summary decisions and Discourse Category choice). We supplemented the\\nprompt with extensive contextual information not found in previous work [9],\\nincluding the Discourse Categories, C2STEM variables and their values, physics\\nand computing concepts and their definitions, and students’ actions in the learn-\\ning environment (derived from environment logs). Four labeled instances were\\ninitially appended to the prompt as few-shot examples (one per Discourse Cate-\\ngory). Active Learning was performed for a total of two rounds over 10 validation\\nset instances, at the end of which one additional few-shot instance was added.\\nBefore testing, R1 wrote summaries (and labeled Discourse Categories) for\\nthe 12 test instances. R2 then compared the human-generated summaries to two\\nLLMs’ summaries: GPT-4-Turbo and GPT-4. We compare GPT-4 to GPT-4-\\nTurbo to see which LLM is most promising for use in future work. To evaluate\\nRQ1, R2 used “ranked choice” to rank the three summaries from best to worst\\nfor each test set instance without knowledge of whether the summaries were gen-\\nerated by a human, GPT-4-Turbo, or GPT-4 (the Competitors). Three rankings\\nwere used for the scoring: (1) Wins (the number of times each Competitor was\\nranked higher than another Competitor across all instances, i.e., the best Com-\\npetitor for an individual segment receives two “wins” for outranking the other\\ntwo Competitors for that segment); (2) Best (the number of instances each Com-\\npetitor was selected as the best choice); and (3) Worst (the number of instances\\neach Competitor was selected as the worst choice). To answer RQ1, we used\\n'''. \\n What are the key take-aways?\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prompt_template_paper_questions(prompt: str, paper_contents: str):\n",
    "    return f\"Consider this paper: '''{paper_contents}'''. \\n {prompt}\"\n",
    "\n",
    "prompt1 = \"What problem is the paper trying to solve?\"\n",
    "prompt2 = \"Why is the problem interesting?\"\n",
    "prompt3 = \"What is the primary contribution?\"\n",
    "prompt4 = \"How did they do it?\"\n",
    "prompt5 = \"What are the key take-aways?\"\n",
    "\n",
    "prompt_high_level_question_list = [prompt_template_paper_questions(p, paper_contents) for p in [prompt1, prompt2, prompt3, prompt4, prompt5]]\n",
    "prompt_high_level_question_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_high_level_questions = []\n",
    "for prompt in prompt_high_level_question_list:\n",
    "    output_high_level_questions.append(get_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What problem is the paper trying to solve?\n",
      "The paper is addressing the challenge of analyzing students' collaborative discourse to better understand and characterize their synergistic learning processes in STEM+C (Science, Technology, Engineering, Mathematics, and Computing) environments. Synergistic learning, which involves the simultaneous development and application of knowledge in science and computing to solve modeling tasks, can be complex and challenging for students. The traditional approach of manually analyzing collaborative discourse to identify and characterize these synergistic learning processes is time-consuming and labor-intensive. The paper explores the use of a human-in-the-loop large language model (LLM) to automatically summarize and categorize the content of students' discussions during collaborative tasks, aiming to make this analysis more efficient and effective.\n",
      "****\n",
      "Why is the problem interesting?\n",
      "The problem addressed in the paper is interesting for several reasons:\n",
      "\n",
      "1. **Integration of Human and AI Capabilities**: The paper explores a human-in-the-loop (HITL) approach to analyze collaborative discourse using large language models (LLMs). This integration aims to leverage the strengths of both humans and AI to achieve better outcomes than either could accomplish alone. By combining human insights with the scalability and data-processing capabilities of LLMs, this research could provide a more efficient and effective method to analyze complex interactions in educational settings.\n",
      "\n",
      "2. **Advancements in Educational Techniques**: The work focuses on the analysis of synergistic learning—where students integrate knowledge from multiple domains (like physics and computing) to solve problems. Understanding how students interact and learn collaboratively in STEM (Science, Technology, Engineering, and Mathematics) subjects is crucial for developing educational strategies that enhance learning outcomes. The paper's approach could lead to more personalized and adaptive learning experiences, tailored to the specific dynamics of student interactions and their problem-solving processes.\n",
      "\n",
      "3. **Improvement of Collaborative Learning Analysis**: Traditional methods for analyzing collaborative learning are often labor-intensive and involve manual coding of discourse. The researchers are proposing to use an LLM to automate and potentially enhance this process. This could significantly reduce the time and effort required to obtain insights into student learning behaviors and interactions, making it easier to scale up such analyses to larger groups or more frequent assessments.\n",
      "\n",
      "4. **Potential for Real-Time Feedback in Education**: If successful, the methodologies being developed could be applied in real-time educational settings, providing immediate feedback to educators about the nature of student interactions and learning. This could help teachers to intervene more effectively and timely, customizing their educational approaches to better meet the needs of their students.\n",
      "\n",
      "5. **Exploration of LLM Capabilities in New Contexts**: The research explores the application of the latest advancements in LLMs (like GPT-4-Turbo) for educational content analysis, a relatively novel area of study. Understanding how well these models perform in analyzing educational discourse can provide insights into the limits and capabilities of current AI technologies, potentially guiding future developments in AI and education.\n",
      "\n",
      "Overall, this research is situated at the intersection of AI technology and educational methodology, offering potential innovations that could impact how collaborative learning is understood and facilitated in educational environments.\n",
      "****\n",
      "What is the primary contribution?\n",
      "The primary contribution of the paper titled \"Towards A Human-in-the-Loop LLM Approach to Collaborative Discourse Analysis\" is the development and exploratory application of a human-in-the-loop large language model (LLM) methodology to analyze and characterize synergistic learning within collaborative student discourse in the context of STEM education. This approach utilizes a novel combination of Chain-of-Thought Prompting and Active Learning techniques with GPT-4-Turbo to generate summaries and categorize the synergistic learning displayed in students' discourse as they engage with computational modeling tasks. The key innovation lies in using advanced LLMs to potentially streamline and enhance the analysis of student interactions, aiming to provide qualitative insights comparable to those of human experts, thereby supporting educational research and potentially informing teaching practices.\n",
      "****\n",
      "How did they do it?\n",
      "The paper outlines an exploratory study to investigate how a large language model (LLM), specifically GPT-4-Turbo, can be utilized for summarizing and characterizing synergistic learning in students' collaborative discourse within a STEM+C learning environment. The primary focus is on human-in-the-loop (HITL) approaches, which integrate human feedback into the machine learning process to improve the model's performance and applicability in educational settings. Here's a breakdown of the methods used in the study:\n",
      "\n",
      "### 1. Chain-of-Thought Prompting + Active Learning Methodology\n",
      "This method is a human-in-the-loop prompt engineering approach consisting of three main steps:\n",
      "   - **Response Scoring:** Two human reviewers manually label a sample of student discourse segments on synergistic learning. They work to resolve any discrepancies through discussion, reaching a consensus on labels for each segment.\n",
      "   - **Prompt Development:** The model is provided with explicit task instructions and extensive contextual information, including curriculum context and specific guidelines for the model to cite evidence directly from the students’ discourse in its summaries.\n",
      "   - **Active Learning:** Involves a knowledgeable human (\"the oracle\") who assesses the model's performance based on its reasoning outputs, identifying and correcting the model's misconceptions by adding more labeled examples and refining the prompts.\n",
      "\n",
      "### 2. Data Collection and Segmentation\n",
      "The data includes audiovisual and logged actions from high school students working within the C2STEM curriculum, focusing on kinematics and block-based programming tasks. Student discourse was transcribed and segmented based on their interactions and the specific elements of the model they were working on, identified via their activity logs in the learning environment.\n",
      "\n",
      "### 3. LLM Setup and Training\n",
      "The model used was the GPT-4-Turbo variant with an expanded context window, which allows it to consider a broader range of input data in generating outputs. Ten training instances were selected, along with additional segments for testing and validation to ensure a comprehensive evaluation.\n",
      "\n",
      "### 4. Evaluation\n",
      "The evaluation involved comparing the summaries generated by the LLM to those produced by human experts (the authors themselves). They used a unique \"ranked choice\" method where each summary was blindly rated against the others to objectively determine which was most effective—considering factors such as adherence to Discourse Categories, clarity, and the integration of relevant concepts.\n",
      "\n",
      "### 5. Implementation of Discourse Categories\n",
      "The discourse was categorized based on the focus and integration of physics and computing concepts:\n",
      "   - Physics-focused\n",
      "   - Computing-focused\n",
      "   - Physics-and-computing-synergistic\n",
      "   - Physics-and-computing-separate\n",
      "\n",
      "This categorization helped in evaluating the model’s ability to identify and articulate the nature of the discussion in STEM+C context-dependent terms.\n",
      "\n",
      "### 6. Feedback and Iteration\n",
      "Based on the outcomes of the initial model evaluations, additional training and feedback were iteratively applied to refine the model's performance, focusing especially on areas where discrepancies between the LLM and human assessments were evident.\n",
      "\n",
      "### 7. Supplementary Materials and Analysis\n",
      "Additional materials and detailed discussions with the educators involved in the study were used to further contextualize the findings and refine the model's approach.\n",
      "\n",
      "This approach allows for a detailed analysis of how students integrate knowledge across different domains (physics and computing), providing insights into their synergistic learning processes and offering a novel way to support educators in assessing and enhancing collaborative learning using AI technologies.\n",
      "****\n",
      "What are the key take-aways?\n",
      "The key takeaways from this paper on \"Towards a Human-in-the-Loop LLM Approach to Collaborative Discourse Analysis\" are:\n",
      "\n",
      "1. **Exploration of LLMs for Educational Discourse:** The paper presents exploratory research on using large language models (LLMs) to analyze synergistic learning in collaborative student discourse. The authors focus on characterizing the interplay of physics and computing concepts within student conversations during problem-solving tasks.\n",
      "\n",
      "2. **Human-in-the-Loop Approach:** The study adopts a human-in-the-loop (HITL) framework, combining human expertise with automated machine learning processes to improve the analysis of educational data. This methodology leverages previous frameworks that involve Chain-of-Thought (CoT) Prompting and Active Learning to refine the LLM’s understanding and outputs.\n",
      "\n",
      "3. **Promising Preliminary Results:** The initial findings suggest that LLMs can potentially match human performance in summarizing and categorizing collaborative discourse related to synergistic learning. This is significant as it can reduce the time and labor required in manual discourse analysis, offering more scalable solutions for educational settings.\n",
      "\n",
      "4. **Application to STEM Education:** Specifically, the research focuses on a STEM+C (Science, Technology, Engineering, Mathematics, and Computing) learning environment where the analysis targets students' collaborative discussions as they build computational models. The ability to automatically discern the nature of discourse (physics-focused, computing-focused, synergistic, or separate discussions) illustrates a valuable application of LLMs in educational contexts.\n",
      "\n",
      "5. **Challenges and Complexity in Classification:** The paper highlights the complexity in classifying discourse into distinct categories due to the nuanced interplay of different knowledge domains within student interactions. Initial low agreement rates between human reviewers (Cohen’s k = 0.315) emphasize the challenge and the need for a sophisticated approach to achieve reliable classifications.\n",
      "\n",
      "6. **Methodological Rigor and Iterative Refinement:** The research methodology involves log-based discourse segmentation, iterative scoring and consensus building among human reviewers, and extensive prompt engineering to guide the LLM to better performance. Active learning rounds further refine the model’s accuracy.\n",
      "\n",
      "7. **Comparison of LLM Versions:** The study compares summaries generated by two versions of LLMs (GPT-4 and GPT-4-Turbo) against human-produced summaries to evaluate which model performs best in a real-world educational context. This helps gauge the current capabilities and limitations of state-of-the-art LLMs in specific applied research scenarios.\n",
      "\n",
      "8. **Future Directions:** The paper sets the stage for more comprehensive studies with larger datasets and refined methodologies. The results encourage further investigation into the potential of LLMs as supportive tools for educators, allowing them to provide more targeted and effective guidance based on automated insights into student learning processes.\n",
      "\n",
      "Overall, the paper demonstrates an innovative application of advanced language models in the educational sector, with a special emphasis on enhancing the analysis and understanding of student interactions in complex learning environments.\n",
      "****\n"
     ]
    }
   ],
   "source": [
    "prompt_questions = [prompt1, prompt2, prompt3, prompt4, prompt5]\n",
    "for q,o in zip(prompt_questions,output_high_level_questions):\n",
    "    print(q)\n",
    "    print(o)\n",
    "    print(\"****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_template_paper_questions(prompt: str, paper_contents: str):\n",
    "    return f\"Consider this paper: '''{paper_contents}'''. \\n {prompt}. At the end of your answer, include at least one citation from the original paper to validate your response.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_questions = [prompt1, prompt2, prompt3, prompt4, prompt5]\n",
    "\n",
    "prompt_high_level_question_list = [prompt_template_paper_questions(p, paper_contents) for p in [prompt1, prompt2, prompt3, prompt4, prompt5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_high_level_questions = []\n",
    "for prompt in prompt_high_level_question_list:\n",
    "    output_high_level_questions.append(get_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "What problem is the paper trying to solve?\n",
       "\n",
       "The paper addresses the challenge of analyzing students' collaborative discourse in STEM+C (Science, Technology, Engineering, Mathematics, and Computing) learning environments to identify and characterize synergistic learning—which involves the integrated use of physics and computing knowledge. The main problem it seeks to solve is the time-consuming and labor-intensive nature of manually analyzing such discourse to understand how students interweave concepts from different domains during problem-solving activities. The paper proposes leveraging a Large Language Model (LLM) with a human-in-the-loop approach to automate and potentially enhance the efficiency and effectiveness of this discourse analysis process, aiming to provide useful insights and feedback that can aid in educational settings.\n",
       "\n",
       "The significance of addressing this problem is underscored by the benefits of synergistic learning in improving students' understanding of complex concepts when compared to traditional learning methods, as students engaging in synergistic learning often achieve a better grasp of the interconnected concepts of physics and computing. Therefore, developing automated tools to analyze such learning processes can provide crucial support for educational research and instructional design.\n",
       "\n",
       "A direct citation that supports this problem description is: \"Research has shown that problem-solving environments promoting synergistic learning in domains such as physics and computing often facilitate a better understanding of physics and computing concepts and practices when compared to students taught via a traditional curriculum [5]. Analyzing students’ collaborative discourse offers valuable insights into their application of both domains’ concepts as they construct computational models [8]. Unfortunately, manually analyzing students’ discourse to identify their synergistic processes is time-consuming, and programmatic approaches are needed\" (Cohn et al.)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(prompt1 + \"\\n\\n\" + output_high_level_questions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will employ some of the techniques and ideas we've learned and instead of doing a rigid experiment, per se, we will play around with these ideas to perform different actions with LLMs to try and better understand the contents of scientific papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential Extensions\n",
    "\n",
    "- LLM outlines the background knowledge required to understand the paper as bullet points.\n",
    "- LLM quizzes the learner about the contents of the paper\n",
    "- Extracting citations\n",
    "- Extracting quotes from the paper to validate statements  \n",
    "- Paper summary in different formats\n",
    "- LLM provides opportunity for discussion about contents of the paper\n",
    "- LLM gives feedback on learner's understanding of the paper based on sections of the paper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-prompt-eng",
   "language": "python",
   "name": "oreilly-prompt-eng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
