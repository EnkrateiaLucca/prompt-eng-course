{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToT Example\n",
    "\n",
    "> ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to\n",
    "decide the next course of action, as well as looking ahead or backtracking when\n",
    "necessary to make global choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to set your OpenAI API key in the .env file\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/dave1010/tree-of-thought-prompting\n",
    "\n",
    "tree_of_thoughts_prompting = '''\n",
    "Simulate three brilliant, logical experts collaboratively answering a question. \n",
    "\n",
    "Each one verbosely explains their thought process in real-time, considering the prior explanations of others and openly acknowledging mistakes. \n",
    "\n",
    "At each step, whenever possible, each expert refines and builds upon the thoughts of others, acknowledging their contributions. \n",
    "\n",
    "They continue until there is a definitive answer to the question. \n",
    "\n",
    "All experts will write down the step and their thinking about the step, then share it with the group.\n",
    "Then, all experts will go on to the next step, etc.\n",
    "At each step all experts will score their peers response between 1 and 5, 1 meaning it is highly unlikely, and 5 meaning it is highly likely.\n",
    "If any expert is judged to be wrong at any point then they leave.\n",
    "After all experts have provided their analysis, you then analyze all 3 analyses and provide either the consensus solution or your best guess solution.\n",
    "For clarity, your entire response should be in a markdown table. \n",
    "The question is...\n",
    "'''\n",
    "\n",
    "\n",
    "question = '''\n",
    "Bob is in the living room.\n",
    "He walks to the kitchen, carrying a cup.\n",
    "He puts a ball in the cup and carries the cup to the bedroom.\n",
    "He turns the cup upside down, then walks to the garden.\n",
    "He puts the cup down in the garden, then walks to the garage.\n",
    "Where is the ball?\n",
    "'''\n",
    "\n",
    "full_prompt = tree_of_thoughts_prompting + \"\\n\" + question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The ball is in the garden, as Bob put the ball in the cup before going to the bedroom and then turned the cup upside down in the garden."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def get_response(prompt_question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "Markdown(get_response(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| **Step** | **Expert 1** - Explanation | **Expert 2** - Explanation | **Expert 3** - Explanation | **Scoring Round** |\n",
       "|----------|---------------------------|--------------------------|--------------------------|---------------------|\n",
       "| 1        | The ball is initially inside the cup in the kitchen with Bob. | Agreed with Expert 1. | Agreed with Expert 1. | 5, 5, 5 |\n",
       "| 2        | Bob carries the cup with the ball to the bedroom. The ball is still in the cup. | Bob's actions create a clear progression of the cup with the ball from kitchen to bedroom. | Agreed with Expert 2. | 5, 5, 5 |\n",
       "| 3        | Bob turns the cup upside down in the bedroom. The ball falls out and remains in the bedroom. | Agreed with Expert 1. | The ball falls out in the bedroom, as stated by Expert 1. | 5, 5, 5 |\n",
       "| 4        | Bob goes to the garden, leaving the cup behind in the bedroom, so the ball remains in the bedroom. | Agreed with Expert 1. | The ball should be left in the bedroom as per the statements. | 5, 5, 5 |\n",
       "| 5        | Bob puts the cup down in the garden without the ball in it, confirming the ball is in the bedroom. | The cup was left in the bedroom, so the ball remains there. | Agreed with Expert 2. | 5, 5, 5 |\n",
       "| 6        | Bob goes to the garage. The ball is definitely in the bedroom, based on the established sequence of events. | Agreed with Expert 1. | The ball must be in the bedroom since Bob has not gone back. | 5, 5, 5 |\n",
       "\n",
       "Based on the scoring round, the consensus solution is that the ball is in the **bedroom**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(get_response(full_prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-prompt-eng",
   "language": "python",
   "name": "oreilly-prompt-eng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
